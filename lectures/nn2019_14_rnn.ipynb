{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Sieci rekurencyjne</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rekurencyjne sieci neuronowe\n",
    "<img src=\"../nn_figures/rnn-diagrams.jpeg\" width=\"90%\">\n",
    "\n",
    "1. obraz w klasyfikację\n",
    "2. klasyfikacja obraz przez sekwencję; wiele klasyfikacji\n",
    "3. sekwencja wyrazów w analizę (np. sentymentu)\n",
    "  * generacja obrazu z sekwencji\n",
    "4. translacja: sekwencja w sekwencję, zwykle różnej długości\n",
    "5. klasyfikacja każdej ramki\n",
    "\n",
    "Sieć rekurencyjna używa pewnego rekurencyjnego wzoru w każdym kroku \n",
    "$$\\begin{align}\n",
    "h_t&=f(h_{t-1}, x_t; U,V,W)\\\\\n",
    "h_t&=\\tanh(Wh_{t-1}+Ux_t)\\\\\n",
    "y_t&=Vh_t\n",
    "\\end{align}$$\n",
    "%![rnn-diags.jpeg](attachment:rnn-diags.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozwinięcie sieci rekurencyjnej\n",
    "<img src=\"../nn_figures/rnn.jpeg\" width=\"90%\"> [Nature]\n",
    "\n",
    "1. __rozwinięcie sieci__ w $n$ składowych dla wygenerowania $n$-elementowego ciągu\n",
    "2. $x_t$ to wejscie w chwili $t$\n",
    "  * gdy generujemy więcej niż jedno słowo (znak) wprzód, to poprzednio wygenerowane staje się wejściem do następnego\n",
    "  * oczywiście może być tylko jedno $x$\n",
    "3. __pamięć__ (stan ukryty)) $h_t$ \n",
    "  * obliczane na podstawie poprzednich $h_t=f(Ux_t+Ws_{t-1})$\n",
    "  * $f$ funkcją nieliniową\n",
    "4. __wyjście__ $y_t$ w chwili $t$\n",
    "  * zwykle jako $softmax(Vh_t)$\n",
    "    * z tego stan wyjściowy wybierany przez $\\arg\\max$ albo sampling\n",
    "  * zwraca wektor prawdopodobieństw stanów dyskretnych\n",
    "  * interesujący może być np. tylko ostatni stan określający znaczenie zdania (sentiment analysis)\n",
    "5. $h_t$ przechowuje __całą__ informację na temat poprzednich stanów obliczeń\n",
    "  * własność Markowa\n",
    "  * praktycznie nie jest wystarczająca\n",
    "6. __wszystkie__ kroki __dzielą__ te same parametry $U, V, W$\n",
    "\n",
    "### Sieci głębokie RNN\n",
    "2. Model u góry to oczywiscie model płytki\n",
    "1. ten model może był także __deep__\n",
    "  * może być więcej niż jedna warstwa ukryta\n",
    "    * każda z nich aktualizuje swój stan\n",
    "  * warstwa ukryta może aktualizować swój stan przez wielokrokowe przeliczenia (głęboka sieć)\n",
    "  * też kombinacje obu podejść"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemy RNN\n",
    "1. od dawna znane różne podstawowe architektury RNN\n",
    "  * stanem pamięci jest stan ukryty i tam następuje rekurencja\n",
    "  * aktualny stan wyjsciowy staje się _dodatkowym_ stanem wejściowym (jak w automatach)\n",
    "2. podstawowymi problemami są\n",
    "  * pamięć jedynie ostatnich akcji, _zapominanie_ stanów poprzednich\n",
    "  * pamięć jedynie pojedynczych stanów globalnych dla całego modelu bez pamięci stanów ostatnich\n",
    "  * stąd potrzeba modelu wypełniającego tą dziurę - __long-short time memory__\n",
    "  * eksplodujące / zanikające gradienty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zastosowania\n",
    "1. __modelowanie i generowanie języka__\n",
    "  * predykcja __prawdopodobieństwa__, że zdanie jest poprawne\n",
    "  * samplując z tego dostajemy model __generatywny__\n",
    "  * model językowy z użyciem __n-gramów__\n",
    "  $$P(w_1,\\dots,w_m)=\\prod_{i=1}^mP(w_i\\mid w_1,\\dots,w_{i-1})\\approx\\prod_{i=1}^mP(w_i\\mid w_{i-(n-1)},\\dots,w_{i-1})$$\n",
    "  dla n-gramów $$P(w_i\\mid w_{i-(n-1)},\\dots,w_{i-1})=\\frac{\\#(w_{i-(n-1)},\\dots,w_{i-1}, w_i)}{\\#(w_{i-(n-1)},\\dots,w_{i-1})}$$\n",
    "2. __tłumaczenie języka__\n",
    "  * podobne do modelowania\n",
    "  * wymaga zwykle przeczytania kompletnego zdania w jednym języku __przed__ wygenerowaniem pierwszego słowa nowego zdania\n",
    "3. __rozpoznawanie języka__\n",
    "  * wejściem są odczytane __fonemy__\n",
    "  * wyjściem nowe fonemy lub transkrypcja na zdania (tłumaczenie)\n",
    "4. Modele RNN pozwalają przyjmować wejścia o __zmiennej długości__\n",
    "  * na przykład opis obrazu jako wiele losowych sampli z niego\n",
    "\n",
    "<img src=\"../nn_figures/rnn-diagrams.jpeg\" width=\"80%\"> [Karpathy]\n",
    "\n",
    "\n",
    "Koszt\n",
    "1. many-to-many: \n",
    "  * w każdym kroku można obliczyć koszt i gradient, \n",
    "  * po końcu sekwencji zsumować\n",
    "  * zaaplikować zmiany\n",
    "2. many-to-one\n",
    "  * wartość znana jest dopiero po końcu całej sekwencji\n",
    "  * koszt i gradient na końcu\n",
    "3. one-to-many\n",
    "  * znowu koszt na każdym kroku\n",
    "4. sekwencja do sekwencji to złożenie many-to-one oraz one-to-many\n",
    "  * _enkoder_ tworzy reprezentację całego wejścia (np. zdania w jednym języku)\n",
    "  * _dekoder_ odtwarza ten ukryty stan do postaci sekwencji\n",
    "  * oczywiście kompresja do jednego stanu jest trudna\n",
    "    * Macron mówił w przemówieniu \"Mes cher compatriots\"\n",
    "    * co w automatycznym tłumaczeniu na angielski (czytaj amerykański) wyszło \"my fellow americans\" :-))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation Through Time BPTT\n",
    "1. za każdym razem patrzymy kilka kroków wstecz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# klasa RNN (za http://wildml.com)\n",
    "class RNNNumpy():\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), \n",
    "                                   (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), \n",
    "                                   (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), \n",
    "                                   (hidden_dim, hidden_dim))\n",
    "        \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    " \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "        # or sample\n",
    "        return np.random.choice(range(len(o)), size=1, p=o)\n",
    " \n",
    "    #RNNNumpy.predict = predict\n",
    "    #RNNNumpy.forward_propagation = forward_propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "<img src=\"../nn_figures/rnn.jpeg\" width=\"70%\"> [Nature]\n",
    "1. w każdym kroku należy znaleźć wszystkie macierze parametrów $U, V, W$\n",
    "  * są wspólne dla wszystkich kroków\n",
    "  * zwykle mają dużo parametrów\n",
    "  * niech będzie $N$ różnych słów, a pamięć jest reprezentowana przez wektor o długosci $K$\n",
    "    * $x_t\\in\\mathbb{R}^{N}$\n",
    "    * $o_t\\in\\mathbb{R}^{N}$\n",
    "    * $s_t\\in\\mathbb{R}^{K}$\n",
    "    * $U\\in\\mathbb{R}^{K\\times{}N}$\n",
    "    * $V\\in\\mathbb{R}^{N\\times{}K}$\n",
    "    * $W\\in\\mathbb{R}^{K\\times{}K}$\n",
    "2. parametry są __dzielone__ we wszystkich przewidywanych krokach\n",
    "  * gradient w aktualnym kroku zależy \n",
    "    * od obliczeń w aktualnym kroku czasu\n",
    "    * od obliczeń w poprzednim kroku\n",
    "  * odpowiada to wykorzystaniu __reguły łańcuchowej__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# za [Britz]\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # wykonanie propagacji wprzód (zwraca ostatnie wyjscie i stan pamięci)\n",
    "    #  forward_propagation() wykonuje kroki wprzód zapamiętując wszystkie wartosci pośrednie,\n",
    "    #  które będą później potrzebne\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # macierze potrzebne dla akumulacji gradientów\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # teraz cofając się wstecz w obliczeniach\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # wstęczne obliczenia dla ostatniego kroku\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # wsteczna propagacja w czasie po poprzedzających krokach, ale co najwyżej bptt_truncate kroków\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # aktualizacja\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "1. Algorytm jest w stanie nauczyć się prostych zależności\n",
    "  * kolejność słów: bi-gramy, tri-gramy\n",
    "  * częstość występowania słów\n",
    "  * prostej składni\n",
    "  * prostej interpunkcji\n",
    "3. Jednak\n",
    "  * podawane zdania są zbyt krótkie by nauczyć poprawnej gramatyki\n",
    "  * dłuższe zdania znacznie zwiększają złożoność uczenia\n",
    "  * __nie jest w stanie__ nauczyć się zależności między __odległymi__ słowami\n",
    "    * proste RNN są w stanie imitować __jedynie__ pamięć krótko-terminową\n",
    "  * BPTT cierpi w dużym stopniu na problem zanikającego / eksplodującego gradientu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher forcing\n",
    "W modelu many-to-many, np w przewidywaniu kolejnego słowa, zamiast przekazywać do następnego stanu tylko stan ukryty, możemy\n",
    "1. przekazywać wektor (ew. znormalizowanych) logitsów predykcji $o_{t-1}$ do następnego stanu,\n",
    "2. __teacher forcing__\n",
    "  * logitsy są zwykle bardzo rozmyte, bo jest wiele wyjściowych słów, stąd sygnał propagujący jest słaby\n",
    "  * w trakcie trenowania można wybrać odpowiedź $y_{t-1}$ i ją przekazać (jako wektor one-hot)\n",
    "  * w trakcie testowania już tylko wektor $o_{t-1}$\n",
    "3. teacher forcing czasem zachowuje się niestabilnie\n",
    "  * rzadka odpowiedź na początku powoduje cżasto dziwne decyzje w kolejnych krokach\n",
    "4. __schedula sampling__\n",
    "  * algorytm uczony jest przewidywania _kilku_ obiektów w przód\n",
    "  * przekazywanie jednej odpowiedzi $y_{t-1}$ nie powoduje niestabilności, nawet jeśli rzadka\n",
    "5. __professor forcing__\n",
    "  * ukryte neurony stanów są regularyzowane\n",
    "  * też zmniejsza niestabilność"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT koszt i wsteczna propagacja\n",
    "<img src=\"../nn_figures/rnn.jpeg\" width=\"70%\"> [Nature]\n",
    "\n",
    "<img src=\"../nn_figures/rnn-bptt1.png\" width=\"70%\"> [Nature]\n",
    "\n",
    "1. koszt\n",
    "$$E(y, \\widehat{y})=\\sum_tE_t(y_t,\\widehat{y}_t)$$\n",
    "2. dla $z_3=Vs_3$ mamy\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_3}{\\partial V} &= \\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial\\widehat{y}_3}{\\partial V}\\\\\n",
    "&=\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial\\widehat{y}_3}{\\partial z_3} \\frac{\\partial z_3}{\\partial V}\\\\\n",
    "&=(\\widehat{y}_3-y_3)\\otimes s_3\n",
    "\\end{align}$$\n",
    "3. dla pochodnej po $W$ zaczyna się pojawiać rekurencja\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_3}{\\partial W} &= \\frac{\\partial E_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial W}\\\\\n",
    "&= \\frac{\\partial E_3}{\\partial \\widehat{y}_3}\\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial W}\\\\\n",
    "&\\hskip3em\\text{jednak $s_3$ bezpośrednio zależy od $s_2$, które nie jest stałe!}\\\\\n",
    "s_3&=\\tanh(U x_t+W s_2)\\\\\n",
    "\\frac{\\partial E_3}{\\partial W} &=\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial s_t}\\frac{\\partial s_t}{\\partial W}\\\\\n",
    "\\end{align}$$\n",
    "<img src=\"../nn_figures/rnn-bptt-gradients.png\" width=\"70%\"> [Nature]\n",
    "4. w rzeczywistości BPTT niewiele się różni od zwykłej wstecznej propagacji\n",
    "  * w sieci warstwowej parametry między warstwami __nie są__ dzielone\n",
    "  * nie ma potrzeby ich sumowania\n",
    "  * w analogiczny sposób można zdefiniować regułę delta\n",
    "  $$\\delta^3_2=\\frac{\\partial E_3}{\\partial z_2}=\\frac{\\partial E_3}{\\partial s_3}\\frac{\\partial s_3}{\\partial s_2}\\frac{\\partial s_2}{\\partial s_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT i zanikający gradient\n",
    "1. podstawowym problemem w uczeniu jest zanikanie gradientu\n",
    "  * problem zauważył Hochreiter, który był autorem modelu LSTM\n",
    "$$\\frac{\\partial E_3}{\\partial W} =\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\frac{\\partial s_3}{\\partial s_t}\\frac{\\partial s_t}{\\partial W}$$\n",
    "2. w rozwiązaniu występuje czynnik\n",
    "$$\\frac{\\partial s_3}{\\partial s_t}$$\n",
    "  * i tak chociażby $$\\frac{\\partial s_3}{\\partial s_1} = \\frac{\\partial s_3}{\\partial s_2}\\frac{\\partial s_2}{\\partial s_1}$$\n",
    "  * skąd mamy\n",
    "  $$\\frac{\\partial E_3}{\\partial W} =\\sum_{t=0}^3\\frac{\\partial E_3}{\\partial \\widehat{y}_3} \\frac{\\partial \\widehat{y}_3}{\\partial s_3} \\left(\\prod_{j=t+1}\\frac{\\partial s_j}{\\partial s_{j-1}}\\right)\\frac{\\partial s_t}{\\partial W}$$\n",
    "  * $s_t=\\tanh(Ux_t+Ws{t-1})$\n",
    "  * $\\tanh()$ ma obszar saturacji po lewej i prawej stronie, a jego gradient maleje __eksponencjalnie__ szybko\n",
    "  * jeśli aktywacje są daleko od własciwych, to gradient spada prawie do zera\n",
    "  * wymnażanie bardzo małych wartosci tylko eksponencjalnie szybko je jeszcze zmniejsza...\n",
    "3. eksplodujący gradient pojawia się równie często\n",
    "  * jest efektem kilku wysokich aktywacji\n",
    "  * może prowadzić do oscylacji, gdy nadchodzące sygnały są sprzeczne\n",
    "  * w miarę łatwo sobie z nim poradzić przez obcinanie gradientu z wysoką normą\n",
    "4. a jak z zanikającym gradientem?\n",
    "  * trudniej: sieć nie uczy się wale albo potrzebuje wykładniczo wiele czasu\n",
    "  * poprawna inicjalizacja\n",
    "  * ReLU zamiast funkcji sigmoidalnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemy\n",
    "1. __krótka__ a __długa__ pamięć\n",
    "  * RNN z algorytmem typu BPTT szybko ___zapomina___ informacje\n",
    "  * korzysta tylko z ostatniej\n",
    "  * model dla angielskiego na poziomie znaków szybko nauczy się, że po znaku `q` __zawsze__ występuje znak `u`\n",
    "  * jednak nie nauczy się informacji kontekstowej z poprzedniego zdania\n",
    "2. wbrew pozorom można łatwo nauczyć model generowania zdań, patrz np. [prosty model Andreya Karpathiego](https://gist.github.com/karpathy/d4dee566867f8291f086)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
