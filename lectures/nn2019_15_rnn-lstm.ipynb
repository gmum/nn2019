{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>LSTM</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Time Memory LSTM\n",
    "1. w 1991 Sepp Hochreiter obronił pracę dyplomową w Monachium w której \n",
    "  * przedstawił szczegółową analizę uczenia sieci rekurencyjnych\n",
    "  * odkrył zjawisko zanikajacego i eksplodujacego gradientu\n",
    "2. w 1997 zaproponował, wraz z Jurgenem Schmidhuberem, model LSTM \n",
    "  * Neural Computation:9(8):1735-1780 \n",
    "  * wcześniej w 1995 w _technical document_ w Monachium\n",
    "3. model LSTM stał się początkiem dla wielu innych modeli\n",
    "  * rozwinięcia LSTM, np. GRU (dodatkowe bramki, inny przepływ, etc.)\n",
    "  * sieci warstwowe typu Highway\n",
    "  * sieci ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (za Hochreiterem)\n",
    "1. jako podstawowy problem Hochreiter zauważył ___zanikający sygnał błędu___\n",
    "  * błąd zanika i sieć nie uczy się niczego w sensownym czasie\n",
    "  * dla sigmoidalnych funkcji aktywacji wagi musiałyby być większe od $\\approx4$ by sygnał miał wystarczającą wartość\n",
    "  * inicjalizacje wag do wyższych wartości nie pomagają, bo odpowiednia pochodna maleje jeszcze szybciej\n",
    "  * BPTT jest bardzo czuły na ostatnie zmiany/sygnały\n",
    "  * Hochreiter zauważył, że konieczne jest zapewnienienie stałego przepływu sygnału błędu\n",
    "    * wniosek: funkcja aktywacji __musi być liniowa__\n",
    "2. komórka LSTM\n",
    "\n",
    "<img src=\"../nn_figures/LSTM-Hochreiter.pdf\" width=\"80%\"> [Neural Computation]\n",
    "\n",
    " 1. wejście --> \n",
    " 2. zgniatanie wejścia --> \n",
    " 3. bramkowanie wejścia sygnałem bramki --> \n",
    " 4. zapamiętywanie i zapominanie --> \n",
    " 5. zgniatanie wyjścia --> \n",
    " 6. bramkowanie wyjścia\n",
    "\n",
    "<img src=\"../nn_figures/gers_lstm.png\" width=\"80%\"> [Hochreiter]\n",
    "\n",
    "  * dodaje (multiplikatywną) __bramkę wejściową__ $in$, która ma zabezpieczyć zawartość pamięci od wpływu _nieistotnych_ wejść\n",
    "  * analogicznie __bramkę wyjściową__ $out$ mającą zabezpieczyć inne komórki przed wpływem (aktualnych) nieistotnych informacji w komórce\n",
    "  * __czemu bramki ?__\n",
    "    * bramka wejsciowa $in$ kontroluje przepływ sygnału błędu by zabezpieczyć przed konfliktami wag\n",
    "      * czasem komórka _powinna_ uzyć wejścia z innej komórki\n",
    "      * czasem nie\n",
    "      * bramka wejsciowa kontroluje to\n",
    "    * podobnie bramka $out$ kontroluje wagi wyjściowe\n",
    "    * bramki wejściowa/wyjściowa muszą się __nauczyć__, które sygnały wyłapać/zablokować"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM przykład (za Hochreiterem)\n",
    "<img src=\"../nn_figures/LSTM-Hochreiter-flow.pdf\" width=\"80%\"> [Neural Computation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM krok po kroku (za Christopher Olah)\n",
    "### bramka wejściowa\n",
    "\n",
    "<img src=\"../nn_figures/LSTM3-focus-f.png\" width=\"80%\"> [Colah]\n",
    "  * sigmoidalna bramka wejściowa decyduje __które__ informacje będą aktualizowane\n",
    "    * wartość poprzedniej pamieci jest __wymnażana__ przez $f_t$\n",
    "  * $f_t$ jest bramką _decydującą_ które informacje wejściowe są istotne i należy je zapamiętać\n",
    "    * pojawia się cudzysłów otwierający, trzeba zapamiętać ten stan\n",
    "      \n",
    "\n",
    "### propozycja i aktualizacja stanu\n",
    "<img src=\"../nn_figures/LSTM3-focus-i.png\" width=\"80%\"> [Colah]\n",
    "  * komórka decyduje które wartości z wejścia należy _dodać_ do tej aktualizowanej\n",
    "    * wartości $i_t*\\widetilde{C}_t$ są dodawane do poprzednio wyczyszczonej i odpowiednio przeskalowane\n",
    "    * to decyduje, których informacji z wejścia (i stanu pamięci) należy teraz użyć\n",
    "  * te wartości połączone decydują o wyjściu komórki $C_t$\n",
    "  * tutaj porzucane są informacje zapominane\n",
    "    * pojawia się cudzysłów zamykający, więc zapominamy informację o tym stanie\n",
    "\n",
    "\n",
    "<img src=\"../nn_figures/LSTM3-focus-C.png\" width=\"80%\"> [Colah]\n",
    "\n",
    "### wyjście\n",
    "\n",
    "<img src=\"../nn_figures/LSTM3-focus-o.png\" width=\"80%\"> [Colah]\n",
    "  * po pierwsze bramka __wyjściowa__ (sigmoidalna) decydująca, które elementy stanu należy przekazać na wyjście $h_t$\n",
    "  * stan komórki jest reskalowany do $(-1,+1)$ przez $\\tanh$\n",
    "  * przeskalowany stan komórki jest filtrowany przez bramkę wyjściową\n",
    "  \n",
    "## warianty\n",
    "#### bramki wykorzystują wgląd (tzw. peephole) w stan komórki\n",
    "\n",
    "<img src=\"../nn_figures/LSTM3-var-peepholes.png\" width=\"80%\"> [Colah]\n",
    "  * wykorzystanie aktualnego stanu na każdym kroku\n",
    "    * kontekstowa praca poszczególnych bramek\n",
    "  * nie zawsze wykorzystywane przez wszystkie bramki\n",
    "\n",
    "#### połączenie bramek zapominającej i wejściowej\n",
    "\n",
    "<img src=\"../nn_figures/LSTM3-var-tied.png\" width=\"80%\"> [Colah]\n",
    "  * zapomina jedynie te składniki, w które zostanie wstawiona nowa informacja\n",
    "\n",
    "#### Gated Recurrent Unit (Cho, 2014)\n",
    "\n",
    "<img src=\"../nn_figures/LSTM3-var-GRU.png\" width=\"80%\"> [Colah]\n",
    "  * spore uproszczenie, a przez to staje się popularna\n",
    "  * połączenie bramek zapiminającej i wejściowej\n",
    "  * łączy stan komórki $C_t$ wraz ze stanem ukrytym komórki $h_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jakie parametry?\n",
    "1. uczenie sieci LSTM nie jest trudne\n",
    "  * jest bardzo wiele wariantów\n",
    "  * architektura nie jest oczywista i ma wiele składników możliwe, że na wiarę\n",
    "  * które są najlepsze i jak je uczyć?\n",
    "2. Rafał Jozefowicz, Zaremba i Sutskever wykonali duży przegląd możliwych architektur wyciągając szereg wniosków\n",
    "  * istnieje wiele architektur podobnych do GRU lepszych na wielu zadaniach\n",
    "  * LSTM: jest generalnie najlepszy jeśli użyty był Dropout\n",
    "  * LSTM: dodawanie jedynki do bramki zapominającej zwykle minimalizuje różnicę do najlepszych architektur\n",
    "    * typowa inicjalizacja ustawia wszystkie wagi na małe wartości\n",
    "    * inicjalizacja biasu bramki zapominajacej na małą wartość sprzyja pojawieniu się zanikającego gradientu\n",
    "    * rozwiązaniem może być inicjalizacja go na wyższe wartości rzędu $1 - 2$\n",
    "  * istotność bramek\n",
    "    * bramka wejściowa __jest__ istotna\n",
    "    * bramka wyjściowa __nie jest__ istotna\n",
    "    * bramka zapominająca __jest bardzo__ istotna dla wszystkich problemów __poza__ modelowaniem języka\n",
    "3. autorzy wykorzystali prostą procedurę przeszukiwania\n",
    "  * utrzymywania listy 100 znalezionych najlepszych architektur poszukując dla nich najlepszych hiperparametrów\n",
    "  * w każdym etapie wykonuje jeden z kroków\n",
    "    * losuje jedną ze 100 architektur\n",
    "      * ewaluuje 20 losowych ustawień hiperparametrów dla każdego z 3 zadań\n",
    "      * ocenia przez\n",
    "      $$\\min\\frac{\\text{najlepsza dokładność dla architektury dla zadania}}{\\text{najlepszy wynik GRU dla zadania}}$$\n",
    "      przy czym GRU były wyliczone dla wszystkich dozwolonych architektur\n",
    "    * wybiera jedną ze 100 architektur\n",
    "      * mutuje parametry\n",
    "  * autorzy\n",
    "    * ewaluowali 10 tysięcy różnych architektur\n",
    "    * 1000 z nich przeszło początkowy test zapamiętywania\n",
    "      * 5 znaków w sekwencji, dla wszystkich 26 możliwości\n",
    "      * jest czytanych w sekwencji\n",
    "      * i ma być odtworzone w tej samej sekwencji\n",
    "    * te 1000 architektur było sprawdzone dla średnio 2200 konfiguracji\n",
    "    * razem wykonali testy dla ok. 230 tysiecy konfiguracji\n",
    "    * zadania\n",
    "      * obliczenie sumy (w postaci znaków) dla sekwencji składajacej się z dwóch sekwencji\n",
    "      * predykcja następnego znaku w kodzie XML\n",
    "      * modelowanie języka Penn Tree-Bank\n",
    "      * (dodatkowo) modelowanie muzyki polifonicznej\n",
    "      \n",
    "  * rezultaty\n",
    "    * GRU było lepsze od LSTM na wszystkich zadaniach poza modelowaniem języka\n",
    "    * jeśli wykorzystano dropout, to LSTM było zdecydowanie najlepsze dla problemów modelowania jezyka\n",
    "    * LSTM z wysokim biasem bramki zapominajęcej było lepsze od innych LSTM i GRU na prawie wszystkich zadaniach\n",
    "    * trzy mutacje znalezione okazały się konkurencyjne do innych modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wielkość ukrytej warstwy\n",
    "1. $x_t$ i $h_t$ są wektorami\n",
    "  * jeśli rozpoznajemy znaki, to $h_t$ będzie wektorem prawdopodobieństw znaków\n",
    "  * w przypadku słów, trzeba wykorzystać jakiś word-embedding, by zredukować wymiarowość\n",
    "    * word-embedding pozwoli reprezentować słowa jako wektory w $\\mathbb{R}^K$\n",
    "    * word-embedding może być uczony __w trakcie__ uczenia modelu\n",
    "2. modele LSTM zawierają pojęcie __hidden layer__\n",
    "  * warstwa ukryta jest zbiorem neuronów w bramkach zapominajacych, wejsciowych, wyjsciowych, etc.\n",
    "  * ich wielkość to właśnie warstwa ukryta\n",
    "  * większa warstwa ukryta zapewnia modelowi większą pojemność\n",
    "3. innym typowym parametrem jest żądanie by model zwracał całe sekwencje\n",
    "  * w Keras to parametr `return_sequences`\n",
    " \n",
    "  <img src=\"../nn_figures/Keras-LSTM-return-sequences.png\" width=\"80%\"> [Chollet]\n",
    "  * to pozwala budować sieci __wielopoziomowe__\n",
    "  <img src=\"../nn_figures/LSTM-two-layer.png\" width=\"80%\"> [Colah]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci Highway\n",
    "1. Dla wielu problemów __głębsze__ sieci dają __lepsze__ wyniki\n",
    "  * wciąż problemem jest __zanikający gradient__\n",
    "  * potrzebne nowe modele sieci radzące sobie z tym\n",
    "  * rozwiązaniem może być wykorzystanie __bramek zapominających__ wziętych z LSTM\n",
    "    * początkowe rozwiązania Srivastavy były znacznie bardziej oparte na LSTM\n",
    "    * końcowe jest dużym uproszczeniem\n",
    "2. różnymi podejściami są\n",
    "  * lepsze algorytmy uczące, inicjalizacje, lepiej dopasowane funkcje aktywacji\n",
    "  * połączenia skrótowe\n",
    "    * zawsze obecne w badaniach połączenie między warstwami\n",
    "      * przykładem sieć dla XOR\n",
    "    * połączenie __do__ warstw wyjściowych dodające szum\n",
    "    * dodatkowe funkcje kosztu w trakcie uczenie (patrz Inception)\n",
    "2. aktywacja nowej warstwy\n",
    "  * jest częściowo kopią aktywacji poprzedniej (a więc wejścia)\n",
    "  * częściowo nowo wyliczoną wartością\n",
    "  * połączenie jako kombinacja liniowa\n",
    "  * parametry kombinacji są __uczone__\n",
    "  $$\\begin{align}\n",
    "  y_i &= T(x, w_T)\\,\\,\\sigma\\left(\\sum_{j}w_{ij}x_j+b\\right)+(1-T(x; w_T))x\\\\\n",
    "  T&=\\sigma\\left(\\sum_{j}w_{T_{ij}}x_j+b_T\\right)\n",
    "  \\end{align}$$\n",
    "  gdzie $\\sigma()$ jest funkcją logistyczną\n",
    "  * $T$ to bramka __transfomacji__\n",
    "  * $1-T$ to bramka __przeniesienia__ (ang. carry)\n",
    "3. można tak ustawić $b'$, że $T$ będzie zmierzało do zera\n",
    "  * wtedy aktywacja następnej warstwy jest bliska poprzedniej\n",
    "  $$y = \n",
    "  \\begin{cases} x &\\mbox{if } T() = 0 \\\\ \n",
    "  \\sigma\\left(\\sum_{j}w_{ij}x_j+b\\right) & \\mbox{if } T() = 1 \n",
    "  \\end{cases}\n",
    "  $$\n",
    "  * informacja może przechodzić przez wiele warstw __bez jej _rozcieńczenia___\n",
    "  * Srivastava i Schmidhuber nazwali takie ścieżki __information highways__\n",
    "4. definicja wymaga by warstwy miały __takie same wymiary__!\n",
    "  * jeśli zmiana wymiaru jest konieczna, wtedy można\n",
    "    * zamienić wejście $x$ na __rozszerzone__ przez odpowiednie\n",
    "      * dopełnienie zerami\n",
    "      * odpowiednie wycięcie fragmentu\n",
    "    * dodać __zwykłą__ warstwę i później kontynuować warstwy typu highway\n",
    "  * działa podobnie dla sieci konwolucyjnych\n",
    "  * super! ale nic za darmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uczenie sieci Highway\n",
    "\n",
    "<img src=\"../nn_figures/highway-layers.pdf\" width=\"90%\"> [Srivastava]\n",
    "1. aktualizacja wag wykorzystując SGD z momentum\n",
    "2. inicjalizacja\n",
    "  * inicjalizacja $w_T, b_T$ __niezależna__ od inicjalizacji $w,b$\n",
    "  * $b_T$ inicjalizowane na nieduże wartosci ujemne rzędu $-1 \\ldots -10$\n",
    "    * powoduje na początku uczenia zachowanie w kierunku czynnika przeniesienia (carry)\n",
    "    * z początku większość informacji jest kopiowana\n",
    "    * analogiczne do sugestii inicjalizacji bramki zapominania w sieciach LSTM\n",
    "<img src=\"../nn_figures/highway-weights.pdf\" width=\"90%\"> [Srivastava]\n",
    "2. najlepsze 50 sieci dla MNIST i CIFAR-100\n",
    "  * zerowa warstwa jest zwykła i zmniejsza wymiar do stałego wymiaru 50\n",
    "    * kolejno: bias bramki transformacji, średnie wyjście z bramki transformacji (dla 10 tysiecy przykładów), \n",
    "    * i dalej: przykładowe wyjscie dla bramki transformacji i całkowite wyjście dla losowego przykładu\n",
    "  * biasy bramki transformacji __rosną__ wraz z głębokością\n",
    "* średnie wyjścia bramek transformacji __zanikają__ wraz z głębokością\n",
    "  * czynnik przeniesienia staje się istotniejszy wraz z odległoscią od wejścia\n",
    "  \n",
    "3. istotność warstw w modelu\n",
    "\n",
    "<img src=\"../nn_figures/highway-warstwy.pdf\" width=\"80%\"> [Srivastava]\n",
    "  * jaka jest istotność warstw w modelu?\n",
    "  * uszkodzenie pojeynczej warstwy przez ustawienie bramki transformacji pojedynczej warstwy na 0\n",
    "    * wtedy sieć kopiuje wejście\n",
    "    * dla problemu MNIST wyraźnie widać, że od pewnego momentu problem jest już praktycznie rozwiązany\n",
    "      * pod koniec jest pewna strata, wynikająca zapewne z konieczności zsumowania wyników (ostatnia wyjściowa warstwa to softmax)\n",
    "    * CIFAR-100 jest znacznie trudniejszy, co wyraźnie widać\n",
    "  * to pokazuje, że nadmiar warstw __nie jest__ problemem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask RCNN\n",
    "<img src=\"../nn_figures/mask-rcnn.pdf\" width=\"70%\" align=\"center\"> [He et al.]\n",
    "\n",
    "1. Region Proposal Network RPN wyszukuje obszary potencjalnie zawierające obiekty\n",
    "2. w drugim etapie MASK-RCNN\n",
    "  * przeewiduje klasę\n",
    "  * bounding box\n",
    "  * maskę\n",
    "<img src=\"../nn_figures/mask-rcnn-result.pdf\" width=\"90%\"> [He et al.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci rezydualne z losowo usuwanymi warstwami (Huang et al.)\n",
    "\n",
    "<img src=\"../nn_figures/highway-warstwy.pdf\" width=\"80%\"> [Srivastava]\n",
    "\n",
    "0. Uczenie bardzo głębokich sieci napotyka na wiele problemów\n",
    "  * zanikające gradienty\n",
    "    * wielokrotne wymnażanie (lub konwolucje) przy małych wagach powoduje brak efektywnosci uczenia wag bliskich wejscia\n",
    "    * potrzebne dobra inicjalizacja, nadzorowane uczenie warstw pośrednich (patrz Inception), Batch Normalization\n",
    "  * zanikające uzycie cech\n",
    "    * cechy wejściowe lub wcześnie wyliczone zanikają wskutek wymnażania wag\n",
    "    * są trudne do uzycia później\n",
    "    * konieczne połączenia skrótowe między wielu warstwami (patrz DenseNet)\n",
    "  *  uczenie może być bardzo czasochłonne ze względu na bardzo dużą liczbę parametrów\n",
    "  * dla wielu problemów wystarczająca jest mniejsza liczba warstw\n",
    "2. A może by część warstw usunąć?\n",
    "  * określić szansę __istotności__ warstwy w trakcie uczenia\n",
    "  * losowo warstwy redukować do identyczności\n",
    "  * przy inferencji uzywać wszystkich\n",
    "3. niech $r\\sim Bernoulli$\n",
    "  $$H_k(x_{k-1}) = ReLU(r_k F_k(x_{k-1}) + x_{k-1})$$\n",
    "  * dla $r_k=1$ mamy zwykły blok rezydualny\n",
    "  $$H_k(x_{k-1}) = ReLU(F_k(x_{k-1}) + x_{k-1})$$\n",
    "  * dla $r_k=0$ tylko przeniesienie\n",
    "  $$H_k(x_{k-1}) = ReLU(x_{k-1})$$\n",
    "    $$x_{k-1}=H_{k-1}(x_{k-2})=ReLU(r_{k-1} F_k(x_{k-2}) + x_{k-2})$$\n",
    "    stąd\n",
    "    $$\\begin{align}\n",
    "    H_k(x_{k-1})=ReLU(x_{k-1})&=ReLU(ReLU(r_{k-1} F_k(x_{k-2}) + x_{k-2}))\\\\\n",
    "    &=ReLU(r_{k-1} F_k(x_{k-2}) + x_{k-2})\\\\\n",
    "    &=x_{k-1}\\\\\n",
    "    &=H_{k-1}(x_{k-1})\n",
    "    \\end{align}$$\n",
    "4. Autorzy proponują używać __prawdopodobieństwa przeżycia__ $p_k$ dla rozkładu Bernoulliego zmniejszanego liniowo wraz z numerem warstwy\n",
    "$$p_k=1-\\frac{k}{K}(1-p_K)$$\n",
    "\n",
    "  <img src=\"../nn_figures/resnet-stochastic.pdf\" width=\"80%\"> [Huang et al.]\n",
    "\n",
    "  * im dalej od wejscia, tym większa szansa na opuszczenie\n",
    "    * zgodne z obserwacjami w sieciach Highway\n",
    "  * warstwy opuszczane dla każdego mini-batcha\n",
    "  * autorzy używają $p_K=0.5$\n",
    "    * \n",
    "    * uczenie okazuje się stabilne\n",
    "  * możliwe jest użycie stałego $p_k=p_K$\n",
    "    * zwykle większe przyspieszenie (czemu???)\n",
    "    * jednak zdecydowanie słabsze wyniki\n",
    "      * dopiero wysoka wartość $p_K$ daje znaczący spadek błędu testowania, jednak porównywalny dla tego z liniowym wzrostem $p_k$ dla znacznie mnieszego $p_K$\n",
    "    * $p_K=0.5$ daje najlepsze wyniki dla reguły liniowej\n",
    "    * a czy możliwe jest __adaptacyjne__ modyfikowanie $p_k$???\n",
    "      * warstwy blisko końcowej mają większe znaczenie\n",
    "      * rozpocząć do reguły liniowej\n",
    "      * co ustaloną liczbę kroków wykonywać test amputacji warstw\n",
    "      * odpowiednio zwiększać/zmniejszać $p_k$\n",
    "5. Inferencja\n",
    "  * w trakcie poszczególne $F_k()$ są wykorzystywane __losowo__ proporcjonalnie do $p_k$\n",
    "  * inferencja wymaga przewtorzenia \n",
    "  $$\\widehat{H}_k(\\widehat{x}_{k-1}) = ReLU(p_k F_k(\\widehat{x}_{k-1}) + \\widehat{x}_{k-1})$$\n",
    "5. efekty?\n",
    "  * lepsze wyniki \n",
    "    * błąd na zbiorze trenującym jest __wyższy__ niż ten dla pełnej sieci\n",
    "    * bład na zbiorze testujacym jest __niższy__ dając jedne z [najlepszych znanych wyników](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)\n",
    "      * dla CIFAR-10 $5.25\\%$ o ponad $1\\%$ (ResNet-110)\n",
    "      * dla CIFAR-100 $24.98\\%$ o prawie $3\\%$ (ResNet-110)\n",
    "  * efekty dla ekstremalnie głębokich sieci\n",
    "    * ResNet o głębokości $1202$ warstw dawał wyniki __słabsze__ niż ResNet-110 dla CIFAR-10\n",
    "      * overfitting\n",
    "    * stochastyczne usuwanie warstw na takiej sieci dało __spadek__ błędu ($4.91\\%$, spadek o ok. $0.3\\%$)\n",
    "  * znaczne przyspieszenie uczenia\n",
    "    * czas uczenia jest proporcjonalny do __oczekiwanej__ głębokości sieci, tzn. liczby pozostawionych warstw\n",
    "      $$\\mathbb{E}(\\overline{K})=\\sum_{k=1}^Kp_k$$\n",
    "    * dla $p_K=0.5$ mamy $$\\mathbb{E}(\\overline{K})\\simeq3/4K$$\n",
    "  * dobry wpływ na wielkość gradientu\n",
    "    * norma gradientu sieci stochastycznej była stale większa od normy dla sieci o stałej długości\n",
    "  * efekty przypominając Dropout\n",
    "    * jednak usuwane są __warstwy__, nie pojedyncze neurony\n",
    "    * generowanych jest losowych $2^K$ sieci"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
