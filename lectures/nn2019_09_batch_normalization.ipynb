{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Batch normalization</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "* ciagła zmiana rozkład wejść do neuronów w warstwach spowalnia uczenie\n",
    "  * tzw. __internal covariate shift__\n",
    "* BN przeciwdziała\n",
    "* pozwala na użycie wyższych współczynników uczenia\n",
    "* problem może być przedstawiony na rózne sposoby\n",
    "  * wykrywanie określonych obiektów na obrazach kolorowych i na czrno białych\n",
    "  * funkcja rozdzielająca jest w zasadzie ta sama (lub bardzo podobna)\n",
    "  * prosta transfomacja z kolorowych do czarno białych (na przykład) może pomóc\n",
    "  * to jest przesunięcie dystrybucji\n",
    "* niech sieć będzie głęboka\n",
    "  * aktywacje k-tej warstwy są uzależnione od poprzednich wag\n",
    "  * jeśli będą się one mniej zmieniać, to aktywacje na k-tym poziomie będą bardziej stabilne i mniej czułe na zmiany\n",
    "  * batchnorm będzie to zapewniać"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization\n",
    "1. uczenie w mini-batchach\n",
    "  * mini-batche polepszają ewaluację gradientu\n",
    "    * gradient dla mini-batchu jest estymacją gradientu obliczonego dla całego zbioru\n",
    "  * uczenie szybsze niż przy pojedynczych przykładach, szczególnie dla GPU\n",
    "2. uczenie sgd wymaga dopasowania wielu hiper-parametrów oraz dobrej inicjalizacji\n",
    "3. __covariate shift__\n",
    "  * sieć ma wiele warstw\n",
    "  * wejścia do neuronów są zdeterminowane przez __wiele__ neuronów warstwy poprzedniej\n",
    "  * rozkład danych dla warstwy wejściowej jest stały (dla danego zbioru)\n",
    "  * w trakcie uczenia rozkład dla warstw kolejnych zmienia się\n",
    "  * ta zmiana tym bardziej im głębsza jest sieć\n",
    "  * wiele kolejnych warstw to jak złożenie funkcji, przy czym wewnątrzna __zmienia się__ w trakcie uczenia \n",
    "  * czyli covariate shift to zmiana rozkładu aktywacji spowodowana zmianą parametrów (wag) sieci w procesie uczenia\n",
    "4. niech warstwa $Z$ ma wejście z warstwy $Y$: $$z=g(w^Ty+b)$$ a $g$ sigmoidalna\n",
    "  * wzrost wartości bezwzględnej $|w^Ty+b|$ prowadzi $g()$ do obszarów o niskim gradiencie\n",
    "  * spowoduje to ruch wielu wymiarów w kierunku saturacji\n",
    "5. użycie ReLU, dobrej inicjalizacji, małych prędkości może pomóc\n",
    "6. BN\n",
    "  * może pozwolic na użycie nieliniowości sigmoidalnych\n",
    "  * powinno przyspieszyć uczenie\n",
    "    * pojedyncze epoki są wolniejsze, ale zbieżność szybsza\n",
    "    * możliwe większe prędkosci uczenia\n",
    "    * większa odporność na niedobrą inicjalizację\n",
    "    * zmniejszy wpływ zanikającego gradientu\n",
    "    * pozwala na użycie większej liczby rodzajów funkcji aktywacji\n",
    "    * ułatwia projektowanie sieci\n",
    "    * jest rodzajem generalizacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Założenia\n",
    "1. dawno wskazywano, że uczenie jest szybsze gdy wejścia są ___whitened___ (LeCun)\n",
    "  * mają zerowe średnie\n",
    "  * jednostkowe wariancje\n",
    "  * są zdekorelowane\n",
    "2. dość proste do realizacji dla danych wejściowych (chociaż pracochłonne)\n",
    "  * niech __wszystkie__ wejścia mają dodatnie wartości\n",
    "  * wszystkie wagi neuronów w pierwszej warstwie będą się __wspólnie__ zwiększać lub zmniejszać\n",
    "  * ścieżka do minimum będzie powolna \n",
    "  * kroki przetwarzania wejść\n",
    "    * usunięcie średnich\n",
    "    * dekorelacja (np. PCA)\n",
    "    * wyrównanie kowariancji by dla różnych wejść były w przybliżeniu równe\n",
    "    * byłoby fajnie dla kazdej warstwy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trudności\n",
    "1. wyrównywanie rozkładów będzie robione równolegle z modyfikacją wag\n",
    "2. niech dla zbioru przykładów $X$ \n",
    "  * algorytm centruje wejścia \n",
    "  $$\\hat{y}=y-\\mathbb{E}[y]\\hskip2em\\text{dla}\\;y=x+b,\\;\\mathbb{E}[y]=\\frac{1}{n}\\sum_iy_i$$\n",
    "  * SGD _ignoruje_ istnienie centrowania i poprawia bias $b$\n",
    "  $$\\begin{align}\n",
    "  b&=b+\\Delta{}b\\\\\n",
    "  \\Delta{}b&\\propto -\\frac{\\partial L}{\\partial \\hat{y}}\n",
    "  \\end{align}$$\n",
    "  * wtedy\n",
    "  $$\\begin{align}y&=x+(b+\\Delta b)-\\mathbb{E}[x+(b+\\Delta b)]\\\\\n",
    "  &=x+b-\\mathbb{E}[x+b]\n",
    "  \\end{align}$$\n",
    "  bez żadnego wpływu\n",
    "  * $b$ będzie tylko rosnąć\n",
    "  * centrowanie i normalizacja muszą __współpracować__ ze sobą\n",
    "3. trzeba __zapewnić__ by dla dowolnych warstości parametrów (wag) model __zawsze__ generował aktywacje o żądanych rozkładach\n",
    "  * to pozwoli by gradient kosztu po parametrach uwzględniał normalizację i jej wpływ na wagi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pełna normalizacja jest trudna __dla wszystkich cech__\n",
    "  * niech $x$ będzie wyjściem z danej warstwy, wtedy normalizacja\n",
    "  $$\\widehat{x}=Norm(x,X)$$\n",
    "  gdzie $X$ jest całym zbiorem\n",
    "  * to wymaga policzenia dla wstecznej propagacji Jakobianów\n",
    "  $$\\frac{\\partial\\,Norm(x,X)}{\\partial\\,x}\\hskip{2em}\\text{oraz}\\hskip{2em}\\frac{\\partial\\,Norm(x,X)}{\\partial\\,x}$$\n",
    "  * potem macierzy kowariancji $Cov[x]$ oraz $Cov[x]^{-1/2}$\n",
    "  * to jest co najmniej pracochłonne\n",
    "2. potrzebne jest rozwiązanie alternatywne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "1. ___uproszczenie___ normalizacja dla każdej $k$-tej cechy osobno\n",
    "$$\\widehat{x}_k=\\frac{x_k-E[x_k]}{\\sqrt{Var[x_k]}}$$\n",
    "  * przyspiesza nawet jeśli cechy są od siebie zależne\n",
    "    * nie są zdekorelowane\n",
    "2. BN zmienia postać funkcji uczonej przez warstwę przyjmującą $x$\n",
    "  * normalizacja wejść sigmoidu ograniczy do obszaru liniowego funkcji aktywacji\n",
    "  * aby __warstwa BN__ potrafiła wykonywać identyczność uczymy\n",
    "  $$y_k=\\gamma_k\\widehat{x}_k+\\beta_k$$\n",
    "    * to pozwoli odtworzyć oryginalne aktywacje, jeśli okażą się optymalne\n",
    "      * $\\beta=\\mathbb{E}[x]$\n",
    "      * $\\gamma=\\sqrt{Var[x]}$\n",
    "      * parametry są uczone równolegle z wagami\n",
    "3. ___uproszczenie___ każdy __mini-batch__ estymuje średnią i wariancję każdej aktywacji\n",
    "  * efektywniejsze niż dla całego zbioru\n",
    "    * psuje efekt SGD\n",
    "  * każdy mini-batch generuje estymacje średniej i wariancji dla każdej aktywacji\n",
    "  * mini-batch często (zwykle) jest mniejszy niż liczba cech\n",
    "    * obliczanie pełnej kowariancji wymagałoby regularyzacji\n",
    "4. Obliczenia dla mini-batchu $B$ o $m$ przykładach (dla pojedynczej cechy $i$)\n",
    "$$\\begin{align}\n",
    "\\mu_B&=\\frac{1}{m}\\sum_i^mx_i\\\\\n",
    "\\sigma_B^2&=\\frac{1}{m}\\sum_i^m(x_i-\\mu_B)\\\\\n",
    "\\widehat{x}_i&=\\frac{x_i-\\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}}\\\\\n",
    "y_i&=\\gamma\\widehat{x}_i+\\beta\n",
    "\\end{align}$$\n",
    "  * końcową transformacją BN() jest $$y_k=BN_{\\gamma, \\beta}(x_k)=\\gamma_k\\widehat{x}_k+\\beta_k$$\n",
    "  gdzie $y_k$ są wyjściami z warstwy BN\n",
    "  * wartości $\\widehat{x}_i$ są __wewnątrz__ warstwy BN, ale to ich cechy są istotne\n",
    "    * $\\widehat{x}_i$ mają średnią $0$ i wariancję $1$\n",
    "    * wyjście $y$ staje się wejściem do liniowej sieci $\\gamma\\widehat{x}_i+\\beta$\n",
    "    * potem następują kolejne warstwy oryginalnej sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization a SGD\n",
    "1. użycie BN wymaga uwzglednienia w algorytmie uczenia\n",
    "  * transformacje są proste, gradienty proste do wyliczenia\n",
    "  * możliwe do uczenia\n",
    "  * transformacja liniowa - sieć zachowuje swoją pojemność\n",
    "2. przekształcenia BN __nie są__ potrzebne w trakcie inferencji\n",
    "  * po fazie uczenia stosujemy w warstwie BN normalizację __na całym zbiorze uczącym__\n",
    "  $$\\widehat{x}=\\frac{x-E[x]}{\\sqrt{Var[x]}}$$\n",
    "3. cały algorytm\n",
    "  1. uczenie warstwy BN\n",
    "  2. dodanie transformacji $y_k=BN_{\\gamma_k,\\beta_k}(x_k)$\n",
    "  3. użycie $y_k$ zamiast $x_k$\n",
    "  4. uczenie calej sieci\n",
    "  5. w sieci inferencji chwilowo obliczone parametry $\\gamma,\\beta$\n",
    "  6. po zakończeniu uczenia uśrednianie (moving window) parametrów po wielu mini-batchach\n",
    "  $$E[x]=E_B[\\mu_B]\\hskip{2em}Var[x]=\\frac{m}{m-1}E_B[\\sigma_B^2]$$\n",
    "  gdzie $\\mu_B, \\sigma^2_B$ to średnia i wariancja dla mini-batchu $B$\n",
    "  7. w warstwie BN zastąpienie $y=BN_{\\gamma,\\beta}(x)$ przez\n",
    "  $$y=\\frac{\\gamma}{\\sqrt{Var[x]+\\epsilon}}x+\\left(\\beta-\\frac{\\gamma\\,E[x]}{Var[x]+\\epsilon}\\right)$$\n",
    "4. w trakcie ewaluacji wyników (inferencji) wykorzystujemy normalizację\n",
    "$$\\hat{x}=\\frac{x-\\mathbb{E}[x]}{\\sqrt{Var[x]+\\epsilon}}$$\n",
    "wykorzystując statystyki dla całej populacji zamiast dla mini-batchy\n",
    "  * te znormalizowane aktywacje mają takie same wartości jak w trakcie uczenia\n",
    "5. BN można wykorzystać dla różnych rodzajów sieci, także dla konwo2lucyjnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efekt regularyzacji\n",
    "1. średnie i wariancje są obliczane na mini-batchach i przez podejście moving window\n",
    "2. są to estymacje\n",
    "3. modeyfikacje z ich wykorzystaniem będą dodawać pewien niewielki szum\n",
    "4. to dodaje pewien (niewielki) efekt regularyzacyjny\n",
    "  * wielkość zależna od wielkości mini-batchów\n",
    "  * to tylko dodatkowy efekt, nie podstawowe zadanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czy BN działa?\n",
    "* aby sieć była zbieżna, konieczne jest dobre modyfikowanie parametrów, inicjalizacja, wiele dodatkowych rzeczy\n",
    "* jeśli dodamy warstwy BN\n",
    "  * statystyki warstw są sterowane (odpowiednimi) współczynnikami $\\gamma$ (wariancja) i $\\beta$ (średnia)\n",
    "  * algorytm może teraz kontrolować w dużym stopniu właśnie $\\beta, \\gamma$ w zamian za wszystkie wagi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wpływ BN na prędkość uczenia\n",
    "* zbyt szybkie uczenie zwykle prowadzi do wybuchu lub zaniania gradientów\n",
    "* normalizacja aktywacji w sieci zabezpiecza małe zmiany przed przesunięciem aktywacji do obszarów saturacji i zanikaniu uczenia\n",
    "* także zabezpiecza przed złym wpływem zbyt dużych współczynników uczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gdzie umieścić BN?\n",
    "1. w pracy o BN Ioffe i Szegedy napisali, że _we add the BN transform immediately before the nonlinearity, by normalizing x=Wu+b_\n",
    "  * normalizujemy $x=w^Tu+b$, by z tego policzyć aktywację $g()$\n",
    "  * właściwie _bias_ może być ominięty, ponieważ normalizacja go niweluje odejmując średnią\n",
    "3. wiele prac pokazuje, że odwrócenie kolejności na warstwa gęsta --> nieliniowość --> BN daje bardzo często (zwykle?) nawet lepsze wyniki\n",
    "2. BN można też wykorzystać w sieciach konwolucyjnych\n",
    "  * trzeba zadbać, by te same cechy w różnych miejscach były normalizowane w ten sam sposób"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zalety\n",
    "1. BN pozwala na wyższe współczynniki uczenia\n",
    "  * zwykle wysokie współczynniki uczenia prowadzą do wybuchu lub też zanikania gradientów\n",
    "  * BN zabezpiecza sieć przed utknięciem w rejonie saturacji\n",
    "2. BN regularyzuje model\n",
    "  * każdy przykład jest uczony w kontekscie swojego mini-batchu\n",
    "  * sieć nie zwraca deterministycznych wartości dla przykładów\n",
    "  * Dropout może być usunięty albo osłabiony\n",
    "    * różne są zdania\n",
    "  * można zredukować regularyzację L2\n",
    "3. przyspiesza uczenie\n",
    "  * zbieżność jest szybsza, co nadrabia z przewagą dodatkowe obliczenia\n",
    "4. potrzebne (konieczne) jest lepsze mieszanie przykładów"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
