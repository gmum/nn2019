{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Uczenie adwersarialne GAN</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele generatywne\n",
    "* stara się nauczyć rzeczywistego rozkładu danych $p_X(x)$ wykorzystując model z parametrami $\\theta$\n",
    "* pierwsze podejście maksymalizuje likelihood $$\\prod_{i=1}^np_\\theta(x^i)$$\n",
    "dla wszystkich przykładów uczących\n",
    "  * problemy z obliczalnością $p(x)$ (patrz VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks GAN\n",
    "* wprowadza ideę __adwersarialnego__ uczenia między __generatorem__ a __dyskryminatorem__\n",
    "  * stają się  adwersarzami (przeciwnikami) w generowaniu przykładów przypominających rzeczywiste\n",
    "  * generator $G:Z\\longrightarrow X$ stara się \"oszukać\" dyskryminator\n",
    "  * dyskryminator $D:X\\longrightarrow [0,1]$ stara się rozpoznać skąd pochodził sygnał: prawdziwy z $X$ czy wygenerowany  $G(z)$ z wylosowanego $z$\n",
    "* GAN nie definiuje $p_\\theta(x)$ bezpośrednio\n",
    "* definicja generatora i dyskryminatora daje ostrzejsze obrazy\n",
    "  * ale są też (poważne) wady"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "* $G$ generuje jak najlepsze elementy ze zmiennej $z\\in Z$\n",
    "* $D$ rozpoznaje czy wejście pochodzi z rzeczywistych obrazów (wysoka wartość wyjścia dyskryminatora) czy jest wygenerowane (niska wartość)\n",
    "* $d$ i $G$ konkurują ze sobą starając się osiągnąć swoje własne cele\n",
    "* uczenie polega na\n",
    "$$\\min_G\\max_D V(G,D)=\\min_G\\max_D \\mathbb{E}_{x\\sim p_X}\n",
    "[\\log D(x)]+\\mathbb{E}_{z\\sim p_Z}[\\log(1-D(G(z)))]$$\n",
    "gdzie \n",
    "  * $p_X=p_{data}$ to prawdziwy, chociaż nieznany, rozkład danych\n",
    "    * $p_{data}$ to częsty opis (może nawet zwykły...)\n",
    "  * $V(G,D)$ to entropia krzyżowa typowa dla binarnej klasyfikacji\n",
    "* na czym polega \"gra\"?\n",
    "  * z opunktu widzenia $D$\n",
    "    * jeśli $x$ pochodzi z prawdziwych danych, to $D$ będzie maksymalizować\n",
    "    * jeśli $x$ pochodzi z $G$, to $D$ będzie minimalizować wyjscie\n",
    "  * z punktu widzenia $G$\n",
    "    * $G$ stara się \"oszukać $D$\n",
    "    * $G$ stara się __maksymalizować__ wyjście $D$, gdy daną dla $D$ jest $G(z)$ dla samplowanego $z$\n",
    "    * tą maksymalizację $G$ osiąga przez coraz lepsze generowanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN i równowaga\n",
    "* jeśli $D$ i $G$ mają wystarczającą pojemność, to __równowaga__ (equilibrium) między nimi następuje gdy $p_X(x)=p_g(x)$\n",
    "* $D$ zwraca wtedy zawsze $1/2$\n",
    "  * generator jest już \"idealny\"\n",
    "  * najlepsze co dyskryminator może robić, to zwracać odpowiedź losową\n",
    "  * dla ustalonego $G$ optymalny dyskryminator $$D^\\ast(x)=\\frac{p_g(x)}{p_g(x)+p_X(x)}$$\n",
    "* wstawiając $D^\\ast$ do równania dostajemy Jensen-Shannon divergence $JSD(p_X\\mid\\mid p_g)$\n",
    "  $$\\begin{align}\n",
    "  JSD(p_X\\mid\\mid p_g)&=\\frac{1}{2}KL(p_X\\mid\\mid M)+\\frac{1}{2}KL(p_g\\mid\\mid M)\\\\\n",
    "  M&=\\frac{1}{2}(p_X+p_g)\n",
    "  \\end{align}$$\n",
    "  * $JSD()$ jest symetryczną i wygładzoną wersją $KL()$\n",
    "  * $0\\leq JSD(p\\mid\\mid q)\\leq1$ dla logarytmu o podstawie 2 w $KL()$\n",
    "* stosunek gęstości prawdopodobieństwa\n",
    "$$Dr(x)=\\frac{p_X(x)}{p_g(x)}=\n",
    "\\frac{p(x\\mid y=1)}{p(x\\mid y=0)}=\n",
    "\\frac{p(y=1\\mid x)}{p(y=0\\mid x)}=\n",
    "\\frac{D^\\ast(x)}{1-D^\\ast(x)}$$\n",
    "pod warunkiem $p(y=0)=p(y=1)$\n",
    "  * GAN __zastępuje__ problem z obliczalnością (intractability) likelihood przez wykorzystanie względnych wartości obu rozkładów\n",
    "  * GAN mierzy niezgodność między generowanym a prawdziwym rozkładem danych\n",
    "  * wykorzystuje do tego dyskryminator\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zalety i problemy GAN\n",
    "1. uczenie gradientowe\n",
    "  * jeśli rozkłady $p_g$ i $p_X$ są rozłączne albo leżą w nisko-wymiarowaym manifoldzie, to istnieje idealny dyskryminator je rozróżniający\n",
    "    * doświadczalnie pokazano, że tak jest często\n",
    "  * gradient $D$ przekazywany do $G$ zanika\n",
    "  * to utrudnia uczenie $G$\n",
    "  * teoretycznie GAN ma gwarancję nauczenia się dla $D\\ast$ aproksymująego $JSD()$, ale to często nie jest implementowane przez algorytm gradientowy\n",
    "2. uczenie iteracyjne\n",
    "  $$\\min_G\\max_D V(G,D)\\neq\\max_D\\min_G V(G,D)$$\n",
    "  * uczenie $D$ aż optymalne dla ustalonego $G$ jest kosztowne\n",
    "  * stąd uczenie $D$ w krokach\n",
    "  * schemat uczenia prowadzi do niepewności czy rozwiązywany jest problem minimax czy maximin\n",
    "    * rozwiązania obu niekoniecznie są równe\n",
    "3. mode collapse\n",
    "  * jeden z największych problemów GAN\n",
    "  * $p_X$ ($p_{data}$) jest zwykle rozkładem multi-modalnym\n",
    "  * $G$ uczy się $p_g$ prostego o kilku modach z nadzieją oszukania dyskryminatora\n",
    "  * $D$ odrzuca to rozwiązanie\n",
    "  * $G$ przerzuca się na inne mody…\n",
    "  * jak sobie z tym radzić?\n",
    "    * przewidywanie w przód reakcji dyskryminatora na poprawki generatora\n",
    "    * ograniczanie gradientów dyskryminatora\n",
    "    * wiele generatorów dla pojedynczego dyskryminatora\n",
    "4. generator i dyskryminator mogą optymalizować tą samą funkcję, ale w przeciwnych kierunkach\n",
    "5. teoretyczne gwarancje są prawdziwe w przestrzeni funkcyjnej, ale implementacja to przestrzeń parametrów i warunki nie muszą być spełnione\n",
    "6. problemy z ewaluacją (to algorytm nienadzorowany)\n",
    "  * miary Inception, Fr\\'{e}chet Inception Distance, etc.\n",
    "7. GANy gorzej od autoenkoderów radzą sobie z danymi dyskretnymi\n",
    "  * wykorzystanie algorytmów typu RL\n",
    "8. GANy pozwalają na równoległe tworzenie wyjscia\n",
    "  * w odróżnieniu od modeli takich jak PixelCNN, PixelRNN, itp.\n",
    "9. GANy nie wymagają aproksymacji ELBO jak w modelu VAE\n",
    "  * nie ma potrzeby zakładania rozkładów prawdopodobieństwa\n",
    "10. GANy generują ostrzejsze obrazy\n",
    "  * model VAE powoduje, że optymalizowany jest błąd średniokwadratowy $\\|x-G(x)\\|^2$, który jest składnikiem regresyjnym\n",
    "  * GANy pozwalają na śledzenie obszarów o wysokiej czętotliwości"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
