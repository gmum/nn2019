{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Mechanizmy atencji Neural Turing Machine</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atencja i rozszerzenia modelu RNN\n",
    "Przydatne rozszerzenia modelu rekurencyjnego\n",
    "1. Neural Turing Machine z pamięcią\n",
    "2. Memory Networks z pamięcią\n",
    "3. Atencja pozwalająca na skupianie się na fragmentach obrazu (obiektu)\n",
    "4. Adaptywne obliczenia pozwalające skupić się dłużej na częściach istotnych\n",
    "5. Neuronowe programowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanizm pamięci\n",
    "<img src=\"../nn_figures/memoryNN.png\" width=\"100%\"> [distill.pub]\n",
    "1. typowe RNN tworzą _jeden_ blok stanu gdzie zapamiętują wszystko\n",
    "2. wyróżnić fragmenty pamięci z której można czytać/pisać\n",
    "  * co czytać?\n",
    "  * gdzie pisać?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Turing Machine\n",
    "<img src=\"../nn_figures/NTM.png\" width=\"50%\" align=\"right\"> [distill.pub]\n",
    "NTM składa się z\n",
    "\n",
    "* kontrolera i pamięci\n",
    "\n",
    "gdzie każdy alement jest różniczkowalny\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "W jaki sposób ustalić adresowanie pamięci? Gdzie czytać? Gdzie pisać? Zapis/odczyt są rozmyte (blurry).\n",
    "\n",
    "NTM tworzy __rozkład atencji__ (attention distribution)\n",
    "\n",
    "* pisze i czyta wszędzie ale w różnym stopniu\n",
    "* zapis i odczyt na podobieństwo LSTM\n",
    "* czytanie $$r_t = \\sum_i w_t(i)M_t(i),$$ gdzie $w_t(i)$ jest wagą obliczaną przez _głowicę odczytującą_\n",
    "* pisanie \n",
    "$$\\begin{align}\n",
    "\\tilde{M}_t(i)&=M_{t-1}(i)\\left[1-w_t(i)e_t\\right]\\\\\n",
    "M_i&=\\tilde{M}_t(i)+w_t(i)a_t,\n",
    "\\end{align}$$\n",
    "gdzie \n",
    "  * $e_t$ to sygnał kasujący (erase), $e_{ti}\\in(0,1)$\n",
    "  * $w_t(i)$ jest wagą obliczaną przez _głowicę zapisującą_\n",
    "  * $e_t$ to sygnał dodawania informacji (add), $a_{ti}\\in(0,1)$\n",
    "  * sygnał jest zerowany jeśli jednocześnie wagi i sygnał kasowania są $1$\n",
    "  * wszystkie składniki wektorów kasowania i dodawania są niezależne\n",
    "\n",
    "\n",
    "<img src=\"../nn_figures/NTM_addressing.jpg\" width=\"35%\" align=\"right\">\n",
    "W jaki sposób ustalić adresowanie?\n",
    "\n",
    "* oparte na zawartości: model przegląda pamięć w poszukiwaniu miejsc, które pasują do poszukiwania\n",
    "  * adresowanie przez zawartość jest realizowane (w oryginalnej implementacji) przez odległość kosinusową $$k_t[u,v]=\\frac{u\\cdot{}v}{\\|u\\|\\,\\|v\\|}$$\n",
    "  * $softmax(\\beta_tk_t(u,v))$ określa wagę $w_t^c$ (content)\n",
    "* oparte na położeniu \n",
    "  * wyrażenia arytmetyczne $a+b$ przy ewaluacji wymagają _położenia_ wartości dla $a$ i $b$\n",
    "  $$\\begin{align}\n",
    "  w_t^g&=g_tw_t^c+(1-g_t)w_{t-1}\\tag{bramka}\\\\\n",
    "  \\tilde{w}_t(i)&=\\sum_j^{N-1}w_t^g(j)s_t(i-j)\\tag{konwolucje}\\\\\n",
    "  w_t(i)&=softmax(\\tilde{w}_t(i)^{\\gamma_t}\\tag{wyostrzanie}\n",
    "  \\end{align}$$\n",
    "  * $w_t^g$ to bramkowanie sygnału; jeśli $g_t=0$, to aktualny sygnał jest ignorowany\n",
    "  * $s_t$ jest sygnałem pozwalajacym na dopasowanie się do przesunięcia\n",
    "  * $\\gamma_t$ jest sygnałem wyostrzania\n",
    "\n",
    "Łącznie sygnały $w_t, e_t, a_t$ tworzą rozkład atencji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Turing Machine\n",
    "<img src=\"../nn_figures/NTM_diagram.gif\" width=\"100%\"> [distill.pub]\n",
    "\n",
    " \n",
    "NTM jest w stanie działać jak pamięć adresowalna zawartością, powtarzać sekwencje, nawet sortować liczby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NTM\n",
    "1. NTM składa się z pamięci i _kontrolera_\n",
    "  * parametrami są wielkość pamięci, liczba głowic, etc.\n",
    "2. Jaka sieć powinna pracować jako kontroler\n",
    "  * LSTM ma własną pamięć stanu, która może uzupełniać pamięć NTM\n",
    "    * ukryte aktywacje można porównać do rejestrów\n",
    "  * możliwa jest sieć typu feedforward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTM - zadanie kopiowania\n",
    "1. NTM dostawał sekwencje wektorów o losowych długościach do 20\n",
    "2. zadaniem odtwarzanie sekwencji\n",
    "  * w trakcie odtwarzania NTM _nie dostawał_ żadnych wejść\n",
    "3. NTM uczy się znacznie szybciej niż model LSTM\n",
    "  * kontroler feedforward był skuteczniejszy niż kontroler LSTM\n",
    "  * NTM nie ma problemów z wydłużającymi się sekwencjami, skuteczność LSTM gwałtownie spada powyżej 20\n",
    "  \n",
    "  * NTM nauczony dla sekwencji do 20 działął w miarę poprawnie dla dłuższych\n",
    "    * pojawiał sie problem przesunięcia w sekwencji\n",
    "    \n",
    "### NTM także kopiowanie ustaloną liczbę razy, pamięć asocjacyjna, uczenie n-gramów, sortowanie\n",
    "* NTM ma zwykle sporo mniej parametrów niż odpowiadający mu funkcjonalnie LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
