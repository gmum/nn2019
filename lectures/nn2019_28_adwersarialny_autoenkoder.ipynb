{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Adwersarialny autoenkoder</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "Dzisiaj, czwartego czerwca, mamy rocznice.\n",
    "* Czego?\n",
    "* Które?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adwersarialny autoenkoder\n",
    "* auto-enkodery mają przewagę nad tradycyjną architekturą GANów w tym, że zawierają enkoder\n",
    "* uczenie adwersarialne daje ostrzejsze obrazy\n",
    "  * ma też swoje wady…\n",
    "  \n",
    "<img src=\"../nn_figures/adversarial_autoencoder.pdf\" width=\"90%\">  \n",
    "* adwersarialne uczenie\n",
    "  * przetworzone przez enkoder przykłady $z\\sim q(z)$ jako przykłady _fałszywe_ (negative, fake) $$q(z)=\\int_x q(z\\mid x)p_X(x)dx,$$\n",
    "  gdzie $p_X(x)$ jest prawdziwym rozkładem danych, a $q(z)$ jest rozkładem kodowania\n",
    "  * adwersarialny autoenkoder __dopasowuje__ posterior $q(z)$ do założonego prioru $p(z)$\n",
    "  * przykłady $z\\sim p(z)$ są losowane z prioru\n",
    "  * adwersarialny koszt dyskryminatora jest kosztem rozpoznawanie pozytywnych $p(z)$ od negatywnych $q(z)$\n",
    "  * można powiedzieć, że _generatorem_ modelu jest autoenkoder $q(z\\mid x)$\n",
    "  * dekoder odtwarza $z$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie AAE\n",
    "* równoległe uczenie sieci adwersarialnej (dyskryminatora) i autoenkodera\n",
    "  * faza __rekonstrukcja__ \n",
    "    * aktualizuje enkoder i dekoder dla minimalizacji błędu\n",
    "  * faza __regularyzacji__ w której sieć adwersarialna\n",
    "    1. poprawia dyskryminator w celu lepszego rozróżniania przykładów _prawdziwych_ (z prior) od _wygenerowanych_ (wyliczonych przez enkoder) [pierwszy krok]\n",
    "    2. poprawia generator (czyli enkoder auto-enkodera) dla zmylenia dyskryminatora [drugi krok]\n",
    "* generator może być\n",
    "  * deterministyczny -- źródłem stochastyczności $q(z)$ pozostaje rozkład danych $p_X(x)$\n",
    "  * stochastyczny -- $q(z\\mid x)\\sim\\mathcal{N}()$ (jak VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAE a inne modele\n",
    "* VAE\n",
    "  * bardzo podobny w założeniu\n",
    "  * w AAE próba dopasowania zagregowanego posterioru $q(z\\mid x)$ do założonego prioru $p(z)$\n",
    "  * VAE korzysta z KL, a tu uczenie adwersarialne\n",
    "  * składnik KL w kosztcie VAE $$\\mathbb{E}_{q(z\\mid x)}\\log p(x\\mid z)-KL(q(z\\mid x)\\mid\\mid p(z))$$ jest zastąpiony składnikiem uczenia adwersarialnego\n",
    "* GAN\n",
    "  * GAN (w swojej podstawowej postaci) opiera się na adwersarialnym wymuszeniu rozkładu danych _na poziomie pikseli_ na wyjście modelu\n",
    "  * AAE uczą autoenkoder dla wykryciu rozkładu danych przez model\n",
    "  * to powinno pozwalać na wymodelowanie cech _semantycznych_\n",
    "  * AAE uczy się także na prostszym rozkładzie (gausowski vs. rozkład danych) i w niższej wymiarowości warstwy latent\n",
    "<img src=\"../nn_figures/adversarial_autoencoder_trained.pdf\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacja przez dodanie etykiet\n",
    "* dodanie etykiet ułatwi generowanie rozkładów poszczególnych klas\n",
    "<img src=\"../nn_figures/adversarial_autoencoder_supervised.pdf\" width=\"70%\">\n",
    "* dodatkowy wektor klasyfikacji (one-hot) dodawany do wejścia dyskryminatora\n",
    "  * rozdziela granicę decyzyjną na warunkową zależnie od danej etykiety\n",
    "  * dodatkowa pozycja dla nie etykietowanych przykładów\n",
    "  * to daje miksturę gausów dla różnych klas\n",
    "    * w fazie negatywnej uzenia adwersarialnego podawana jest etykieta przykładu uczącego\n",
    "    * w fazie pozytywnej podawana jest etykieta składnika mikstury z której jest losowany\n",
    "  \n",
    "<img src=\"../nn_figures/adversarial_autoencoder_supervised_trained.pdf\" width=\"90%\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAE semi-nadzorowany\n",
    "* dane są opisywane w warstwie latent przez\n",
    "  * rozkład klas $p(y)\\sim Cat(y)$ (kategoryczny prior)\n",
    "  * rozkład opisujacy dane $p(z)\\sim\\mathcal{N}(0,\\mathbb{I})$ (ciągły prior)\n",
    "<img src=\"../nn_figures/aae_semisupervised.pdf\" width=\"80%\">\n",
    "* uczenie\n",
    "  * __rekonstrukcja__\n",
    "    * model generatora przewiduje $y$ oraz mapuje $x$ do ciągłego $z$ przez enkoder $q(z, y\\mid x)$\n",
    "    *  dekoder wykorzystuje $y, z$ do odtworzenia $x$ z minimalnym błędem\n",
    "  * __regularyzacja__\n",
    "    * dyskryminatory starają się rozróżnić \n",
    "      * pozytywne przykłady wygenerowane z dyskretnego prior $Cat(y)$ od utworzonego przez enkoder (negatywne)\n",
    "      * pozytywne z ciągłego prioru $z\\sim p(z)$ od negatywnych z enkodera\n",
    "    * poprawia enkoder dla zmylenia dyskryminatorów\n",
    "  * __semi-nadzorowanej klasyfikacji__\n",
    "    * autoenkoder poprawia $q(y\\mid x)$ minimalizując entropię krzyżową na _etykietowanym_ mini-batchu\n",
    "* wyniki uczenia znacznie lepsze od modelu VAE, ale też słabsze od kilku innych"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
