{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Autoenkodery</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoenkoder\n",
    "ma za zadanie \n",
    "$$D(E(x))=x$$\n",
    "1. jeśli nauczy się $D(E(x))=x$ dla wszystkich elementów $x$, to taki model niewiele nam daje, w rzeczywistości minimalizują koszt $$L(x,D(E(x)))$$\n",
    "zapamiętując wszystkie przykłady\n",
    "  * dla liniowego dekodera i kwadratowej funkcji kosztu mamy model identyczny z PCA\n",
    "    * generuje tą samą podprzestrzeń\n",
    "  * nieliniowy dekoder powinien dawać modele silniejsze\n",
    "2. autoenkodery\n",
    "  * mają wyszukiwać informacje __istotne__\n",
    "  * są zwykle w jakiś sposób ograniczone\n",
    "  * mogą pozwalać na redukcję wymiarowości i uczenie cech (podstawowe zastosowanie)\n",
    "  * obecnie najważniejsze stały się modele __generatywne__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ograniczanie enkoderów\n",
    "1. niewielkie wymiary przestrzeni kodów czy modele dekoderów / enkoderów o małej pojemności nie pozwolą na nauczenie się istotnych informacji\n",
    "2. duża przestrzeń latent pozwoli na proste kopiowanie\n",
    "  * AE nie uczy się niczego istotnego\n",
    "3. potrzebne ograniczenie oparte na złożoności problemu\n",
    "  * dodatkowa funkcja kosztu __regularyzująca__ rozwiązanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modele rzadkie\n",
    "$$L(x,D(E(x)))+\\Omega(h),$$\n",
    "gdzie $h$ jest przestrzenią ukrytą (latent)\n",
    "1. dla $x$ i $h$ mamy rozkład łączny\n",
    "$$p(x,h)=p(h)p(x\\mid h)$$\n",
    "  gdzie $p()$ jest realizowane przez budowany model\n",
    "  * ucząc model __maksymalizujemy__\n",
    "  $$\\log\\,p(x,h)=\\log\\,p(h)+\\log\\,p(x\\mid h)$$\n",
    "  * składnik $\\log\\,p(h)$ opisuje ograniczenia na warstwę latent\n",
    "  * może to być wymaganie, by aktywacje były __rzadkie__\n",
    "  * przyjmując $$p(h_i)=\\frac{\\lambda}{2}\\exp(-\\lambda|h_i|)$$\n",
    "  dostajemy\n",
    "  $$\\begin{align}\n",
    "  \\log p(h)&=\\log\\,\\prod_i\\frac{\\lambda}{2}\\exp(-\\lambda|h_i|)\\\\\n",
    "  &=\\sum_i\\log\\left(\\frac{\\lambda}{2}\\exp(-\\lambda|h_i|)\\right)\\\\\n",
    "  &=\\sum_i\\left[-\\lambda|h_i|+\\log\\frac{\\lambda}{2}\\right]\\\\\n",
    "  &=\\Omega(h)+const\n",
    "  \\end{align}$$\n",
    "    * składnik $\\sum_i\\log\\frac{\\lambda}{2}$ nie jest istotny dla uczenia\n",
    "\n",
    "\n",
    "\n",
    "5. Typowym użyciem jest uczenie cech dla późniejszego uczenia nadzorowanego\n",
    "  * na wejściu reprezentacja dźwięku\n",
    "  * w warstwie latent ograniczona warstwa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising AE\n",
    "minimalizuje koszt\n",
    "$$L(x, D(E(\\tilde{x})),$$\n",
    "gdzie $\\tilde{x}$ jest __zaburzoną__ wartością $x$\n",
    "\n",
    "1. Denoising AE ma za zadanie __usuwać__ zaburzenia $x$\n",
    "2. koszt obliczany między wartością odzyskaną a __prawdziwą__\n",
    "<img src=\"../nn_figures/dae.pdf\" width=\"88%\"> [Bengio et al.]\n",
    "  * oryginalny przykład $x$ jest zaburzany procesem $C(\\tilde{x}\\mid x)$\n",
    "  * $D(E(\\tilde{x}))$ aproksymuje wartość oczekiwaną $$E_{x,\\tilde{x}}[x\\mid \\tilde{x}]$$\n",
    "  gdzie $\\tilde{x}$ pochodzi z rozkładu danych zmodyfikowanych zaburzeniem $$p_{Data}(x)C(\\tilde{x}\\mid x)$$\n",
    "  * AE minimalizuje koszt $\\|D(E(\\tilde{x}))-x\\|^2$\n",
    "  * DAE uczy się pola wektorowego $D(E(x))-x$\n",
    "  <img src=\"../nn_figures/dae-learn.pdf\" width=\"86%\"> [Bengio et al.]\n",
    "    * gausowskie zaburzenie o średniej $\\mu=x$ i macierzy kowariancji $\\Sigma=\\sigma^2I$ dla ustalonego $\\sigma^2$\n",
    "    * każda strzałka jest proporcjonalna do rekonstrukcji pomniejszonej o wejście\n",
    "    * wskazuje na obszar o największej estymacji prawdopodobieństwia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rozpoznawanie mowy\n",
    "<img src=\"../nn_figures/widmo.pdf\" width=\"88%\"> [Kowenzowski]\n",
    "1. częstotliwości są obcinane do pasma mowy, część jest podbijana\n",
    "2. sygnał dzielony na ramki o długości 25ms i przesunięciu 10 ms\n",
    "3. przejście z domeny częstotliwosciowej do domeny czasowej (DFT)\n",
    "4. zamiana na logarytmiczną skalę Mel Frequency Cepstral Coefficient MFCC\n",
    "5. dodanie sygnału (wysokiej częstotliwości) tzw. pobudzenia (glottal)\n",
    "6. co w sumie daje cechy różnicowe (delta features) podstawowy i drugiego stopnia\n",
    "\n",
    "Algorytm wykorzystujący np. algorytm Viterbiego w modelu HMM albo sieć neuronowa pozwala na transformację na fonemy. Modele wykorzystują dodatkowo model językowy (NLP).\n",
    "\n",
    "Dla 6-wartwowej sieci (DBN) model uzyskuje do 17% WER (Word Error Rate)\n",
    "$$WER=100\\frac{I+S+D}{A},$$\n",
    "gdzie I -- insertions, S -- substitutions, D -- deletions, A -- all words\n",
    "\n",
    "Wyniki są zależne także od wejścia: same MFCC, MFCC + delta, + sygnał pobudzenia)\n",
    "### Dodanie cech bottleneck\n",
    "1. transformacje jak wcześniej\n",
    "2. budowa auto autoenkodera dla reprezentacji wejść \n",
    "  * na wejściu np. 11 kolejnych MFCC\n",
    "  * warstwa ukryta (latent) o wielkości jednego MFCC\n",
    "  * po nauczeniu pozostaje sam enkoder\n",
    "3. model rozpoznający\n",
    "  * głęboka sieć neuronowa jak dotychczas\n",
    "  * cechy wejściowe MFCC (11 kolejnych ramek) podawane są do enkodera, który zwraca cechę __bottleneck__\n",
    "  * cecha bottleneck dodawana do wejścia\n",
    "  \n",
    "Tak model rozpoznający ma WER poprawiony zwykle o co najmniej kilka procent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozmaitości i autoenkodery\n",
    "1. w ML __rozmaitość__ to zbiór punktów, które mogą być dobrze aproksymowane z wykorzystaniem niewielu stopni swobody (wymiarów)\n",
    "  * wymiarowość może się zmieniać dla danej rozmaitości od punktu do punktu\n",
    "  * każdy punkt ma swoje otoczenie z metryką\n",
    "    * metryka może być __różna__ dla różnych obszarów\n",
    "2. zwykle rozmaitości w problemach ML są __zanurzone__ w wysoko-wymiarowych przestrzeniach\n",
    "3. __hipoteza rozmaitości__ mówi o tym, że problemy w ML są reprezentowane przez nisko-wymiarowe rozmaitości (albo ich zbiory) zanurzone w wysokowymiarowych przestrzeniach oryginalnych danych\n",
    "  * nie zawsze jest prawdziwa\n",
    "  * poprawnie zbudowane AE mają szansę znaleźć te rozmaitości\n",
    "  <img src=\"../nn_figures/manifold.pdf\" width=\"86%\"> [Bengio et al.]\n",
    "  * możliwą procedurą odtwarzania rozmaitości jest budowa grafu opartego na najbliższych sąsiadach\n",
    "  * krawędzie mogą odpowiadać prostym przekształceniom\n",
    "  <img src=\"../nn_figures/manifold2.pdf\" width=\"86%\"> [Bengio et al.]\n",
    "  * okręgi reprezentują płaszczyzny styczne do rozmaitości w każdym punkcie\n",
    "  * pozwala to na ich sklejenie\n",
    "  * jeśli lokalne elementy są lokalnie płaskimi Gausianami, to otrzymujemy model jako mixture of Gaussians\n",
    "  * w rozmaitości można znaleźć kierunki odpowiadające prostym cechom\n",
    "  * to pozwala na proste operacje arytmetyczne na wektorach reprezentacji\n",
    "    * AE w ogólności tworzą reprezentacje latent, które są pokawałkowane\n",
    "    * wiele obszarów jest pustych\n",
    "    * interpolacje w przestrzeni mogą nie odpowiadać obszarom o wysokiej gęstosci prawdopodobieństwa\n",
    "  * potrzebne są dodatkowe ograniczenia, które będą budowały obszary spójne\n",
    "    * modele generatywne, np. VAE, WAE, GAN, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
