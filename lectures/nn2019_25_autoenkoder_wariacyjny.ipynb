{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Modele generatywne - VAE</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele generatywne\n",
    "* autoenkodery mają za zadanie __wyłącznie__ dobrze odtwarzać\n",
    "  * dane w latent __nie mają__ żadnego rozkładu i nie da się generować\n",
    "* co to znaczy generować?\n",
    "  * model ma się nauczyć rozkładu danych (prawdziwy, ale nieznany) $P_X$\n",
    "  * musi być możliwość generowania z zadanego rozkładu $P_Z$ i dekodowania do $P_X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler\n",
    "$KL(p\\mid\\mid q)\\; \\text{to prawie} -\\sum_x q(x)\\log q(x)- (-\\sum_x p(x)\\log p(x))=H(p)-H(q)$\n",
    "\n",
    "Co jest __prawie__ dokładnie mówiące, że $KL(p\\mid\\mid q)=0$ jest równe $0$ jeśli $p$ niesie tyle samo informacji ile niesie $q$\n",
    "\n",
    "Prawie, bo KL jest niesymetryczne i odnosi się do $p$, więc powinno być __pod warunkiem $p$__\n",
    "\n",
    "Stąd \n",
    "$$\\begin{align*}\n",
    "KL(p\\mid\\mid q)&=-\\sum_x p(x)\\log q(x)+\\sum_x p(x)\\log p(x))\\\\\n",
    "&=\\sum_xp(x)\\log\\frac{p(x)}{q(x)}=-\\sum_xp(x)\\log\\frac{q(x)}{p(x)}\n",
    "\\end{align*}$$\n",
    "\n",
    "1. $KL(p\\mid\\mid q)\\geq0$\n",
    "2. $KL(p\\mid\\mid q)\\neq KL(q\\mid\\mid p)$, więc __nie jest__ metryką\n",
    "\n",
    "$KL$ jest miarą podobieństwa rozkładów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder VAE\n",
    "* model składa się z \n",
    "  * enkodera $E:X\\longrightarrow Z$\n",
    "  * dekodera (generatora) $D:Z\\longrightarrow X$\n",
    "  \n",
    "1. potrzebujemy wyliczyć $$p(z\\mid x)=\\frac{p(x\\mid z)p(z)}{p(x)}$$\n",
    " jednak wyliczenie $p(x)$ jest trudne (ang. intractable)\n",
    "  * metodami Monte-Carlo\n",
    "    * pracochłonne\n",
    "    * wysoka wariancja, zerowy bias\n",
    "  * wariacyjnymi\n",
    "    * zerowa wariancja, wysoki bias\n",
    "2. __pomysł__: weźmy jakieś $q(z)$, które\n",
    "  * jest możliwe do policzenia\n",
    "  * niech $q(z)$ przypomina $q(z\\mid x)$\n",
    "  * niech miarą podobieństwa będzie $KL()$\n",
    "3. obliczamy odległość (prawie)\n",
    "$$\\begin{align}\n",
    "KL(q(z)\\mid\\mid p(z\\mid x))&=-\\sum q(z)\\log\\frac{p(z\\mid x)}{q(z)}\\\\\n",
    "&\\hskip5em\\text{ale}\\; p(z\\mid x)=\\frac{p(x,z)}{p(x)}\\\\\n",
    "&=-\\sum q(z)\\log\\frac{p(x,z)/p(x)}{q(z)}\\\\\n",
    "&\\hskip5em\\text{rozpisując logarytm ilorazu}\\\\\n",
    "&=-\\sum q(z)\\left[\\log\\frac{p(x,z)}{q(z)}-\\log p(x)\\right]\\\\\n",
    "&=-\\sum q(z)\\log\\frac{p(x,z)}{q(z)}+\\sum q(z)\\log p(x)\\\\\n",
    "&\\hskip5em\\text{sumowanie jest po $z$, więc $p(x)$ można wyciągnąć}\\\\\n",
    "&=-\\sum q(z)\\log\\frac{p(x,z)}{q(z)}+p(x)\\sum q(z)\\\\\n",
    "&\\hskip5em\\text{ale}\\;\\sum q(z)=1\\\\\n",
    "&=-\\sum q(z)\\log\\frac{p(x,z)}{q(z)}+p(x)\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "3. Teraz zamieńmy kolejność w równaniu\n",
    "$$\n",
    "p(x)=KL(q(z)\\mid\\mid p(z\\mid x))+\\underbrace{\\sum q(z)\\log\\frac{p(x,z)}{q(z)}}_{L}\n",
    "$$\n",
    "4. $x$ jest punktem, więc $p(x)=const$\n",
    "  * chcemy __minimalizować__ $KL(q(z)\\mid\\mid p(z\\mid x))$\n",
    "  * co jest wobec tego osiągalne przez __maksymalizację__ $L$\n",
    "  * $L$ to variational lower bound\n",
    "5. jak jednak maksymalizować $L$???\n",
    "\n",
    "$$\\begin{align}\n",
    "L&=\\sum q(z)\\log\\frac{p(x,z)}{q(z)}\\\\\n",
    "&\\hskip5em\\text{zastępujemy $p(x,z)$ definicją $p(x\\mid z)p(z)$}\\\\\n",
    "&\\hskip5em\\text{ponieważ potrzebna jest definicja $px\\mid z)$ jako dekodera}\\\\\n",
    "&=\\sum q(z)\\log\\frac{p(x\\mid z)p(z)}{q(z)}\\\\\n",
    "&\\hskip5em\\text{znowu rozpisując logarytm iloczynu}\\\\\n",
    "&=\\sum q(z)\\left[\\log p(x\\mid z) +\\log\\frac{p(z)}{q(z)}\\right]\\\\\n",
    "&=\\sum q(z)\\log p(x\\mid z) +\\sum q(z)\\log\\frac{p(z)}{q(z)}\\\\\n",
    "&\\hskip5em\\text{pierwszy składnik to po prostu wartość oczekiwana po wartościach $q(z)$}\\\\\n",
    "&=\\mathbb{E}_{q(z)}\\log p(x\\mid z) +\\sum q(z)\\log\\frac{p(z)}{q(z)}\\\\\n",
    "&\\hskip5em\\text{a drugi to wartość miary $-KL(q(z)\\mid\\mid \\log p(z))$}\\\\\n",
    "&=\\mathbb{E}_{q(z)}\\log p(x\\mid z) -KL(q(z)\\mid\\mid \\log p(z))\\\\\n",
    "&\\hskip5em\\text{$q(z)$ jest funkcją realizowaną przez enkoder na podstawie danych $x\\in ~X$}\\\\\n",
    "&=\\mathbb{E}_{q(z)}\\log p(x\\mid z) -KL(q(z\\mid x)\\mid\\mid \\log p(z))\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder VAE \n",
    "W modelu VAE uczenie będzie polegało na __maksymalizowaniu__\n",
    "$$L=\\mathbb{E}_{q(z\\mid x)}\\log p(x\\mid z)-KL(q(z\\mid x)\\mid\\mid p(z))$$\n",
    "1. pierwszy składnik odpowiada w rzeczywistości błędowi rekonstrukcji\n",
    "  * niech założony prior $p(z)$ będzie rozkładem normalnym $N(0,1)$\n",
    "  * w rozkładzie normalnym mamy czynnik $\\exp(-(x-\\hat{x})^2)$\n",
    "  * biorąc logarytm zostaje nam dokładnie $(x-\\hat{x})^2$\n",
    "2. drugi składnik jest odpowiedzialny za to, by rozkład warunkowy $q(z\\mid x)$ był jak najbardziej podobny do założonego rozkładu prior $p(z)$\n",
    "3. model składa się z \n",
    "  * __enkodera__ realizujacego $q(z\\mid x)$\n",
    "  * dekodera realizującego $p(x\\mid z)$\n",
    "    * $z$ nie jest dowolne, lez zależy od $x$\n",
    "4. dekoder jest deterministyczny\n",
    "5. enkoder jest stochastyczny i zwraca wszystkie parametry założonego rozkładu prior\n",
    "  * dla $\\mathcal{N}(0,\\mathbb{I})$ są to dwa parametry $\\mu, \\Sigma$\n",
    "    * $\\mu$ jest $D$ wymiarowy\n",
    "    * $\\Sigma$ jest $D\\times D$\n",
    "    * $\\mathbb{I}$ zakładamy jako macierz przekątniową dla prostoty i symetrii (i obliczalności), enkoder zwraca drugi $D$ wymiarowy partametr\n",
    "  * po wyliczeniu $(\\mu, \\sigma)$ wartość $z$ jest __samplowana__ z wyliczonego rozkładu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operacje w przestrzeni latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kodowanie i dekodowanie danych wejściowych\n",
    "* ze zbioru uczącego, ze zbioru testującego\n",
    "* miarą jest wartość błędu odtwarzania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### samplowanie z okolic\n",
    "* kodujemy przykład $x$\n",
    "* samplujemy z okolic\n",
    "  * enkoder zwraca __rozkład__ podany przez parametry\n",
    "  * dla rozkładu normalnego samplujemy z tego rozkładu o wyliczonych parametrach $z=(\\mu,\\sigma^2)$ i dekodujemy (dekoder jest deterministyczny!)\n",
    "* wygenerowane przykłady powinny należeć do przykładów z rozkładu $X$\n",
    "* wygenerowane przykłady powinny być __podobne__ do oryginalnego\n",
    "  * co to znaczy _podobne_?\n",
    "  * podobne zdjęcia, częsty problem z dobrą oceną\n",
    "  * przykłady mogą pochodzić ze zbiorów o małej wariancji danych, np. twarze (CelebA)\n",
    "  * niezależne miary, np. biblioteki dla oceny grafów z danej klasy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolacja\n",
    "* interesuje nas przejście między dwoma przykładami\n",
    "* kodujemy $x_1$ i $x_2$ dostając $z_1, z_2$\n",
    "* interpolacja między nimi\n",
    "* liniowa?\n",
    "* paradoks \"mydlanej bańki\"\n",
    "  * niech $P_Z$ $D$-wymiarowym rozkładem normalnym $\\mathcal{N}(0,\\mathbb{I})$\n",
    "  * losując $z\\sim P(Z)$ losujemy $D$ zmiennych $z_i\\sim\\mathcal{N}(0,1)$ _niezależnie_\n",
    "  * kwadrat normy $Y=z_1^2+z_2^2+\\ldots+z_D^2$\n",
    "  * zmienna losowa $Y$ ma rozkład $\\chi^2$\n",
    "  * dla $D>30$ korzystamy w przybliżeniu z rozkładu normalnego $\\mathcal{N}(\\sqrt{2D-1},1)$\n",
    "  * a więc normy wszystkich przykładów dla np. $D=100$ mają wartość oczekiwaną rzędu $14$!\n",
    "  * oznacza to, że wszystkie $z\\sim p(z)$ leżą na bańce o niewielkiej grubości\n",
    "    * grubość jest stała, więc stosunkowo coraz węższa w stosunku do średnicy\n",
    "  * punkty interpolacji liniowej przechodzą przez obszar o małej masie prawdopodobieństwa\n",
    "* jak sobie z tym poradzić w interpolacjach?\n",
    "  * interpolacje sferyczne\n",
    "    * heureza oparta na paradoksie bańki\n",
    "  * inny rozkład\n",
    "    * weźmy model o ustalonym prior $p(z)$\n",
    "    * losujemy dwa $z$, obliczamy punkt w środku $z=0.5z_1+0.5z_2$\n",
    "    * jaki będzie rozkład punktów środkowych?\n",
    "    * dla rozkładów prior o znanej średniej, rozkład środkowych będzie __inny__\n",
    "    * uczenie i interpolacje będą na innych rozkładach, stąd interpolacje nie mogą być poprawne\n",
    "    * potrzebny rozkład prior, dla którego średnia nie będzie się różnić\n",
    "    * takim rozkładem jest np. rozkład Cauchy-ego\n",
    "    $$C(x)=\\frac{1}{\\pi}\\frac{\\lambda}{\\lambda^2+(x-\\alpha)^2}$$\n",
    "      * $\\alpha$ to parametr położenia, $\\lambda>0$ skali\n",
    "    * to tzw. rozkład niewidomego łucznika\n",
    "    * rozkład Cauchy-ego __nie ma__ średniej, wariancji, ani wyższych momentów\n",
    "    * rozkład średnich $z\\sim C$ jest identyczny z $C$\n",
    "    * można nauczyć model biorąc $p(z)\\sim C$\n",
    "    * jednak $C$ ma \"grube\" ogony i samplowanie stamtąd daje nienajlepsze wyniki\n",
    "    * inne rozwiązanie \n",
    "      * użyć normalny prior do uczenia\n",
    "      * przy interpolacji\n",
    "        * wylosować dwa punkty z normalnego\n",
    "        * przez funkcje $CDF$ rozkładu normalnego i rozkład Cauchy przeprowadzić je do przestrzeni rozkładu Cauchy\n",
    "        * przeprowadzić interpolację liniową\n",
    "        * wykonać odwzorowanie odwrotne\n",
    "      * możliwe, dające ten sam rezultat, rozwiązanie przenoszące rozkład punktów średnich do założonego rozkładu wykorzystując tzw. earth-moving (OT)\n",
    "      \n",
    "      \n",
    "<img src=\"../nn_figures/automata-manifold.pdf\" width=\"100%\"> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
