{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Optymalizatory</big></big></big></big></big>\n",
    "\n",
    "<big><big>(z wykładu Uczenie maszynowe 2018/19)</big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oznaczenia\n",
    "\n",
    "* $\\theta$ - wektor wszystkich parametrów modelu\n",
    "* $\\theta^{(0)}$ - początkowa wartość parametrów\n",
    "* $\\theta^{(t)}$ - wartość parametrów po $t$ krokach (po czasie $t$)\n",
    "\n",
    "\n",
    "* $\\Delta\\theta^{(t)} := \\theta^{(t+1)} - \\theta^{(t)}$ - krok w czasie $t$\n",
    "\n",
    "\n",
    "* $L$ - różniczkowalna funkcja kosztu\n",
    "\n",
    "\n",
    "* $\\eta$ - __learning rate__\n",
    "    * kontroluje szybkość uczenia\n",
    "    * obecna w praktycznie każdym optimizerze\n",
    "\n",
    "Wartość $L$ zależy od:\n",
    "* wektora $\\theta$\n",
    "* danych treningowych\n",
    "* być może jeszcze innych stałych\n",
    "\n",
    "Optymalizujemy tylko $\\theta$, będziemy pisać w skrócie $L(\\theta)$.\n",
    "\n",
    "$L$ zwraca liczbę. Zakładamy, że umiemy policzyć gradient $\\nabla L(\\theta)$.\n",
    "\n",
    "__Gradientowa__ i __iteracyjna__ minimalizacja $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (typowe wartości: od 0.001 do 0.01)\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $\\theta^{(t+1)}=\\theta^{(t)} - \\eta\\nabla L(\\theta^{(t)})$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. $- \\nabla L(\\theta^{(t)}$ to kierunek najszybszego spadku $L$\n",
    "2. ale $- \\nabla L(\\theta^{(t)}$ __nie wskazuje__ w kierunku optymalnego $\\theta$\n",
    "3. wektor gradientu $\\nabla L(\\theta^{(t)})$ jest prostopadły do hiperpłaszczyzny stycznej do powierzchni o równych wartościch funkcji kosztu (_isosurface_) w miejscu $\\theta^{(t)}$\n",
    "4. zmniejszenie $L$ odpowiada __co najmniej__ rozwartemu kątowi między $\\nabla\\theta^{(t)}$ a $\\Delta\\theta^{(t)}$\n",
    "5. __zbyt duże__ $\\eta \\|\\Delta\\theta^{(t)}\\|$ spowoduje, że $\\theta^{(t+1)}$ może znaleźć się po _drugiej stronie krzywizny_\n",
    "6. learning rate\n",
    "    * mała wartość - spowalnia uczenie\n",
    "    * duża wartość - powoduje __oscylacje__, problem ze zbieżnością\n",
    "7. dlaczego nie normalizujemy $\\nabla L(\\theta^{(t)})$?\n",
    "    * duża wartość gradientu to __duża lokalna zmienność $L$__ - powinniśmy robić __małe kroki__ (precyzyjne)\n",
    "    * mała wartość gradientu to __mała lokalna zmienność $L$__ - powinniśmy robić __duże kroki__ (żeby szybciej opuścić __plateau__)\n",
    "\n",
    "7. wariant GD ze zmiennym w czasie $\\eta$\n",
    "  * malejący w stosunku odwrotnym do kroku uczenia $t$\n",
    "    * warunki konieczne osiągnięcia optymalnego $\\theta$ (przy jakich założeniach? uwaga na __lokalne minima__)\n",
    "        $$\\begin{align}\\sum_t\\eta_t^2\\lt\\infty\\\\\\sum_t\\eta_t=\\infty\\end{align}$$\n",
    "    * $v_0(1-\\lambda\\eta_0)^t$ dla poczętkowego $\\eta_0$ i stałej $\\lambda$\n",
    "    * $\\exp(-t/\\tau)$ dla stałej $\\tau$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (typowe wartości: od 0.001 do 0.01)\n",
    "* $\\gamma$ - zawsze $<1$, zwykle równe 0.9.\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $v^{(t)}$ - wektor __pędu__ (_momentum_) w czasie $t$, wymiar taki sam jak $\\theta$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "$v^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $v^{(t+1)} = \\gamma v^{(t)} + \\eta\\nabla L(\\theta^{(t)})$\n",
    "2. $\\theta^{(t+1)}=\\theta^{(t)} - v^{(t+1)}$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. Analogia do kulki toczącej się ze wzgórza, $\\gamma$ to tarcie lub opór powietrza.\n",
    "2. __Pamięć__\n",
    "    * wzajemnie wzmacniają się kroki w __istotnym kierunku__\n",
    "    * __oscylacje__ uśredniają się do małej wartości\n",
    "    * mniejsze spowolnienie na __plateau__, jeśli \"kulka\" była rozpędzona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov accelerated gradient NAG\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (typowe wartości: od 0.001 do 0.01)\n",
    "* $\\gamma$ - zawsze $<1$, zwykle równe 0.9.\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $v^{(t)}$ - wektor __pędu__ (_momentum_) w czasie $t$, wymiar taki sam jak $\\theta$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "$v^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $v^{(t+1)} = \\gamma v^{(t)} + \\eta\\nabla L(\\theta^{(t)}-\\gamma v^{(t)})$\n",
    "2. $\\theta^{(t+1)}=\\theta^{(t)} - v^{(t+1)}$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. __spojrzenie wprzód__\n",
    "    * zgrubne oszacowanie _prawdopodobnego_ nowego $\\theta^{(t+1)}=\\theta^{(t)}-\\gamma v^{(t)}$\n",
    "    * gradient liczony w nowym miejscu\n",
    "    * rozszerzenie momentum z _ekstrapolacją_\n",
    "    \n",
    "2. __parametry__\n",
    "    * learning rate ustalony\n",
    "        * ewentualne modyfikacje w eksperymentach\n",
    "    * __momentum nesterova__\n",
    "        * schemat: $\\gamma^{(t)}=1-3/(5+t)$\n",
    "        * rosnące\n",
    "        * w dalszych iteracjach dalej wybiega wprzód"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (typowe wartości: od 0.001 do 0.01)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8})$\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $h^{(t)}$ - wektor sumujący kwadraty gradientów do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "* $h^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $h^{(t+1)} = h^{(t)} + (\\nabla L(\\theta^{(t)}))^2$\n",
    "2. $\\theta^{(t+1)}=\\theta^{(t)} - \\eta \\dfrac{\\nabla L(\\theta^{(t)})}{\\sqrt{h^{(t+1)} + \\epsilon}}$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. dzielenie ma __znormalizować__ gradient\n",
    "2. normalizacja oddzielnie dla:\n",
    "    * każdej współrzędnej gradientu\n",
    "    * czyli dla każdej współrzędnej $\\theta$\n",
    "    * czyli dla każdego parametru modelu\n",
    "3. $h$ jest sumą kwadratów, więc trzeba w mianowniku wziąć pierwiastek - bez pierwiastka działa słabo\n",
    "4. __akumulacja__ kwadratów gradientów\n",
    "    * $h$ to suma, a nie średnia\n",
    "    * gdyby gradienty były stałe, mianownik rósłby proporcjonalnie do $\\sqrt{t}$\n",
    "    * w praktyce bardzo maleje $\\Delta\\theta$, model może __przestać się uczyć__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (typowe wartości: od 0.001 do 0.01)\n",
    "* $\\gamma$ - współczynnik średniej kroczącej (zazwyczaj: 0.9)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8})$\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $h^{(t)}$ - wektor średniej kroczącej kwadratów gradientów do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "* $h^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $h^{(t+1)} = \\gamma h^{(t)} + (1-\\gamma)(\\nabla L(\\theta^{(t)}))^2$\n",
    "2. $\\theta^{(t+1)}=\\theta^{(t)} - \\eta \\dfrac{\\nabla L(\\theta^{(t)})}{\\sqrt{h^{(t+1)} + \\epsilon}}$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. __cel__: usunąć problem Adagrad z szybko malejącym $\\Delta\\theta$\n",
    "2. średnia krocząca (__moving average__)\n",
    "    * podobna do zwykłej średniej\n",
    "    * zapomina daleką przeszłość\n",
    "    * nie wymaga pamiętania pełnej historii gradientów\n",
    "    * __ograniczenie akumulowania__ gradientów do okienka, $\\frac{1}{1-\\gamma}$ (stałe) zamiast $\\sqrt{t}$ (rosnące)\n",
    "3. zaproponowane przez Hintona\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "#### Parametry\n",
    "* $\\gamma$ - współczynnik średniej kroczącej (zazwyczaj: 0.95)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero, umożliwia rozpoczęcie uczenia (zazwyczaj: od $10^{-6}$ do $10^{-2}$)\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $h^{(t)}$ - wektor średniej kroczącej kwadratów gradientów do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "* $d^{(t)}$ - wektor średniej kroczącej $\\Delta\\theta$ do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "* trzeba też pamiętać $\\theta^{(t-1)}$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "* $h^{(0)} = \\mathbf{0}$\n",
    "* $d^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $h^{(t+1)} = \\gamma h^{(t)} + (1-\\gamma)(\\nabla L(\\theta^{(t)}))^2$\n",
    "2. $\\theta^{(t+1)}=\\theta^{(t)} - \\sqrt{d^{(t)} + \\epsilon} \\dfrac{\\nabla L(\\theta^{(t)})}{\\sqrt{h^{(t+1)} + \\epsilon}}$\n",
    "3. $d^{(t+1)} = \\gamma d^{(t)} + (1-\\gamma)(\\theta^{(t+1)} - \\theta^{(t)})^2$\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "1. rozszerzenie RMSProp (ale wymyślone niezależnie jako poprawka Adagrad)\n",
    "2. zastąpienie learning rate przez __wektor__\n",
    "    * learning rate __proporcjonalny do__ średniego $\\Delta\\theta$\n",
    "    * upodobnienie szybkości poprawek do poprawek\n",
    "    * __eliminacja__ stałej uczenia z parametrów\n",
    "3. ważna rola parametru $\\epsilon$\n",
    "    * nie tylko zapobiega dzieleniu przez zero\n",
    "    * umożliwia rozpoczęcie uczenia - $d^{(1)}$ większe od zera\n",
    "    * $\\sqrt{\\epsilon}$ wyznacza __dolne ograniczenie__ $\\sqrt{d^{(t)} +\\epsilon}$ - uczenie nie zatrzymuje się"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Adaptive Moment Estimation\n",
    "\n",
    "#### Parametry\n",
    "* $\\eta$ - learning rate (zazwyczaj: 0.001)\n",
    "* $\\beta_1$ - współczynnik średniej kroczącej pierwszego momentu (zazwyczaj: 0.9)\n",
    "* $\\beta_2$ - współczynnik średniej kroczącej pierwszego momentu (zazwyczaj: 0.999)\n",
    "* $\\epsilon$ - zapobiega dzieleniu przez zero (zazwyczaj: $10^{-8}$)\n",
    "\n",
    "#### Parametry wewnętrzne\n",
    "* $m^{(t)}$ - wektor średniej kroczącej pierwszego momentu gradientu do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "* $v^{(t)}$ - wektor średniej kroczącej drugiego momentu gradientu do czasu $t$, wymiar taki sam jak $\\theta$\n",
    "\n",
    "#### Inicjalizacja\n",
    "\n",
    "* $m^{(0)} = \\mathbf{0}$\n",
    "* $v^{(0)} = \\mathbf{0}$\n",
    "\n",
    "#### Update\n",
    "\n",
    "1. $m^{(t+1)} = \\beta_1 m^{(t)} + (1-\\beta_1)(\\nabla L(\\theta^{(t)}))$\n",
    "2. $v^{(t+1)} = \\beta_2 v^{(t)} + (1-\\beta_2)(\\nabla L(\\theta^{(t)}))^2$\n",
    "3. $\\widehat m = \\dfrac{m^{(t+1)}}{1-\\beta_1^{t+1}}$\n",
    "4. $\\widehat v = \\dfrac{v^{(t+1)}}{1-\\beta_2^{t+1}}$\n",
    "5. $\\theta^{(t+1)} = \\theta^{(t)} - \\eta\\dfrac{\\widehat m}{\\sqrt{\\widehat v} + \\epsilon}$\n",
    "\n",
    "Uwaga: $\\beta^{t+1}$ to \"$\\beta$ do potęgi $t+1$\", a nie $\\beta$ w czasie $t+1$.\n",
    "\n",
    "#### Dyskusja\n",
    "\n",
    "* __gradient__ adaptowany\n",
    "  * krocząca średnia gradientów (pierwszy moment) - tłumione oscylacje\n",
    "  * krocząca średnia kwadratów gradientów (drugi moment) - normalizacja gradientów\n",
    "* __inicjalizacja__ $v_0$ i $m_0$ na $0$\n",
    "    * średnia krocząca __zbiasowana__ w kierunku zera\n",
    "    * __przeciwdziałanie__\n",
    "        * symbole z \"daszkiem\" wprowadzają poprawkę\n",
    "        * im później (duże $t$), tym mniejsza poprawka\n",
    "* dobrze dobrane parametry domyślne - zazwyczaj nie ma potrzeby ich modyfikacji\n",
    "* jeden z __najlepszych__ optimizerów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I wiele wiele innych\n",
    "\n",
    "AdaMax, Nadam, AMSGrad, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
