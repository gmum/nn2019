{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Głębokość sieci warstwowych</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Głębokość\n",
    "1. jest bardzo płynna granica między modelami __płytkimi__ a __głębokimi__\n",
    "2. zwykle 2-3 warstwy to płytkie, wyżej już głębokie\n",
    "3. obecnie wykorzystuje się modele o setkach warstw\n",
    "  * typowe MLP o kilkudziesięciu warstwach\n",
    "  * modele ResNet o kilkuset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liczba warstw sieci głębokiej\n",
    "1. sieci \"_płytkie_\" mają jedną-dwie wartswy ukryte (hidden, latent)\n",
    "2. od kiedy stają się _głębokie_?\n",
    "  * Bengio mówi, ze 3-4 warstwy ukryte to już sieć głęboka\n",
    "3. czemu sieci głębokie?\n",
    "  * warstwowe mapują $X$ na przestrzenie ukryte tak, by ostatnia para warstw (ukryta, wyjściowa) dawały problem separowalny (lub bliski niemu)\n",
    "  * teoretycznie każdy problem można rozwiązać siecią płytką\n",
    "  > (Uniwersalne twierdzenie o aproksymacji) Dla ciągłej, monotonicznie rosnącej $\\varphi(\\cdot)$ i dla ciągłej $f$ określonej na hipersześcianie $[0,1]^M_0$, dla dowolnie małego $\\epsilon>0$ istnieją $M_1,\\alpha_i, w_{ij}$ takie, że dla $x\\in[0,1]^P$ $$F(x)=\\sum_{i=1}^{M_1}\\alpha_i\\varphi\\left(\\sum_{j=1}^{M_0}w_{ij}x_j+b_i\\right)$$ jest aproksymacją $f(\\cdot)$ taką, żę $$|F(x)-f(x)|<\\epsilon$$ dla wszystkich x w przestrzeni wejsciowej.\n",
    "  * jeden z dowodów Cybenko (1989)\n",
    "  * taki model będzie wykorzystywać __wykładniczą__ liczbę neuronów ukrytych\n",
    "<img src=\"../nn_figures/shallow_net_regions.pdf\" width=\"100%\"> (Montufar, Pascanu, Cho, Bengio, 2014)\n",
    "  * maksymalna liczba obszarów sieci z jedną warstwą ukrytą o $n_0$ wejściach i $n_1$ neuronach ukrytych wynosi $\\sum_{i=0}^{n_1} \\binom{n_1}{i}$\n",
    "    * przy warunku, że hiperpłaszczyzny są ułożone tak, że dowolne $k$ płaszczyzn w $n$-wymiarowej przestrzeni przecina się w $n-k$ hiperpłaszczyźnie\n",
    "\n",
    "4. sieci głębokie stosują __bardzo__ wiele mapowań\n",
    "  * wykrywanie cech od bardziej szczegółowych do bardziej ogólnych\n",
    "  * liczba neuronów pozostaje ograniczona\n",
    "  * pojawia się bardzo wiele wag\n",
    "5. pojawiają się problemy w uczeniu dotychczasowymi algorytmami\n",
    "  * warstwy zaczynają się uczyć z __bardzo różnymi__ prędkościami\n",
    "    * wczesne warstwy zwalniają podczas gdy późniejsze uczą się szybko\n",
    "    * może też być odwrotnie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## czy głębokie sieci są efektywniejsze?\n",
    "1. wykorzystując model o jednej warstwie ukrytej z monotoniczną nieliniową funkcją aktywacji można aproksymować problem z dokładnością lepszą niż $\\epsilon$ z prawdopodobieństwem wyższym od $1-\\delta$ dla dowolnych $\\epsilon, \\delta$\n",
    "\n",
    "5. W sieciach głębokich wydaje się, że liczba regionów jest zależna liniowo od głębokości, a proces \"rozkładania\" pozwala na ich wykładniczę zwielokrotnienie\n",
    "<img src=\"../nn_figures/number_of_regions.pdf\" width=\"100%\"> (Montufar, Pascanu, Cho, Bengio, 2014) [https://arxiv.org/pdf/1402.1869.pdf]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
