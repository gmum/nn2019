{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Wprowadzenie</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"../nn_figures/learning_types.png\" width=\"100%\"> [https://www.pinterest.com/pin/274578908514098592/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Uczenie maszynowe\n",
    "1. __uczenie maszynowe__ tworzenie rozwiązań na podstawie zbioru przykładów\n",
    "2. w przypadku sieci neuronowych będzie to zwykle uczenie __nadzorowane__ i __nie-nadzorowane__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci neuronowe\n",
    "\n",
    "<img src=\"../nn_figures/neuron.png\" width=\"100%\">[https://en.wikipedia.org/wiki/Neuron]\n",
    "  * ciało komórki\n",
    "  * dendryty i aksony\n",
    "  * strefa spustowa\n",
    "  * potencjał synaptyczny\n",
    "  <img src=\"../nn_figures/potential.gif\" width=\"100%\">\n",
    "  * synapsa i neuroprzekaźniki\n",
    "  <img src=\"../nn_figures/synapse.jpg\" width=\"100%\">[http://learn.genetics.utah.edu/content/neuroscience/neurons/]\n",
    "  * budowa całego systemu nerwowego jest bardzo złożona\n",
    "* neuron biologiczny jest wysoce nieliniowy\n",
    "* ciało komórki musi osiągnąć wystarczający potencjał by zainicjować (ang. fire) sygnał wzdłuż aksonu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Różne rodzaje sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Modele liniowe__\n",
    "<img src=\"../nn_figures/linear.png\" width=\"70%\">\n",
    "\n",
    "$$Y=\\theta^TX$$\n",
    "  2. X może być przedstawione przez __funkcje bazowe__\n",
    "    * problem rozwiązywany bezpośrednio w przestrzeni wejściowej\n",
    "    * np. wielomiany, f. Fouriera, wavelet, etc.\n",
    "    * funkcje bazowe muszą być definiowane _a priori_\n",
    "  3. rozwiązanie __dla problemu regresji__ będzie polegać na minimalizacji __kwadratowej funkcji kosztu__\n",
    "  $$L(\\theta)=\\sum_k(y_k-\\widehat{y}_k)^2,$$\n",
    "  gdzie $\\widehat{y}$ jest wynikiem działania modelu\n",
    "    * kwadratowa fuknkcja kosztu wynika z założenia o normalności błędów \n",
    "    $$y_i=F(x_i)+\\epsilon$$\n",
    "    gdzie $F$ nieznaną funkcją, a $\\epsilon$ to błąd i $\\epsilon\\sim\\mathcal{N}(0,\\sigma)$\n",
    "    * przykładowo: \n",
    "      1. fabryka produkuje patyki opisane przez parametry $x_i$\n",
    "      2. mają długości $y_i$ zmierzone za pomocą pewnej miarki\n",
    "        * a więc obciążone __nieusuwalnym__ błędem\n",
    "        * zakładamy, że ten błąd ma rozkład normalny\n",
    "      3. zadanie polega na przewidzeniu długości\n",
    "  4. zwykle __rozszerzony__ wektor wejsciowy $x=[1, x]$ co wprowadza przesunięcie\n",
    "  $$\\theta^Tx=\\theta^t[1,x_1,\\dots,x_K]=\\theta_0+\\theta_1x_1+\\dots+\\theta_Dx_D$$\n",
    "    * bez rozszerzenia hiperpłaszczyzna przechodzi przez początek układu współrzędnych\n",
    "  5. model liniowy ma jedno minimum -- funkcja kosztu jest wypukła\n",
    "    * można rozwiązać z pomocą\n",
    "    $$\\widehat{\\theta}=(X^TX)^{-1}X^Ty$$ \n",
    "    pod warunkiem istnienia $(X^TX)^{-1}$\n",
    "      * zwraca wynik dokładny, jednak odwracanie $(X^TX)^{-1}$ jest złożone obliczeniowo i źle uwarunkowane\n",
    "      * $X^TX$ m wymiar $D\\times{}D$\n",
    "    * uczenie za pomocą SGD\n",
    "  6. modele liniowe mają ograniczone zastosowanie, ale są czytelne\n",
    "\n",
    "W problemie klasyfikacji\n",
    "1. wykorzystujemy warstwy __softmax__\n",
    "$$y_i=\\frac{\\exp(x_i)}{\\sum_j\\exp(x_j)}$$\n",
    "  * wartości w przedziale $[0,1]$ i sumują do $1$\n",
    "  * można je interpretować jako rozkłady prawdopodobieństw $K$ zmiennych $y_i$\n",
    "2. warstwa liniowa wraz z softmax pozwala na wieloklasową klasyfikację\n",
    "\n",
    "$$y_i=\\frac{\\exp(\\sum_jw_{ij}x_j+b_i)}{\\sum_k\\exp(\\sum_jw_{kj}x_j+b_k)}$$\n",
    "  * wykorzystując kodowanie wyjść typu __one-hot__\n",
    "    * $t_i=1$ jeśli prawdziwa jest klasa $i$-ta, $0$ w przeciwnym wypadku\n",
    "  * trenowanie przez minimalizację __negative log-likelihood__ entropii krzyżowej prawdziwych etykiet dla dystrybucji przewidywanych rozkładów\n",
    "  $$L(t,y)=-\\sum_it_i\\log(y_i)$$\n",
    "\n",
    "Jednak regresja logistyczna __nie rozwiązuje__ problemów, które nie są liniowo separowalne\n",
    "* do tego będziemy potrzebowali warstw ukrytych\n",
    "  * rozumianych jako"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele warstwowe\n",
    "1. <img src=\"../nn_figures/perceptron.jpg\" width=\"100%\">[https://towardsdatascience.com]\n",
    "  * tzw. McCulloch-Pitts model, także Rosenblatta\n",
    "  * wprowadzona twarda nieliniowość\n",
    "2. algorytm uczenia\n",
    "```\n",
    "while not wszystkie przykłady poprawnie klsyfikowane:\n",
    "   (x, y) = kolejny przykład\n",
    "   if y != net(x):\n",
    "      # przykład źle klasyfikowany\n",
    "      w = w + eta * (y - net(x)) * x\n",
    "```\n",
    "  * jeśli przykłady są separowalne liniowo, to algorytm na pewno zakończy się dla $\\eta > 0$\n",
    "  * to algorytm przez poprawianie błędów\n",
    "  * to algorytm przez spadek gradientu\n",
    "3. wiele problemów __nie jest__ liniowo separowalnych\n",
    "  * nawet jeśli są __tylko trochę__ nieseparowalne, to algorytm może doprowadzić do bzdurnych rozwiązań\n",
    "    * są pewne modyfikacje, tzw. algorytm __pocket__\n",
    "4. potrzebne rozwinięcie architektury - sieci __wielowarstwowe__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele wielowarstwowe\n",
    "<img src=\"../nn_figures/multilayer.png\" width=\"100%\">[https://www.researchgate.net]\n",
    "1. jedna lub więcej warstw __ukrytych__ (hidden, latent)\n",
    "2. warstwy ukryte wykorzystują __nieliniowe__ funkcje aktywacji\n",
    "  * sigmoidalne, ReLU, RBF, i wiele pochodnych\n",
    "  * użycie aktywacji liniowej __redukuje__ model do liniowej bez warstw ukrytych\n",
    "3. uczenie\n",
    "  * analogicznie do sieci bez warstw uktytych?\n",
    "    1. oblicz błąd $(y-\\widehat{y})$\n",
    "    2. zaaplikuj poprawkę $ w=w+\\eta(y-\\widehat{y})x$\n",
    "  * jaki jest błąd dla neuronów w warstwach ukrytych?\n",
    "    * a raczej jaka jest poprawna aktywacja???\n",
    "4. dopiero algorytm wstecznej propagacji...\n",
    "5. problem klasyfikacji\n",
    "  * podstawowe modele są __binarne__\n",
    "  * __wieloklasowe__ (multiclass) $$y\\in\\{c_1,\\dots,c_K\\}$$\n",
    "    * zwykle reprezentacja __one_hot__\n",
    "    * może odpowiadać $K$ klasyfikatorom binarnym o wspólnych warstwach\n",
    "    * wyjście można próbować zinterpretować jako __prawdopodobieństwo__ klas $P(c_i\\mid x)$\n",
    "      * przez funkcję __softmax__\n",
    "      $$\\widehat{y}_i=\\frac{\\exp(y_i)}{\\sum_l\\exp(y_l)}$$\n",
    "  * __wieloetykietowe__ (multilabel)\n",
    "    * \"prawdziwa\" może być więcej niż jedna klasa wyjściowa\n",
    "    * może odpowiadać problemowi klasyfikacji wielopoziomowej\n",
    "    * trudny problem: zależności miedzy klasami, trudna deifinicja funkcji kosztu, wartość wyjść nie sumuje się do jedności, ani do ustalonej wartości\n",
    "    \n",
    "Bardzo przyjemne [doświadczalne demo](https://playground.tensorflow.org/) wygenerowane przez Tensorflow\n",
    "* dobre do wykształcenia sobie intuicji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inne modele\n",
    "<img src=\"../nn_figures/neuralnetworks.png\" width=\"100%\">[asimovinstitute.org]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metody uczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pakiety\n",
    "(w żadnym wyróżnionym porządku, niewyczerpująca lista)\n",
    "1. __numpy__, __scipy__\n",
    "  * wysoko wydajna biblioteka matematyczna\n",
    "  * wektoryzacja obliczeń\n",
    "  * obliczenia statystyczne w scipy\n",
    "1. __scikit-learn__ http://scikit-learn.org/stable/\n",
    "  * głównie metody bardziej klasycznego uczenia maszynowego\n",
    "  * wiele metod preprocessingu, redukcji wymiarowości, wyboru modelu,  wizualizacji\n",
    "  * sieci neuronowe wyłącznie dla zastosowań w małej skali\n",
    "2. __Theano__ https://github.com/Theano/Theano\n",
    "  * środowisko definiujące model obliczeniowy jako optymalizowany graf\n",
    "  * MILA przerywa rozwijanie środowiska\n",
    "  * to samo wcześniej spotkało Pylearn2\n",
    "    * okropnie nieczytelny poprzez użycie poprawnego obiektowego programowania (moim zdaniem ;-)))\n",
    "  * trudny w zrozumieniu: algorytm zrozumienia\n",
    "    1. rozpocznij czytanie instrukcji obsługi Theano\n",
    "    2. gdy zorientujesz się, że programowanie w czystym Theano jest wyjatkowo złożone, znajdź bibliotekę używajacą Theano\n",
    "    3. używaj jej od tego momentu nie usiłując zrozumieć więcej (na ile się da)\n",
    "  * __Lasagne__ https://github.com/Lasagne/Lasagne ułatwia programowanie NN w Theano\n",
    "3. __Tensorflow__ https://www.tensorflow.org\n",
    "  * opracowywane przez Google Brain\n",
    "  * graf obliczeniowy z operacjami definiowanymi jako tensory\n",
    "    * grafy są statyczne\n",
    "  * ma złożoną architekturę:\n",
    "    * system wykonawczy na CPU, GPU, IoS, Android, etc.\n",
    "    * interface w C++, Pythonie\n",
    "  * wbudowana obsługa wielu GPU\n",
    "  * szybko rozwijany (spory zespół)\n",
    "  * dokumentacja w miarę czytelna\n",
    "  * Tensorboard do śledzenia postępów uczenia\n",
    "    * da się wykorzystać w innych środowiskach\n",
    "  * duży zespół prowadzi szeroki rozwój, np. badając ograniczenie pamięci na aktywacje\n",
    "  * dobry system gdy tworzymy modele produkcyjne\n",
    "4. __Keras__ The Python Deep Learning Library https://keras.io\n",
    "  * \"Keras (κέρας) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).\"\n",
    "  * obsługuje Theano, Tensorflow a także CNTK (Microsoft Cognitive Toolkit) jako backendy\n",
    "  * działa na CPU / GPU bez różnicy\n",
    "  * nie wymaga dodatkowej wiedzy i rozumienia grafów obliczeniowych, a wszystko jest napisane jak Python\n",
    "  * prosta konstrukcja stosunkowo złożonych architektur\n",
    "  * od wersji 2.0 umozliwia wygodny zapis _funkcjonalny_ (obecnie wersja 2.0.8)\n",
    "  ```\n",
    "  x = Dense(128)(inputs)   \n",
    "  x = Dense(64)(x)\n",
    "  ```\n",
    "  * ma czasem błędy, które pojawiają się całkiem niespodziwanie\n",
    "  * czasem zupełnie bez wcześniejszego ostrzeżenia znikają niektóre architektury\n",
    "    * np. auto-enkoder czy Highway (jest w osobnej bibliotece)\n",
    "  * jest fajny blog https://blog.keras.io\n",
    "  * całkiem dobra instrukcja obsługi wraz z przykładami\n",
    "  * twórca Francois Chollet jest teraz w Google AI\n",
    "    * prawdopodobnie Keras stanie się jeszcze bardziej wysokopoziomowym interfacem do Tensorflow\n",
    "    * Chollet bardzo aktywny na Twitterze\n",
    "5. __Torch__ http://torch.ch\n",
    "  * napisany dla Lua pakiet przypominający numpy w użyciu\n",
    "  * PyTorch dla Pythona\n",
    "  * w Torchu (w odróżnieniu od np. Tensorflow) graf jest definiowany dynamicznie\n",
    "    * różniczkowanie z wykorzystaniem Autograd\n",
    "  * debugging w miarę prosty (nie jak w Tensorflow)\n",
    "  * http://pytorch.org dla Pythona\n",
    "    * \"A replacement for NumPy to use the power of GPUs\"\n",
    "  * obecnie wersja 1.0 obsługuje już nowe karty NVidia i CUDA 10\n",
    "6. __Caffe__ http://caffe.berkeleyvision.org\n",
    "  * utworzony w Berkeley AI Research\n",
    "  * dużo aplikacji rozpoznawania obrazów\n",
    "  * nie ma automatycznego różniczkowania"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
