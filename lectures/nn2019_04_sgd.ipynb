{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Wsteczna propagacja</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "sns.set(font_scale=2.0)\n",
    "# from bokeh.io import gridplot, output_file, show\n",
    "#from bokeh.plotting import figure, output_notebook\n",
    "# from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metoda spadku gradientu\n",
    "2. __iteracyjna__ metoda minimalizacji kosztu $L(\\theta)$\n",
    "  1. start of $\\theta^0$\n",
    "  2. $\\theta^{i+1}=\\theta^i+\\eta\\Delta\\theta^i$\n",
    "  \n",
    "3. __różniczkowalna__ funkcja kosztu $L()$ \n",
    "\\begin{align}L(\\theta^{i+1})&=L(\\theta^i+\\mu\\Delta\\theta^i)\\\\\n",
    "&\\simeq L(\\theta^i)+\\mu\\nabla^TL(\\theta^i)\\Delta\\theta^i\n",
    "\\end{align}\n",
    "  * wybór $\\Delta\\theta^i$, że $$L(\\theta^{i+1})-L(\\theta^i) \\simeq\\eta\\nabla^TL(\\theta^i)\\Delta\\theta^i\\lt0$$\n",
    "  \n",
    "\n",
    "4. kąt między gradientem a poprawką $\\Delta\\theta^{(i)}$ jest __rozwarty__\n",
    "  * __zbyt duże__ $\\eta_i\\|\\Delta\\theta^{(i)}\\|$ spowoduje, że nowe $\\theta^{i+1}$ może znaleźć się po _drugiej stronie krzywizny_\n",
    "  * szczególnie gdy funkcja kosztu ma bardzo różne nachylenia dla różnych parametrów (wag)\n",
    "  \n",
    "5. __aktualizacja__  $$\\theta^{i+1}=\\theta^i-\\eta\\nabla\\,L$$\n",
    "  * __liniowy__ model i __kwadratowa__ funkcja błędów\n",
    "  $$\\theta^{i+1}=\\theta^i-\\eta(y_k-\\theta^{i^T}x_k)x_k$$\n",
    "    * __błąd__ predykcji $(y_k-\\eta^{i^T}x_k)$\n",
    "    * __uczenie przez poprawianie błędów__\n",
    "4. __współczynnik uczenia__ $\\eta$\n",
    "  * stały, mała wartość\n",
    "  * __duży__ powoduje fluktuacje i oscylacje\n",
    "    * problem ze zbieżnością\n",
    "  * dla sieci liniowej i problemów regresji zwykle malejący w stosunku odwrotnym do kroku uczenia $t$\n",
    "    * warunki konieczne dla regresji\n",
    "    $$\\begin{align}\n",
    "    \\sum_t\\eta_t^2\\lt\\infty\\\\\n",
    "    \\sum_t\\eta_t=\\infty\n",
    "    \\end{align}$$\n",
    "  * dla sieci wielowarstwowych (i innych architektur) możliwe inne procesy adaptacji\n",
    "    * zwiększanie/zmniejszanie $\\eta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wsteczna propagacja błędów\n",
    "<img src=\"../nn_figures/multilayer.png\" width=\"60%\"> [https://www.researchgate.net]\n",
    "0. algorytm __Gradient Descent__\n",
    "1. średni błąd sumą kosztów dla wszystkich przykładów, np. błąd kwadratowy\n",
    "$$E(w)=\\frac{1}{2N}\\sum_n^NE_n=\\frac{1}{2N}\\sum_n^N\\sum_k(y_k-\\widehat{y}_k)^2$$\n",
    "2. aktywacja $k$-tego neuronu to $$\\widehat{y}_k=\\varphi\\left(\\sum_{j=0}^Hw_{kj}y_j\\right)$$\n",
    "gdzie $y_j$ jest aktywacją j-tego neuronu w warstwie ukrytej\n",
    "3. wsteczna propagacja jest __implementacją__ algorytmu spadku gradientu\n",
    "  * modyfikuje wagę $w_{kj}$ (z $j$-tego do $k$-tego neuronu) __proporcjonalnie__ do pochodnej czątkowej $\\partial{}E/\\partial{}w_{kj}$\n",
    "  $$\\dfrac{\\partial{}E}{\\partial{}w_{kj}}=\\frac{\\partial{\\,}E}{\\partial{\\,}e_k}\n",
    "  \\frac{\\partial{\\,}e_k}{\\partial{\\,}y_k} \\frac{\\partial{\\,}y_k}{\\partial{\\,}v_k} \\frac{\\partial{\\,}v_k}{\\partial{\\,}w_{kj}}$$\n",
    "  gdzie $e_k=(y_k-\\widehat{y}_k)$\n",
    "4. poszczególne elementy\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial{\\,}E}{\\partial{\\,}e_k}&=e_k\\\\\n",
    "\\frac{\\partial{\\,}e_k}{\\partial{\\,}y_k}&=-1\\\\\n",
    "\\frac{\\partial{\\,}y_k}{\\partial{\\,}v_k}&=\\varphi'(v_k)\\\\\n",
    "\\frac{\\partial{\\,}v_k}{\\partial{\\,}w_{kj}}&=y_j\\\\\n",
    "\\\\\n",
    "\\text{stąd}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial{\\,}E}{\\partial{\\,}w_{kj}}&=-e_k\\varphi'(v_k)y_j\n",
    "\\end{align}$$\n",
    "5. __reguła delta__ podaje jak modyfikować wagi\n",
    "$$\\Delta{}w_{kj}=-\\eta\\frac{\\partial{\\,}E}{\\partial{\\,}w_{kj}}=\\eta\\mathbf{\\delta_k}y_j$$\n",
    "gdzie $$\\delta_k=-\\frac{\\partial{\\,}E}{\\partial{\\,}v_k}=e_k\\varphi'(v_k)$$\n",
    "  * reguła delta odpowiada regule poprawy wag w jednowarstwowym modelu\n",
    "  * $\\delta_k$ odpowiada wartości błędu w jednowarstwowej sieci\n",
    "6. __jak znaleźć $\\delta_j$ dla neuronów ukrytych__????\n",
    "  * niech $e_j$ będzie _błędem_ dla neuronu ukrytego, wtedy\n",
    "  $$\\begin{align}\n",
    "  \\delta_j&=-\\frac{\\partial{\\,}E}{\\partial{\\,}e_j}\\\\\n",
    "          &=-\\frac{\\partial{\\,}E}{\\partial{\\,}y_j}\\frac{\\partial{\\,}y_j}{\\partial{\\,}v_j}\\\\\n",
    "          &=-\\frac{\\partial{\\,}E}{\\partial{\\,}y_j}\\varphi'(v_j)\n",
    "  \\end{align}$$\n",
    "  * jedynym problemem jest wyliczenie $\\frac{\\partial{\\,}E}{\\partial{\\,}y_j}$\n",
    "  $$\\begin{align}\n",
    "  \\frac{\\partial{\\,}E}{\\partial{\\,}y_j}\n",
    "  &=\\frac{1}{2}\\frac{\\partial}{\\partial{\\,}y_j}\\sum_ke_k^2\\\\\n",
    "  &=\\sum_ke_k\\frac{\\partial{\\,}e_k}{\\partial{\\,}y_j}\\\\\n",
    "  &=\\text{niech $t_k$ będzie prawdziwą wartością dla $x_k$}\\\\\n",
    "  &=\\sum_k\\frac{\\partial{\\,}e_k}{\\partial{\\,}v_k}\\frac{\\partial{\\,}v_k}{\\partial{\\,}y_j}\\\\\n",
    "  &=\\sum_ke_k\\frac{\\partial{\\,}}{\\partial{\\,}v_k}(t_k-y_k)\n",
    "    \\frac{\\partial{\\,}}{\\partial{\\,}y_j}\\left(\\sum_jw_{kj}y_j\\right)\\\\\n",
    "  &=-\\sum_ke_k\\varphi'(v_k)\\frac{\\partial{\\,}}{\\partial{\\,}y_j}\\left(\\sum_jw_{kj}y_j\\right)\\\\\n",
    "  &=-\\sum_k\\underbrace{e_k\\varphi'(v_k)}_{=\\delta_k}w_{kj}\\\\\n",
    "  &=-\\sum_k\\delta_kw_{kj}\n",
    "  \\end{align}$$\n",
    "  * mamy już, że \n",
    "  $$\\begin{align}\n",
    "  \\delta_j&=-\\frac{\\partial{\\,}E}{\\partial{\\,}y_j}\\varphi'(v_j)\\\\\n",
    "  &=\\varphi'(v_j)\\sum_k\\delta_kw_{kj}\n",
    "  \\end{align}$$\n",
    "  * stosując regułę delta dostajemy\n",
    "  $$\\Delta{}w_{ji}=-\\eta\\frac{\\partial{\\,}E}{\\partial{\\,}w_{ji}}=\\eta\\mathbf{\\delta_j}y_i$$\n",
    "  * $\\delta_j$ dla neuronu ukrytego jest ważoną sumą wszystkich $\\delta_k$ __kolejnej__ warstwy połączonej wagami $w_{kj}$ __wychodzącymi__ z neuronu $j$-tego\n",
    "    * stąd nazwa __wstecznej propagacji błędów__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graf obliczeń\n",
    "1. Tradycyjnie patrzy się na _warstwy_ jako grupę identycznie zbudowanych neuronów, z których każdy jest najpierw sumatorem, a potem stosuje nieliniowość\n",
    "  * każdy neuron ma __własne__ parametry (wagi, biasy)\n",
    "2. obecnie patrzymy jako graf operacji\n",
    "  * każda jest sumatorem, nieliniowością, funkcją\n",
    "  * dla każdej definiujemy \n",
    "    * __forward pass__: jak obliczana jest wartość\n",
    "    * __backward pass__ sposób liczenia pochodnej\n",
    "  * ogólne reguły definiują sposób obliczania gradientu funkcji kosztu po parametrach\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktywacje w modelach warstwowych\n",
    "1. wsteczna propagacja sugeruje różniczkowalność\n",
    "2. idealna wydaje się funkcja logistyczna\n",
    "  * różniczkowalna, pochodna łatwa do obliczenia z wartości\n",
    "  * jednak oddalając się od początku pochodna sbliża się do zera\n",
    "  * to daje efekt długich plateau z wolnym uczeniem\n",
    "  * im więcej będzie warstw, tym bardziej ten efekt będzie się skupiał na wcześniejszych (bliżej wejścia)\n",
    "  * gradienty stają się też bardzo niestabilne\n",
    "3. dobrym rozwiązaniem okazuje się ReLU\n",
    "  * nie jest ciągła wszędzie\n",
    "  * jednak gradient w części dodatniej jest stały"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastyczny spadek gradientu\n",
    "1. funkcja kosztu sieci neuronowej\n",
    "  * wiele minimów lokalnych\n",
    "  * badania pokazują, że same minima nie są największym problemem\n",
    "    * w wysokich wymiarach istnieją \"przejścia\" między minimami\n",
    "    * stochastyczność algorytmu pozwala na ucieczkę\n",
    "    * dużym problemem są punkty siodłowe\n",
    "3. __tryby__\n",
    "  * __stochastyczny__ (_Stochastic Gradient Descent_) aktualizacja $\\theta$ dla każdego przykładu\n",
    "    * __szybki__ mało operacji\n",
    "    * __chaotyczny__ losowe przeszukiwanie w różnych kierunkach\n",
    "    * __lokalne__ poprawki\n",
    "    * __GPU__ nie wykorzystane\n",
    "  * __batch__ (_Gradient Descent_)\n",
    "    * aktualizacja $\\Delta\\Theta=\\frac{1}{N}\\sum_i\\Delta\\theta(x_i)$\n",
    "    * każda aktualizacja wymaga $N$ predykcji\n",
    "    * __podobieństwo__ przykładów\n",
    "      * niech $x_k$ i $x_j$ podobne\n",
    "      * modyfikacja identyczna, stąd strata czasu\n",
    "      * modyfikacja dla $x_j$ a __potem__ dla $x_k$ szybciej przybliża\n",
    "    * __globalne__ poprawy\n",
    "    * __GPU__ trudne dla większych zbiorów\n",
    "<img src=\"../nn_figures/stochastic-vs-batch-gradient-descent.png\" width=\"60%\">\n",
    "  * __mini-batch__ \n",
    "    * aktualizacja dla małych batchy\n",
    "    * redukcja wariancji poprawek\n",
    "    * __podobieństwo__ wykorzystane\n",
    "    * __szybka__ zbieżność\n",
    "    * __GPU__ możliwe optymalne wykorzystanie\n",
    "4. Jak korzystać z SGD\n",
    "  * __mieszanie__ przykładów\n",
    "    * przykłady brać losowo - wystarczy mieszać po epoce\n",
    "    * przykład z tej samej klasy/podobnej wartości __nie__ obok siebie\n",
    "  * __preprocessing__ wstępny\n",
    "    * bardzo ważne dla regularyzacji powierzchni kosztu\n",
    "  * __śledzenie__ błędu trenowania i walidacji\n",
    "    1. uczyć przez kilka epok\n",
    "    2. błędy __trenowania__/__walidacji__ w osobnych epokach\n",
    "  * __eksperymentowanie__ ze wzpółczynnikami uczącymi\n",
    "    * używać małych podzbiorów przykładów\n",
    "      * asymptotyczna zbieżność jest niezależna od wielkości pliku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanishing gradient \n",
    "  <p float=\"left\">\n",
    "  <img src=\"../nn_figures/training_speed_2_layers.png\" width=\"51%\" align=\"left\">\n",
    "  <img src=\"../nn_figures/training_speed_3_layers.png\" width=\"51%\" align=\"left\">\n",
    "  <img src=\"../nn_figures/training_speed_4_layers.png\" width=\"51%\" align=\"left\">\n",
    "  </p>[http://neuralnetworksanddeeplearning.com]\n",
    "  \n",
    "  \n",
    "  * sieć o 4 neuronach kolejno po sobie (jeden neuron w jednej warstwie) z wagami\n",
    "    $$(w_1, b_1)\\longrightarrow(w_2, b_2)\\longrightarrow(w_3, b_3)\\longrightarrow(w_4, b_4)$$ \n",
    "    * $b$ to bias\n",
    "    * $y_i=\\sigma(v_i)=\\sigma(w_iy_{i-1}+b_i)$\n",
    "  * zaburzamy nieco bias $b_1$ o $\\Delta b_1$. Jaki będzie wpływ na działanie sieci?\n",
    "    * $f(x+\\Delta x)\\simeq f(x)+f'(x)\\Delta x$\n",
    "    * $$\\begin{align}\\Delta y_1&=\\sigma(w_1y_0+b_1+\\Delta b_1)-\\sigma(w_1y_0+b_1)\\\\&\\simeq\\dfrac{\\partial y1}{\\partial b_1}\\Delta b_1=\\sigma'(v_1)\\Delta b_1\\end{align}$$\n",
    "    * $\\Delta v_2\\simeq\\dfrac{\\partial v_2}{\\partial y_1}=w_2\\Delta y_1\\simeq w_2\\sigma'(v1)\\Delta b_1$\n",
    "    * $\\Delta y_2\\simeq\\sigma'(v_2)\\Delta v_2\\simeq \\sigma'(v_2)w_2\\sigma'(v1)\\Delta b_1$\n",
    "    * $\\Delta v_3\\simeq w_3\\Delta y_2\\simeq w_3\\sigma'(v_2)w_2\\sigma'(v1)\\Delta b_1$\n",
    "    * $\\Delta y_3\\simeq\\sigma'(v_3)\\Delta v_3\\simeq\\sigma'(v_3)w_3\\sigma'(v_2)w_2\\sigma'(v1)\\Delta b_1$\n",
    "    * ...\n",
    "    * $\\Delta E(w)\\simeq\\sigma'(v_1)w_2\\sigma'(v_2)w_3\\sigma'(v_3)w_4\\sigma'(v_4)\\dfrac{\\partial E}{y_4}\\Delta b_1$\n",
    "    * $\\dfrac{\\partial\\Delta E(w)}{\\Delta b_1}\n",
    "    \\simeq\\sigma'(v_1)w_2\\sigma'(v_2)w_3\\sigma'(v_3)w_4\\sigma'(v_4)\\dfrac{\\partial E}{y_4}$\n",
    "    <img src=\"../nn_figures/logistic.png\" width=\"50%\">\n",
    "  * jeśli zainicjalizujemy wagi na wartości $[-1, 1]$, to kazdy składnik $w_i\\sigma'(v_i)<1/4$, a wiele takich składników __bardzo__ zmniejszy gradient\n",
    "  * jednocześnie modyfikacja biasu gdzieś póżniej w sieci będzie odpowiadała fragmentowi ostatniego\n",
    "  $$\\dfrac{\\partial\\Delta E(w)}{\\Delta b_1} \\simeq\\sigma'(v_1)w_2\\sigma'(v_2)w_3\\underbrace{\\sigma'(v_3)w_4\\sigma'(v_4)\\dfrac{\\partial E}{y_4}}_{\\dfrac{\\partial E}{\\partial b_3}}$$\n",
    "  * wagi oczywiście nie muszą być silnie mniejsze od 1\n",
    "  * przy dużej liczbie neuronów wpływ wartości wag i ich zmian staje się coraz bardziej rozmyty\n",
    "  * gdy będą odpowiednio wysokie, to składniki mogą stać się na tyle duże, że spowodują eksplozję gradientu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __exploding gradient__\n",
    "1. niech dana będzie sieć\n",
    "  $$(w_1, b_1)\\longrightarrow(w_2, b_2)\\longrightarrow(w_3, b_3)\\longrightarrow(w_4, b_4)$$ \n",
    "  * niech $w_1=w_2=w_3=w_4=1000$ i biasy tego nie niwelują\n",
    "  * składniki $w_i\\sigma'(v_i)$ w $\\dfrac{\\partial\\Delta E(w)}{\\Delta b_1}\n",
    "    \\simeq\\sigma'(v_1)w_2\\sigma'(v_2)w_3\\sigma'(v_3)w_4\\sigma'(v_4)\\dfrac{\\partial E}{y_4}$\n",
    "spowodują eksplozję\n",
    "  * ale jeśli $w$ wysokie, to (możliwe, że) $wx+b$ wysokie, ale wtedy $\\sigma'(wx+b)$ może być bardzo małe\n",
    "8. __niestabilność__\n",
    "  * gradient wcześniejszych warstw jest __iloczynem__ gradientów w warstwach późniejszych\n",
    "  * konieczny jest mechanizm stabilizujący\n",
    "    * obcinanie gradientu\n",
    "    * różne współczynniki uczenia w warstwach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje aktywacji\n",
    "<img src=\"../nn_figures/activation_functions.png\" width=\"80%\">\n",
    "1. są niemalejące\n",
    "2. wygodnie gdy są różniczkowalne - patrz backprop\n",
    "3. funkcje sigmoidalne sprawiają kłopoty w sieciach głębokich\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zanikający i eksplodujący gradient\n",
    "1. prędkość uczenia w \n",
    "1. __vanishing gradient__\n",
    "2. __exploding gradient__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wariacje na temat SGD\n",
    "1. SGD podąża w kierunku największego spadku przy stałym $\\eta$\n",
    "<img src=\"../nn_figures/nnet-error-surface.png\" width=\"80%\">\n",
    "2. nachylenie w różnych kierunkach może byc całkowicie różne\n",
    "<img src=\"../nn_figures/lin_reg_mse.png\" width=\"80%\">[https://ml4a.github.io/ml4a/how_neural_networks_are_trained/]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
