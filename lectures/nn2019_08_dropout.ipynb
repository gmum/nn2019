{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Dropout</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout \n",
    "1. pojedyncze neurony uczą się zbyt dokładnie (mogą się uczyć)\n",
    "  * overfitting\n",
    "2. sieć tworzy czasem zbyt dużą współzależność grup neuronów\n",
    "  * pary neuronów specjalizują się w rozpoznawaniu bardzo specyficznych problemów\n",
    "  * dropout __losowo__ wyłącza neurony\n",
    "  * neurony mają małą szansę na powtórzenie konfiguracji\n",
    "3. pozwala na uczenie badziej zgrubnych cech\n",
    "  * po wyłączeniu jedych, inne neurony muszą zastąpić tamte w rozpoznawaniu\n",
    "  * można zastosować dla neuronów wejściowych\n",
    "4. wprowadza efekt regularyzacji\n",
    "  * dodawany jest szum do aktywacji\n",
    "4. końcowy model jest __ensemblem__, przy czym liczba tworzonych modeli jest olbrzymia\n",
    "  * w ensemblu końcowy model jest złożeniem wielu modeli podstawowych\n",
    "  * każdy model rozwiązuje _prawie_ ten sam problem\n",
    "  * uśrednienie daje model silniejszy\n",
    "  * przy wyłączaniu tworzy się wiele niezaleźnych wewnętrznych reprezentacji\n",
    "5. zwykle wymaga więcej epok uczenia\n",
    "6. niekoniecznie dobrze współpracuje z BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout - założenia\n",
    "1. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov\n",
    "2. najlepszym sposobem, przy nieograniczonych zasobach, będzie\n",
    "  * uśrednić predykcje po wszystkich ustawieniach parametrów\n",
    "  * ważyć każde przez posterior prawdopodobieństwa przy zadanych danych\n",
    "  * jest to zwykle możliwe tylko dla bardzo małych modeli\n",
    "3. przy łączeniu modeli w ensemble ważne by modele były __różne__ (ang. diverse)\n",
    "  * modele są różne, jeśli _popełniają błędy w różnych miejscach_\n",
    "  * uczenie sieci jest kosztowne\n",
    "4. Dropout stara się rozwiazać oba problemy\n",
    "  * czasowo odrzucane są poszczególne neurony\n",
    "    * widzialne lub ukryte\n",
    "  * prawdopodobieństwo dla ukrytych jest zwykle około $0.5$\n",
    "  * dla wejściowych nawet bliżej $1$!\n",
    "  * sieć staje się bardzo rozrzedzona\n",
    "  * wybór neuronów można zaimplementować jako przemnożenie wszystkich aktywacji przez binarną maskę\n",
    "    * łatwe w implementacji dla współczesnych procesorów\n",
    "    * wygodne także w algorytmie SGD\n",
    "5. poszczególne neurony __nie mogą__ się teraz uczyć skomplikowanych wzorców\n",
    "  * niektóre wejścia są czasem aktywne, czasem nie\n",
    "  * to generalizuje\n",
    "6. uczenie mioże być wolniejsze\n",
    "5. pełna sieć to zbiór $2^n$ różnych zubożonych modeli\n",
    "  * dzielą jednak parametry\n",
    "  * połączenie słabych modeli może dawać model silny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout jako regularyzacja\n",
    "7. Dropout jest metodą regularyzacji przez dodanie szumu\n",
    "  * tzw. _denoising_ autoencoder dostaje zaszumione dane\n",
    "  * model ma odtworzyć dane niezaszumione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "1. sieć warstwowa\n",
    "$$\\begin{align}\n",
    "\\textbf{bez dropout}\\hskip{2em}&\\hskip{2em}\\textbf{z dropout}\\\\\n",
    "\\hskip{2em}&\\hskip{2em}r\\sim{}Bernoulli(p)\\\\\n",
    "\\hskip{2em}&\\hskip{2em}\\widetilde{y}^{(l)}=r*y^{(l)}\\\\\n",
    "z_i^{(l)}=w_i^{(l+1)}\\,y^{(l)}+b_i^{(l+1)}\\hskip{2em}&\\hskip{2em}z_i^{(l)}=w_i^{(l+1)}\\,\\widetilde{y}^{(l)}+b_i^{(l+1)}\\\\\n",
    "y_i^{(l+1)}=g(z_i^{(l+1)})\\hskip{2em}&\\hskip{2em}y_i^{(l+1)}=g(z_i^{(l+1)})\n",
    "\\end{align}$$\n",
    "2. uczenie\n",
    "  * dowolny algorytm typu SGD\n",
    "  * po wylosowaniu neuronów wsteczna propagacja __wyłącznie__ dla pozostawionych\n",
    "  * ważną formą regularyzacji jest __ograniczenie__ normy wektora wag wchodzącego do dowolnego neurona do ustalonej wartości\n",
    "    * tzw. __max-norm__ przez ograniczenie wektora do kuli o promieniu $c$\n",
    "  * także wysokie wartości __decay__ przyspieszają zbieżność"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted dropout\n",
    "* wyzerowanie neuronów w warstwie z prawdopodobieństwem $p$ redukuje wartość oczekiwaną wejść nauronów następnej warstwy o $1-p$\n",
    "* po wyzerowaniu, przemnożyć wartości warstwy liniowej przez $1-p$\n",
    "* sprawdza się lepiej podczas testowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja w czasie testowania\n",
    "1. __nie__ używać dropout w czasie testowania\n",
    "  * wyjście nie ma być teraz stochastyczne\n",
    "2. obliczenia z pomocą modelu dropout\n",
    "  * wykonać wiele powtórzeń stosując dropout\n",
    "  * wystarczająco dużo obliczeń da dobre wyniki\n",
    "  * pracochłonne\n",
    "3. model obliczony wprost ma wadę, bo wartości oczekiwane aktywacji różnią się od prawdziwych\n",
    "  * inverted dropout niweluje ten efekt już w trakcie uczenia\n",
    "4. bez inverted dropout\n",
    "  * przemnożyć aktywacje przez wartość $1-p$\n",
    "  * wybranie modelu i przemnożenie wag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czemu dropout działa\n",
    "1. usunięcia silnych wzorców z uczenia\n",
    "2. podobne do L2 w procesie osłabiania nieistotnych wag\n",
    "  * można pokazać, że czyni to nawet bardziej inteligentnie\n",
    "  * ensemble\n",
    "    * wybrać $k$ modeli\n",
    "      * dla każdego postąpić znaleźć predykcje\n",
    "      * wszystkie wyniki uśrednić\n",
    "      * dla MNIST ok. $50$ modeli równa się z pełnym mnożonym\n",
    "      * zwykle wystarczy wziąć ok. $10$ niezależnych predykcji\n",
    "    * bardziej kosztowne\n",
    "4. Cechy uczenia\n",
    "  * uczenie wymaga zwykle sporo większej liczby epok\n",
    "  * neurony ukryte mają rzadką aktywację\n",
    "  * parametr prawdopodobieństwa pozostawienie neuronu $p$ można wybrać\n",
    "    * niskie $p$ (mało neuronów) powoduje niedofitowanie,  (dla MNIST)\n",
    "    * zwykle $p\\geq0.4$ daje najlepsze wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uwagi\n",
    "* często parametr $p$ jest ustalany osobno dla warstw\n",
    "  * im więcej jest wag między warstwami (duże warstwy), tym mniejsze prawdopodobieństwo usunięcia $p$ (dla neuronów warstwy wcześniejszej) \n",
    "* można także zastosować do neuronów wejściowych\n",
    "* bardzo często dla przetwarzania obrazów\n",
    "  głównie ze wzgledu na brak danych\n",
    "* funkcja kosztu jest stochastyczna i nie dość dobrze zdefiniowana\n",
    "  * debugować bez włączonego dropoutu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
