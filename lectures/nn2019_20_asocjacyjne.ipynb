{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Modele autoasocjacyjne</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "#from bokeh.io import gridplot, output_file, show\n",
    "#from bokeh.plotting import figure, output_notebook\n",
    "#from bkcharts import Scatter\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reguła Hebba\n",
    "> Jeśli akson neuronu A jest wystarczająco blisko neuronu B i regularnie bierze udział w jego aktywacji, wtedy następuje pewien proces wzrostu lub zmian metabolicznych w obu komórkach tak, że zdolność A do aktywacji B wzrasta (Donald Hebb, 1949)\n",
    "\n",
    "\n",
    "1. reguła __uczenia hebbowskiego__ spowoduje wzrost wag $w_{mn}$ i $w_{nm}$\n",
    "\n",
    "  $$\\dfrac{dw_{ij}}{dt}\\propto x_i^Tx_j$$\n",
    "\n",
    "2. to jest uczenie __asocjacyjne__ z lokalną regułą aktywacji\n",
    "\n",
    "  $$\\Delta w_{ij}=\\eta x_i() y_j()$$\n",
    "  * nauczanie bez nauczyciela\n",
    "  * korelacyjne\n",
    "  * jeśli istnieje struktura, to tworzy pamięć asocjacyjną\n",
    "3. uczenie gradientowe poprawiające błąd (error correcting) stosuje identyczną zasadę (tutaj dla kwadratowej funkcji błędu)\n",
    "\n",
    "$$w(t+1)=w(t)-\\eta\\,\\underbrace{(y_k-w(t)^Tx_k)}_{\\text{błąd predykcji}}\\,\\underbrace{x_k}_{\\text{sygnał wejściowy}}$$\n",
    "  gdzie $x_k$ jest $k$-tym przykładem wejściowym\n",
    "  * asocjacja między sygnałem wejściowym a błędem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pamięć asocjacyjna\n",
    "1. __auto__-asocjacja $$\\{x_1,\\dots,x_N\\} \\hskip2em\\text{z}\\hskip2em \\{x_1,\\dots,x_N\\} $$\n",
    "2. __hetero__-asocjacja $$\\{x_1,\\dots,x_N\\} \\hskip2em\\text{z}\\hskip2em \\{y_1,\\dots,y_N\\} $$\n",
    "3. prosty sposób zdefiniowania wag\n",
    "\n",
    "$$\\begin{align*}\n",
    "W=&W_{1}+\\dots+W_{n}\\\\\n",
    "W_{k}=&[y_{k}(x_{k})^{T}]\\\\\n",
    "x_{k}W=&x_{k}(W_{1}+\\dots+W_{n})\n",
    "\\end{align*}$$\n",
    "\n",
    "4. odpowiedź składa się z poprawnego sygnału oraz szumu\n",
    "\n",
    "$$\\begin{align*}\n",
    "(x_k)^{T}W=&(x_k)^{T}(W_1+W_2+\\dots+W_m)\\\\\n",
    "=&(x_k)^{T}W_k+\\sum_{i\\neq k}(x_k)^{T}W_i\\\\\n",
    "=&\\underbrace{y_k(x_k^T x_k)}_{sygnał}+\\underbrace{\\sum_{i\\neq k}y_i(x_i^Tx_k)}_{szum}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pamięć asocjacyjna - problemy\n",
    "1. co zrobić, by odtwarzanie było idealne?\n",
    "  * szum musi być zerowy\n",
    "  * wyktory muszą być parami ortogonalne by $x_i^Tx_j=0$\n",
    "    * nierealne - pamięć ograniczona do liczby wymiarów\n",
    "    \n",
    "  * a gdyby wprowadzić __nieliniowość__ $y_k=\\varphi(x_k^TW)$?\n",
    "    * zakładamy, że wektory są kodowane za pomocą $0/1$\n",
    "    * niech $\\varphi()=sgn()$\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    sgn(x_{k}^{T}W)=&sgn\\left(y_k(x_k\\cdot x_k)+\\sum_{i\\neq k}y_i(x_i\\cdot x_k)\\right)\n",
    "    &\\tag{$x_k^Tx_k>0$}\\\\\n",
    "    =&sgn\\left(y_k+\\sum_{i\\neq k}y^i\\frac{(x_i\\cdot x_k)}{(x_k\\cdot x_k)}\\right)\n",
    "    \\end{align*}$$\n",
    "    \n",
    "    * aby spełnić równość, wystarczy by czynnik szumu był mniejszy od jedności\n",
    "    $$\\left|\\sum_{i\\neq k}y^i\\frac{(x^i\\cdot x_k)}{(x_k\\cdot x_k)}\\right|<1$$\n",
    "\n",
    "5. jakie są szanse, by dla $n$ pamiętanych wektorów rozwiązanie było prawidłowe?\n",
    "    * zależy od liczby pamiętanych wektorów\n",
    "    * od ich postaci\n",
    "    * od odległości od punktu startowego\n",
    "      * $H(\\cdot,\\cdot)$ odległość Hamminga od punktu początkowego iteracji (wiersze od 1 do 4)\n",
    "      * liczba pamiętanych wektorów (kolumny od 2 do 7)\n",
    "      * w kolumnach obok siebie kodowanie za pomocą $0, 1$ (lewa) i za pomocą $-1, +1$ (prawa)\n",
    "      * wartości podają procent poprawnie zbieżnych wektorów\n",
    "\n",
    "| H( , ) | 2 | 3 | 4 | 5 | 6 | 7 |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| 1 | 100 100 | 90 90 | 85 85 | 60 60 | 60 60 | 54 54 |\n",
    "| 2 |  87 100 | 64 73 | 57 71 | 40 42 | 32 35 | 23 28 |\n",
    "| 3 |  50  80 | 39 49 | 25 48 | 14 18 |  8 11 |  5  9 |\n",
    "| 4 |  10  43 | 10 22 |  7 22 |  5  7 |  3  4 |  1  2 |\n",
    "\n",
    "\n",
    "3. w modelu Hopfielda dla $n$ elementowych wektorów pojemność wynosi $0.18 n$ z błędem co najwyżej jednego bitu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
