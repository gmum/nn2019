{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Model Hopfielda</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "#from bokeh.io import gridplot, output_file, show\n",
    "#from bokeh.plotting import figure, output_notebook\n",
    "#from bkcharts import Scatter\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hopfielda\n",
    "<img src=\"../nn_figures/hopfield-round.png\" width=\"51%\" align=\"right\">\n",
    "1. $N$ neuronów\n",
    "\n",
    "  * symetryczne połączenia $w_{ij}=w_{ji}$\n",
    "  * __brak__ połączeń neuronów samych ze sobą\n",
    "  \n",
    "  $$w_{ii}=0$$\n",
    "  * nieliniowa progowa funkcja aktywacji\n",
    "  $$\\begin{align*}\n",
    "  x(v_{i})=\\left\\{\\begin{array}{rl}\n",
    "           +1&v_{i}\\geq{}0\\\\\n",
    "           -1&v_{i}<0\n",
    "           \\end{array}\n",
    "  \\right.\\hskip3em v_{i}=&\\sum_{j}w_{ij}x_{j}\n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "2. __tryby aktualizacji__\n",
    "  * __synchroniczny__\n",
    "    1. __równoczesne__ obliczenie wartości $v_k$ dla __wszystkich__ neuronów\n",
    "    2. aktualizacja stanów\n",
    "  * __asynchroniczny__\n",
    "    1. wybór neuronu \n",
    "      * losowy, w ustalonej kolejności, itp.\n",
    "      * kolejność __nie związana__ z obliczonymi aktywacjami\n",
    "    2. obliczenie aktywacji tego neuronu\n",
    "  * model Hopfielda nakłada dodatkowe ograniczenie na aktualizacje\n",
    "3. __uczenie__ jest jednokrokowe\n",
    "$$w_{ij}=\\frac{1}{N}\\sum_k x_i^kx_j^k$$\n",
    "gdzie $i, j$ są indeksami w wektorze, a $k$ indeksami przykładów\n",
    "  * $w_{ii}=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja energii modelu Hopfielda\n",
    "$$E(x)=-\\frac{1}{2}\\sum_{i}\\sum_{j}w_{ij}x_{i}x_{j}-\\sum_{i}h_{i}x_{i}$$\n",
    "gdzie $h_i$ jest __progiem__ (bias) neuronu $x_i$\n",
    "1. ograniczenie dla uczenia: stan modelu może się zmieniać __tylko wtedy__ gdy poziom energii się __zmniejsza__\n",
    "  1. wybrany $k$-ty neuron (tryb asynchroniczny)\n",
    "  2. $v_{k}=\\sum_{j}w_{kj}x_{j}-h_{k}$\n",
    "  3. $x'_k=sgn(v_k)$\n",
    "  4. $x=x'\\;\\longrightarrow\\;E(x')=E(x)$\n",
    "  5. $x\\neq x'$\n",
    "  6. $$\\begin{align*}\n",
    "     E(x)-E(x')&=\\left(-\\sum_{j}w_{kj}x_{k}x_{j}-h_{k}x_{k}\\right)-\\left(-             \\sum_{j}w_{kj}x'_{k}x'_{j}-h_{k} x'_ { k } \\right)\\\\\n",
    "     &=-(x_{k}-x'_{k})\\left(\\sum_{j}w_{kj}x_{j}-h_{k}\\right)\\\\\n",
    "     &=-(x_{k}-x'_{k})v_{k}\n",
    "     \\end{align*}$$\n",
    "  7. aktywacja neuronu $x_k$ zmieni się __tylko__ wtedy, gdy __zmieni__ się wartość $v_k$\n",
    "  $$\\begin{align*}\n",
    "  sgn(v_{k})&\\neq{}sgn(x_{k})\\\\\n",
    "  sgn(v_{k})&\\neq{}sgn(-x'_{k})\n",
    "  \\end{align*}$$\n",
    "  8. jeśli $(x_k-x'_k)>0\\,\\longrightarrow\\,E(x)-E(x')>0$\n",
    "  9. wartość energii __maleje__ jeśli zmienia się stan\n",
    "    * ponieważ liczba stanów jest ograniczona, to energia musi osiągnąć minimum!\n",
    "2. funkcja energii definiuje __baseny atrakcji__\n",
    "  * baseny atrakcji powinny być związane z pamięciami fundamentalnymi\n",
    "  * ponieważ funkcja jest zdefiniowana dla $I$-neuronowej sieci o __binarnych__ aktywacjach, baseny atrakcji stanu odpowiadają __rogom__ $I$-wymiarowej hiperkostki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zbieżność\n",
    "$$\n",
    "E(x;w)=-\\frac{1}{2}\\sum_{i}\\sum_{j}w_{ij}x_{i}x_{j}-\\sum_{i}h_{i}x_{i}\n",
    "$$\n",
    "1. Funkcja energii jest dla $v_{i}=\\sum_{j}w_{ij}x_{j}$ oraz $x_{i}=\\tanh(v_{i})$ funkcją __Lyapunova__ systemu dynamicznego\n",
    "  * Funkcja E(x) jest f. Lyapunova jeśli ma ciągłe pochodne, $E(\\bar{x})=0$ oraz $E(x)>0$ w pewnym małym otoczeniu punktu stabilności $\\widehat{x}$\n",
    "  * Stan równowagi $\\bar{x}$ jest stabilny jeśli w małym otoczeniu $\\bar{x}$ pochodna funkcji Lyapunowa ze względu na czas jest ujemnie określona\n",
    "  <img src=\"../nn_figures/hopfield-map1.pdf\" width=\"80%\"> [McKay]\n",
    "    * niewielkie różnice (tu nawet do 11) prowadzą do stanów stabilnych\n",
    "    * niektóre też prowadzą do stanów stabilnych, ale __różnych__ od zapamiętywanych\n",
    "      * \"_negatywy_\" stanów pamiętanych są także stabilne ([m] jest negatywem innego stanu stabilnego [l])\n",
    "    * stanami stabilnymi są często także kombinacje pamięci oryginalnych\n",
    "2. jeśli dla systemu istnieje funkcja Lyapunova, to jego przestrzeń stanów jest podzielona na baseny atrakcji\n",
    "3. jeśli funkcja energii (Lyapunowa) jest wypukła dla każdego $v_{i}$, to sieć Hopfielda __zawsze__ będzie zbieżna do jakiegoś stabilnego punktu stałego\n",
    "4. istnienie funkcji Lyapunova zależy od symetryczności wag, braku sprzężenia zwrotnego, oraz asynchronicznego trybu aktywacji\n",
    "5. Usunięcie wag z modelu powoduje zawężenie basenów atrakcji\n",
    "<img src=\"../nn_figures/hopfield-map3.pdf\" width=\"80%\"> [McKay]\n",
    "5. Pamięć modelu jest ograniczona i próba zapamiętania zbyt dużej liczby wzorców kończy się zwykle utworzeniem tylko kilku stanów sztucznych (ang. spurious)\n",
    "<img src=\"../nn_figures/hopfield-map2.pdf\" width=\"75%\"> [McKay]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czy można zapamietać więcej wektorów?\n",
    "1. można poprawiać zapamiętywane wektory\n",
    "  * wektory powinny być bardziej ortogonalne\n",
    "2. więcej wektorów niż zapamiętywane jest stabilne\n",
    "  * stany odwrotne do zapamiętywanych są stabilne\n",
    "  * stabilne mogą być także także mieszaniny stanów\n",
    "3. silnie zaburzona siec także będzie działać\n",
    "4. im więcej zapamiętamy stanów, tym jakość odtwarzania bedzie niższa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pojemność sieci Hopfielda\n",
    "> uwaga notacja $x_i^{(m)}$: \n",
    ">\n",
    "> indeks dolny $i$ to indeks neuronu w modelu, \n",
    ">\n",
    "> indeks górny $(m)$ to indeks przykładu ze zbioru uczącego\n",
    "\n",
    "1. aktywacja i wagi dla wzorca $x^{(n)}$\n",
    "$$\\begin{align}\n",
    "v_{i}=&\\sum_{j}w_{ij}x_{j}^{(n)}\\\\\n",
    "w_{ij}=&x_{i}^{(n)}x_{j}^{(n)}+\\sum_{m\\neq{}n}x_{i}^{(m)}x_{j}^{(m)}\\\\\n",
    "v_{i}=&\\sum_{j\\neq{}i}x_{i}^{(n)}x_{j}^{(n)}x_{j}^{(n)}\n",
    "+\\sum_{j\\neq{}i}\\sum_{m\\neq{}n}x_{i}^{(m)}x_{j}^{(m)}x_{j}^{(n)}\\\\\n",
    "=&(N-1)\\,x_{i}^{(n)}+\\sum_{j\\neq{}i}\\sum_{m\\neq{}n}x_{i}^{(m)}x_{j}^{(m)}x_{j}^{(n)}\n",
    "\\end{align}$$\n",
    "2. pierwszy składnik jest $N-1$ razy większy od oczekiwanej wartości $x_{i}^{(n)}$ (dla przykładu $n$)\n",
    "3. __szum__ ma wartość rzędu $(N-1)(K-1)$ razy losowe wartości $x_{i}^{(m)}x_{j}^{(m)}x_{j}^{(n)}$ (dla $K$ wzorców)\n",
    "5. to losowe wartości o średniej $0$ i wariancji $1$\n",
    "6. $v_{i}$ ma średnią $(N-1)x_{i}^{(n)}$ oraz wariancję $(N-1)(K-1)$\n",
    "7. jakie jest prawdopodobieństwo, że sieć w stanie $x^{(n)}$ będzie stabilna?\n",
    "  * ustawiamy sieć w stanie pamięci $x^{(n)}$\n",
    "  * jaka jest szansa, że i-ty bit zmieni swój stan w pierwszej iteracji?\n",
    "  \n",
    "    * aby neuron nie zmienił stanu, jego wartość oczekiwana powinna wynosić $(I-1)x_i$\n",
    "      * $I$ to liczba nauronów\n",
    "      * średnia rozkładu na rysunku (na osi poziomej aktywacje, załóżmy, że dodatnia)\n",
    "    * prawdopodobieństwo, że stan się zmieni, odpowiada powierzchni gdzie zmienia znak\n",
    "    * jeśli będziemy pamiętać $N\\simeq 0.18I$, to z $1\\%$ szansą jeden bit będzie niestabilny przy pierwszej iteracji\n",
    "8. stany przejściowe w pamięci Hopfielda\n",
    "  * prosta analiza mówi ile neuronów może być niestabilnych w pierwszej iteracji \n",
    "    * następne mogą zmienić sytuację\n",
    "    * niestabilność pierwszego może pociągnąć następne\n",
    "  <img src=\"../nn_figures/hopfield-capacity.pdf\" width=\"100%\">\n",
    "  * w $N/I=0.138$ zachodzi gwałtowna nieciągłość: poniżej większość stanów pożądanych jest stablina z niewielką liczbą zamienionych bitów\n",
    "    * rysunek pokazuje prawdopodobieństwo stabilności pamięci zasadniczych\n",
    "  * jeśli $N/I$ przekracza $0.138$, to istnieją tylko stany fałszywe (tzw. stany szkła spinowego)\n",
    "  * tuż poniżej $0.138$ liczba zamienionych bitów jest rzędu $1.6\\%$\n",
    "  * dla $N/I\\,\\in\\,(0,0.138))$ istnieją stabilne stany blisko stanów zapamiętanych\n",
    "  * dla $N/I\\in(0,0.05)$ stany stabilne związane ze stanami żądanymi mają niższe energie niż stany fałszywe szkła spinowego\n",
    "  * dla $N/I\\in(0.05,0.138)$ stany szkła spinowego dominują i niektóre mają niższą energię niż stany pożądane\n",
    "  * dla $N/I\\in(0.0.03)$ istnieją pewne dodatkowe stany mieszane których energia nie jest niższa od stanów pożądanych\n",
    "  * pojemność może być poprawiona kosztem mniejszych basenów atrakcji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poprawianie pojemności\n",
    "1. dla każdego wzorca, jeśli wszystkie poza $i$-tym neuronem odpowiadają wzorcowi $x^{n}$, to ustaw wagi tak, by $x_{i}=x_{i}^{n}$\n",
    "2. optymalizuj $G(W)$\n",
    "$$G(W)=-\\sum_{i}\\sum_{n}t_{i}^{(n)}\\ln{}y_{i}^{(n)}+(1-t_{i}^{(n)})\\ln{}(1-y_{i}^{(n)})$$\n",
    "gdzie $t_{i}^{(n)}=1$ jeśli $x_{i}^{(n)}=1$, w przeciwnym wypadku $t_{i}^{(n)}=0$,\n",
    "oraz $y_{i}^{(n)}=1/(1+\\exp(-v_{i}^{(n)}))$\n",
    "3. procedura optymalizacji\n",
    "  * inicjalizuj wagi na $W=X^{T}X$ ($X$ -- macierz przykładów)\n",
    "  * poprawiaj\n",
    "    1. $w_{ii}=0\\;\\forall\\,i$\n",
    "    2. $y=\\sigma(xW)$ aktywacje\n",
    "    3. obliczenie błędów $e=t-y$\n",
    "    4. gradienty $gw=x^{T}e$ ($gw$ to macierz dla wszystkich przykładów)\n",
    "    5. $gw=gw+gw^{T}$ dla symetryzacji\n",
    "    6. poprawa wag $w=w+\\eta(gw - \\alpha{}w)$\n",
    "4. to pozwala by więcej wzorców było stabilnymi stanami\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hopfielda dla problemu TSP\n",
    "<img src=\"../nn_figures/hopfield-tsp.pdf\" width=\"90%\"> [McKay]\n",
    "1. Problem NP-zupełny\n",
    "1. opis rozwiązania jako macierz\n",
    "  * każdy wiersz odpowiada miastu\n",
    "  * kolumna kolejności w której miasto zostanie odwiedzone\n",
    "    * naturalnie po jednej jedynce w każdym wierszu/kolumnie\n",
    "  * ujemne wagi między neuronami w kolumnie/wierszu wymuszają __poprawność__ rozwiązania\n",
    "    * wystarczająco wysokie\n",
    "  * ujemne wagi między miastem w pozycji odwiedzenia a innymi miastami w pozycjach poprzedniej/następnej wymuszają __optymalność__ rozwiązania\n",
    "    * proporcjonalne do odległości\n",
    "2. problem z poprawnym określeniem wag\n",
    "  * zbyt duże wartości bezwzględne znajdą rozwiązanie poprawne, ale dalekie od optymalnego\n",
    "  * zbyt małe mogą znaleźć rozwiązania niepoprawne"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
