{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Głębokie sieci warstwowe</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "#from bokeh.io import gridplot, output_file, show\n",
    "#from bokeh.plotting import figure, output_notebook\n",
    "#from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Głębokie sieci warstwowe\n",
    "1. w jaki sposób nauczyć pojedyncze neurony?\n",
    "  * bardzo podstawowe modele nieskończenie wymiarowe\n",
    "    * wykorzystywane przez sieci RBF\n",
    "    * jesli $\\phi(x)$ będzie wystarczająco wysoko wymiarowe, to dopasuje się do zbioru uczącego\n",
    "    * generalizacja słaba\n",
    "  * wyliczyć je ręcznie\n",
    "  * wykorzystać modele __głębokie__ i nauczyć się $\\phi()$\n",
    "    * typowo używamy funckji aktywacji _rectified linear_ __ReLU__$(x)=max(0, x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcje kosztu\n",
    "1. __maximum likelihood__\n",
    "$$L(w)=\\mathbb{E}_{x,y\\sim\\widehat{p}_{DATA}}\\log\\,p_{model}(y\\mid x)$$\n",
    "  * jeśli zakładmy konkretny rozkład prawdopodobieństwa warunkowego, to dostaniemy konkretną funkcję kosztu\n",
    "  * dla $p_{model}(y\\mid x)=\\mathcal{N}(y; f(x;w), \\mathbf{I})$ dostaniemy \n",
    "  $$L(w)=\\frac{1}{2}\\mathbb{E}_{x,y\\sim\\widehat{p}_{DATA}}\\|y-f(x;w\\|^2+const$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurony wyjściowe\n",
    "### __liniowe__ $y=w^Th+b$\n",
    "  * dla średniej warunkowego rozkładu gausowskiego $$p(y\\mid x)\\mathcal{N}(y; f(x;w), \\mathbf{I})$$\n",
    "    * można też przewidywać macierz kowariancji $\\Sigma$\n",
    "      * jednak musi być dodatnio określona\n",
    "      * zwykle ogranicza się do $\\Sigma=\\sigma\\mathbb{I}$\n",
    "      \n",
    "### __sigmoidalne__ dla rozkładu Bernoulliego $y=\\sigma(W^Th+b)$\n",
    "  * nigdy nie osiąga wartosci granicznych\n",
    "  * użycie liniowego obiętego do przedziału $[0,1]$ jest trudne w uczeniu\n",
    "    * po przejściu przez wartości graniczne gradient byłby zerowy\n",
    "\n",
    "### __softmax__ dla wielowymiarowego rozkładu Bernoulliego\n",
    "  * dla zmiennych dyskretnych o $n$ możliwych wartościach\n",
    "    * warstwa liniowa $z=w^Th+b$\n",
    "    gdzie $z_i=\\log\\,P(y=i\\mid x)$\n",
    "    * aby znaleźć $\\widehat{y}$ $$softmax\\,(z)_i=\\frac{\\exp(z_i)}{\\sum_j\\exp(z_j)}$$\n",
    "    * będzie działac prawidłowo przy uczeniu przez maximum log-likelihood\n",
    "    $$\\log\\,softmax(z)_i=z_i-\\log\\,\\sum_j\\exp(z_j)$$\n",
    "      * pierwszy składnik będzie pchany do góry, pozostałe w dół\n",
    "    * drugi składnik można aproksymować \n",
    "    $$\\log\\,\\sum_j\\exp(z_j)\\simeq\\max_j\\,z_j$$\n",
    "      * najsilniej będzie karana największa z niepoprawnych odpowiedzi\n",
    "      * jeśli największą wartość ma już $z_i$, to\n",
    "      $$\\log\\,\\sum_j\\exp(z_j)\\simeq\\max_j\\,z_j=z_i$$\n",
    "      i oba składniki będą się praktycznie zrowały\n",
    "        * przykład nie bedzie miał wielkiego wpływu na uczenie\n",
    "  * może być użyty dla neuronów ukrytych\n",
    "    * by neuron wybierał jedną z wielu wartości\n",
    "  * funkcje celu różne od _log-likelihood_ __nie będą__ dobrze współpracowały z _softmax_\n",
    "    * w szczególności będzie nią błąd kwadratowy\n",
    "  * ustabilizowana wersja softmax\n",
    "    * dla dużych wartości $z$, softmax bedzie się wypłaszczał bardzo\n",
    "    * _softmax_ jest odporny na translację_\n",
    "    * można uniezależnić od ekstremalnych wartosci przez\n",
    "    $$softmax(z)=softmax(z-\\max_j\\,z_j)$$\n",
    "    \n",
    "### __gausowskie__\n",
    "* może reprezentować warunkowy rozkład gausowski dla $y$ pod warunkiem $x$\n",
    "  * dla ustalonego $\\sigma$\n",
    "  * dla uczonej macierzy kowariancji $\\Sigma$\n",
    "    * rzadko kiedy innej niż diagonalna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurony __ukryte__\n",
    "* nigdy nie ma całkowitej pewnosci neurony którego typu będą działały najlepiej\n",
    "* nie wszystkie typy są __wszędzie__ różniczkowalne\n",
    "  * __ReLU__ $\\max(0,x)$ nie jest w $0$\n",
    "  * nie oczekujemy, że sieć rzeczywiscie dotrze do punktu o gradiencie $0$\n",
    "    * stąd punkty o nieokreślonym gradiencie są dopuszczalne\n",
    "    * jeśli pochodne lewa i prawa są różne, to oprogramowanie zwykle zwraca jedną z nich\n",
    "      * którą? to już kwestia gustu$\\ldots$\n",
    "    * rzeczywista wartość zwykle i tak różni się o $\\epsilon$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit __ReLU__\n",
    "1. aktywacja $$g(z)=\\max(0, z)$$\n",
    "  * pochodna dodatnia kiedykolwiek neuron jest aktywny\n",
    "  * druga pochodna prawie wszędzie jest zerowa\n",
    "2. neurony __nie uczą__ się w miejscach, gdzie nie są aktywne\n",
    "  * niezerowe nachylenie dla $z_i<0$\n",
    "  $$g(z;\\alpha)_i=max(0,z_i)+\\alpha_i\\min(0, z_i)$$\n",
    "  * do wykorzystania, gdy cechy są inwariantne na odwrócenie odczytu, np. całkowita zmiana oświetlenie\n",
    "    * __stałe__ $\\alpha_i$ w __Leaky ReLU__\n",
    "    * __parametryczne__ $\\alpha_i$ gdy $\\alpha_i$ jest douczane\n",
    "3. __maxout__ \n",
    "  * złożenie wielu wartości liniowych $z_i$\n",
    "  $$g(z)_i=\\max_{j\\in{}Ind(i)}z_j$$\n",
    "  * maxout uczy się odcinkowo liniowej wypukłej funkcji\n",
    "4. wybór przez założenie o prostocie\n",
    "  * łatwiej uczyć modele, jesli są bardziej podobne do liniowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurony ukryte z sigmoidalnymi funkcjami aktywacji\n",
    "1. __logistyczna__ $g(z)=\\sigma(z)$\n",
    "2. __tanh__ $$g(z)=\\tanh(z)=2\\sigma(z)-1$$\n",
    "  * saturuje podobnie jak logistyczna\n",
    "  * w okolicach zera uczenie przypomina uczenie sieci liniowych\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inne neurony ukryte\n",
    "1. jest bardzo wiele funkcji aktywacji\n",
    "2. __funkcja liniowa__\n",
    "  * zwykle tylko nietóre\n",
    "3. __softmax__\n",
    "  * rzadko, ale mogą wymusić pewne wybory\n",
    "4. __radialne__\n",
    "5. __softplus__\n",
    "$$g(z)=\\log(1+\\exp(z))$$\n",
    "  * wydaje się wygładzoną wersją ReLU\n",
    "  * doświadczalnie sprawuje się słabiej\n",
    "5. __ograniczony $\\tanh$__\n",
    "$$g(z)=\\max(-1,\\min(1,z))$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
