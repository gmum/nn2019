{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Odległość Wassersteina</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from bokeh.io import gridplot, output_file, show\n",
    "from bokeh.plotting import figure, output_notebook\n",
    "from bkcharts import Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"../nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoenkoder z miarą Wassersteina\n",
    "* tzw. optimal-transport OT jest metodą porównywania rozkładów\n",
    "  * wiele silnych definicji odległości (różne dywargencje) często nie dają uzytecznych dla nauczania gradientów\n",
    "  * OT zachowuje się \"przyjemniej\"\n",
    "  * w GANach wymaga jednak spełnienia pewnych ograniczeń\n",
    "\n",
    "* WAE minimalizują koszt _transportu_ $W_c(P_X,P_G)$ dla dowolnej funkcji kosztu $c$\n",
    "* koszt składa się z\n",
    "  * błędu rekonstrukcji $c$\n",
    "  * regularyzatora $D_Z(P_Z,Q_Z)$ karzących za niezgodność rozkładów\n",
    "    * założonego prioru $P_Z$\n",
    "    * rozkładu zakodowanych danych $Q_Z=\\mathbb{E}_{P_Z}[Q(Z\\min X]$\n",
    "      * dla kwadratowego $c$ i celu GAN, WAE jest zgodny z adwersarialnymi auto-enkoderami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem transportowy\n",
    "* (wiki) mamy $m$ kopalni i $n$ fabryk wykorzystujących kopalinę\n",
    "  * niech $$c:\\mathbb{R}^2\\times\\mathbb{R}^2\\longrightarrow[0,\\infty)$$ jest kosztem przewiezienia kopalin z kopalni do fabryki (tu na płaszczyźnie $\\mathbb{R}^2$)\n",
    "    * produkcję jednej kopalni można przewieźć do jednej fabryki\n",
    "  * $T:K\\longrightarrow F$ jest planem (transportem) z kopalni do fabryk\n",
    "  * potrzebujemy __optymalnego__ planu, który minimalizuje koszt $$c(T)=\\sum_mc(m,T(m))$$\n",
    "\n",
    "* (wiki) mamy $n$ książek o równej szerokości na jednej półce\n",
    "  * chcemy przesunąć wszystkie książki w prawo o grubość jednej\n",
    "    1. przesunąć każdą książkę o w prawo\n",
    "    2. przenieść książkę po lewej na sam koniec\n",
    "  * jeśli koszt jest euklidesowski $c(x,y)=\\mid x-y\\mid$, to oba rozwiązania mają równy koszt\n",
    "  * jeśli koszt jest wypukły, np. $c(x,y)=(x-y)^2$, to pierwsze rozwiązanie jest tańsze\n",
    "* rozwiązanie optymalnego transportu OT może służyć jako sposób znalezienia odległości między rozkładami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sformułowanie rozwiązania\n",
    "* niech $c(x,y):X\\times X\\longrightarrow\\mathbb{R}_+$ będzie funkcją kosztu\n",
    "* $P(X\\sim P_X, Y\\sim P_G)$ zbiór rozkładów z rozkładami brzegowymi $P_X$ i $P_G$\n",
    "* wtedy $c(x,y)=d^p(x,y)$ dla $p\\geq1$ jest __odległością Wassersteina__ rzędu $p$\n",
    "* dla $c(x,y)=d(x,y)$ zachodzi $$W_1(P_X,P_G)=\\sup_{f\\in\\mathcal{F}_L}\\mathbb{E}_{X\\sim{}P_X}[f(X)]-\\mathbb{E}_{Y\\sim{}P_G}[f()]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zastosowanie do modeli generatywnych\n",
    "1. samplujemy kod $z\\sim p(z)$, gdzie $p(z)$ jest założonym priorem\n",
    "2. $z$ jest mapowane do obrazu wyjściowego przez generator\n",
    "3. z tego wynika gęstość $$p_G(x)=\\int_Z p_G(x\\mid z)p_Z(z)dz$$\n",
    "  * zakładamy tu, że dekoder jest deterministyczny $x=G(z)$\n",
    "\n",
    "\n",
    "Koszt OT jest planem transportu __przez__ mapowanie $G$\n",
    "  * zamiast szukać powiązania między zmiennymi losowymi w X: jednej z $P_X$, a drugiej z $P_G$\n",
    "  * wystarczy znaleźć takie $Q(z\\mid x)$ takiej, że \n",
    "  $$Q_Z(z)=\\mathbb{E}_{X\\sim{}P_X}[Q(z\\mid x)]$$jest identyczne z rokładem prior $P_Z$\n",
    "  \n",
    "\n",
    "To pozwala na zdefiniowanie kosztu $D_{WAE}$\n",
    "$$D_{WAE}(P_X,P_G)=\\inf_{Q(z\\mid x)}\\mathbb{E}_{P_X}\\mathbb{E}_{Q(z\\mid x)}[c(x,G(z))]+\\lambda D_Z(Q_Z,P_Z),$$\n",
    "gdzie $D_Z()$ jest dywergencją między $P_Z$ a $Q_Z$, $\\lambda>0$\n",
    "<img src=\"../nn_figures/WAE-VAE.pdf\" width=\"80%\">\n",
    "\n",
    "(Tolstikhin,Busquet,Gelly,Scholkopf)\n",
    "1. VAE (po lewej) i WAE (po prawej) minimalizują sumę kosztu rekonstrukcji oraz kosztu niezgodności między priorem $P_Z$ a rozkładem indukowanym przez enkoder $Q$\n",
    "2. VAE wymusza by $Q(z\\mid x)$ odpowiadało $P_Z$ dla wszystkich $x\\sim P_X$ (po lewej)\n",
    "  * każda kula (czerwona) ma odpowiadać $P_Z$ (biała kula)\n",
    "  * kule czerwone przecinają się, co może powodować problemy z rekonstrukcją\n",
    "3. WAE wymusza miksturę $Q_Z=\\int{}Q(z\\mid x)p_X(x)dx$ (zielona) by pasowało do $P_Z$\n",
    "  * kody dla wielu przykładów mają większą szansę nie nakładania się\n",
    "  * co daje lepszą rekonstrukcję"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAE-GAN\n",
    "* $D_Z(Q_Z,P_Z)=D_{JS}(Q_Z,P_Z)$\n",
    "* enkoder $Q$, dekoder $G$, dyskryminator $D$\n",
    "* w pętli aż do zbieżności\n",
    "  1. wylosowanie próbki $\\{x_1,…,x_n\\}$ ze zbioru uczącego\n",
    "  2. wylosowanie próbki $\\{z_1,…,z_n\\}$ z $P_Z$\n",
    "  3. samplowanie $\\tilde{z}_i\\sim Q(z\\mid x_i)$ dla $i=1,…,n$\n",
    "  4. poprawa dyskryminatora przez __wzrost__ $$\\frac{\\lambda}{n}\\sum\\log\\,D(z_i)+\\log\\,(1-D(z_i))$$\n",
    "  5. poprawa enkodera $Q$ i dekodera $G$ przez minimalizację $$\\frac{1}{n}\\sum{}c(x_i,G(\\tilde{x_i}))-\\log\\,D(\\tilde{z}_i)$$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAE-MMD\n",
    "* $\\lambda>0$, kernel charakterystyczny $k$\n",
    "* inicjalizacja enkodera $Q$, dekodera $G$\n",
    "* w pętli aż do zbieżności parametrów\n",
    "  1. wylosowanie próbki $\\{x_1,…,x_n\\}$ ze zbioru uczącego\n",
    "  2. wylosowanie próbki $\\{z_1,…,z_n\\}$ z $P_Z$\n",
    "  3. samplowanie $\\tilde{z}_i\\sim Q(z\\mid x_i)$ dla $i=1,…,n$\n",
    "  5. poprawa enkodera $Q$ i dekodera $G$ przez minimalizację $$\\frac{1}{n}\\sum{}c(x_i,G(\\tilde{x_i}))+\\frac{\\lambda}{n(n-1)}\\sum_{j\\neq{}l}k(z_l,z_j)+\\frac{\\lambda}{n(n-1)}\\sum_{j\\neq{}l}k(\\tilde{z}_l,\\tilde{z}_j)-\\frac{2\\lambda}{n^2}\\sum_{j,l}k(z_l,\\tilde{z}_j)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warunki \n",
    "* dla poprawności funkcje obliczane w rozwiązaniu OT muszą być lipschitzowskie $$\\mid(f(x_1)-f(x_2)\\mid\\leq\\,L\\mid{}x_1-x_2\\mid$$\n",
    "* aby to zapewnić w sieciach neuronowych potrzebnych jest wiele tricków\n",
    "  * clipowanie wag dyskryminatora po wykonaniu update-u\n",
    "  * lepiej sprawdza się nawet clipowanie gradientów\n",
    "  * w związku z tym dyskryminator musi zwykle wykonać szereg cyklów na jedno poprawienie generatora\n",
    "* w modelu MMD zwykle konieczne jest próbkowanie z rozkładu prior\n",
    "  * wygodnie jest zastąpić odpowiednim kernelem, by można było mierzyć odległość między próbką z prioru a próbką z $Q(z\\mid x)$ w sposób analityczny\n",
    "  * takim kernelem może być kernel Gausowski\n",
    "* kernel gausowski bardzo szybko spada\n",
    "  * odległosci dla przykładów w ogonach są zupełnie nieznaczące liczbowo\n",
    "  * potrzebny jest kernel z grubymi końcami, ale możliwy do obliczeń analitycznych"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
