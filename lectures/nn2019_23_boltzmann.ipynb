{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "<big><big><big><big><big><big>Sieci neuronowe 2018/19</big></big></big></big></big></big>\n",
    "\n",
    "---\n",
    "<big><big><big><big><big>Model Boltzmanna</big></big></big></big></big>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "#from bokeh.io import gridplot, output_file, show\n",
    "#from bokeh.plotting import figure, output_notebook\n",
    "#from bkcharts import Scatter\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image inclusion\n",
    "<img src=\"nn_figures/\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maszyna Boltzmanna\n",
    "1. składa się z\n",
    "  * warstwy neuronów ___widzialnych___ $x$\n",
    "  * warstwy neuronów ___ukrytych___ $h$\n",
    "  * wag pomiędzy \n",
    "    * warstwami widzialną i ukrytą $W$\n",
    "    * neuronami warstwy widzialnej $U$\n",
    "    * neuronami warstwy ukrytej $V$\n",
    "  * __wszystkie__ wagi są __symetryczne__\n",
    "2. dla modelu określona jest energia\n",
    "$$E(x,h)=-b^Tx - c^Th -h^TWx -x^TUx - h^TVh$$\n",
    "  * dla prostoty wszędzie dalej będziemy określali \n",
    "    * wszystkie wagi przez $W$\n",
    "    * wszystkie neurony przez $x$\n",
    "  * oraz opuszczali temeperaturę $T$\n",
    "3. prawdopodobieństwo stanu $x$ jest określone przez\n",
    "$$P(x\\mid W)=\\frac{1}{Z}\\,\\exp\\left(\\frac{-E(x;W)}{k_B\\,T}\\right)$$\n",
    "gdzie $Z$ jest __sumą statystyczną__ (partition function)\n",
    "$$Z(W)=\\sum_{x}\\exp\\left(-\\frac{E(x;W)}{k_B\\,T}\\right)$$\n",
    "  * stała Boltzmanna $k_B$ jest dana w Joulach na Kelvin\n",
    "  * jeśli temperaturę określimy w Kelvinach, to energia jest bezjednostkowa\n",
    "  * $Z(W)$ jest sumą energii po __wszystkich__ możliwych stanach\n",
    "  * stany o __niskiej__ energii mają __wyższe__ prawdopodobieństwo\n",
    "  * __zmniejszanie__ temperatury powoduje skupianie się prawdopodobieństwa na małym podzbiorze stanów o niskiej energii\n",
    "    * osiągnie stan stabilny\n",
    "    * __symulowane wyżarzanie__ (simmulated annealing)\n",
    "      * wyższa tamperatura pozwala na łatwiejsze losowe przeskakiwanie do stanów o wyższej energii, co przyspiesza uczenie\n",
    "      * trudne procedury prawidłowego schematu obniżania temperatury\n",
    "      * temperatura musi spadać __nie szybciej__ niż logarytmicznie, co daje gwarancję zbieżności do minimum\n",
    "      * zwykle bardzo powolne __wykładnicze__ obniżanie temperatury, np. $$T_k=\\alpha\\,T_{k-1}$$ dla $\\alpha\\in(0.8, 0.99)$\n",
    "      * zakończenie gdy przez określoną liczbę kroków żadna propozycja nie jest akceptowana\n",
    "4. funkcja log-likelihood układu\n",
    "$$\\begin{align}L(\\theta)&=\\log\\prod_{x\\in\\,Data}P(X=x)\\\\&=\\sum_{x\\in\\,Data}\\log\\,P(X=x)\\end{align}$$\n",
    "5. neurony widzialne i ukryte __różnią__ się\n",
    "  * neurony widzialne dostają zewnętrzny sygnał (mają ustalane aktywacje)\n",
    "  * neurony ukryte mają obliczane aktywacje na podstawie widzialnych\n",
    "6. po obliczeniu ukrytych aktywacji, obliczane są widzialne i warstwami na zmianę aż do osiągnięcia stanu __stabilnego__\n",
    "7. Przeliczając log-likelihood biorąc pod uwagę definicję energii dla stanu dostajemy\n",
    "$$\\log\\left[\\prod_{n=1}^NP(x^{(n)})\\right]=\\sum_{n=1}^N\\left[\\frac{1}{2}x^{(n)^T}Wx^{(n)}-\\log\\,Z(W)\\right]$$\n",
    "8. dla minimalizacji log-likelihood obliczamy pochodną po wagach\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_{ij}}\\log\\,Z(W)&=\\sum_xx_ix_jP(x\\mid W)\\\\\n",
    "&=\\langle\\,x_i\\,x_j\\rangle_{P(x\\mid W)}\\\\\n",
    "\\frac{\\partial}{\\partial w_{ij}}\\log\\,P(\\{x^{(n)}\\}_{n=1}^N\\mid\\,W)&=\\sum_{n=1}^N\\left[x_i^{(n)}x_j^{(n)}-\\langle\\,x_i\\,x_j\\rangle_{P(x\\mid W)}\\right]\\\\\n",
    "&=N\\left[\\langle\\,x_ix_j\\rangle_{Data}-\\langle\\,x_i\\,x_j\\rangle_{P(x\\mid W)}\\right]\n",
    "\\end{align}$$\n",
    "  * pierwszy składnik ostatniego wyrażenia\n",
    "    * iloczyny skalarne pomiedzy neuronami dla wartosci wylosowanych (wysamplowanych) z prawdziwego rozkładu danych $Data$\n",
    "    * to tzw. faza __świadomości__, faza __dodatnia__\n",
    "  * drugi składnik\n",
    "    * iloczyny skalarne dla wszystkich możliwych stanów\n",
    "    * obejmuje także stany warstwy ukrytej\n",
    "    * to tzw. faza __marzeń sennych__, faza __ujemna__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Boltzmann Machine RBM\n",
    "1. uczenie maszyny Boltzmanna w cyklu fazy dodatniej-ujemnej jest __bardzo__ powolne\n",
    "2. ograniczenie maszyny Boltzmanna przez usunięcie połączeń wewnątrz warstw widzialnej i ukrytej\n",
    "$$\\begin{align}\n",
    "E(x,h)&=-b^{T}x-c^{T}h-h^{T}Wx\\\\\n",
    "P(h\\mid x)&=\\prod_{i}P(h_{i}\\mid x)\\\\\n",
    "P(x\\mid h)&=\\prod_{i}P(x_{j}\\mid h)\\\\\n",
    "P(h_{i}=1\\mid x)&=\\frac{exp(c_{i}+W_{i}x)}{1+exp(c_{i}+W_{i}x)}=sigm(c_{i}+W_{i}x)\n",
    "\\end{align}$$\n",
    "3. neurony w poszczególnych warstwach stają się __niezależne__ od siebie i można je liczyć niezależnie\n",
    "  * aktywacje dla ukrytych w zależności od widzialnych z ustawioną (clamped) aktywacją $P(h\\mid x)$\n",
    "  * aktywacje widzialnych na podstawie ukrytych można liczyć niezależnie\n",
    "4. uczenie\n",
    "  1. dla każdej $w_{ij}$ korelacja $\\langle\\,v_i^0h_j^0\\rangle$ dla zadanych widzialnych\n",
    "  2. próbkowanie Gibbsa aż do osiągnięcia stanu stabilnego i korelacji $\\langle\\,v_i^\\infty\\,h_j^\\infty\\rangle$\n",
    "  3. gradient\n",
    "  $$\\frac{\\partial\\,\\log\\,p(v^0)}{\\partial\\,w_{ij}}=\\langle\\,v_i^0h_j^0\\rangle-\\langle\\,v_i^\\infty\\,h_j^\\infty\\rangle$$\n",
    "5. Hinton pokazał, że __wystarczy__ przeprowadzić __dwa__ cykle przetwarzania\n",
    "  1. obliczanie aktywacji ukrytych wprzód\n",
    "  2. obliczenie wejściowych na podstawie ukrytych\n",
    "  \n",
    "  by uzyskać wystarczające przybliżenie całego procesu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Boltzmann Machine\n",
    "1. RBM __nie jest__ wystarczającym modelem\n",
    "2. uczenie głębokiego modelu w oczywisty sposób napotkało na trudności\n",
    "  * trudność z definicją przetwarzania wprzód-wstecz\n",
    "  * zanikanie sygnału gradientu\n",
    "    * wykorzystane aktywacje sigmoidalne\n",
    "  * problem z definicją uczenia nadzorowanego\n",
    "3. Hinton (i współpracownicy) pokazał algorytm __zachłannego__ uczenia modelu\n",
    "  * uczenie nadzorowane ze wstępnym etapem uczenia nienadzorowanego\n",
    "  * rozpoczynamy od prostej RBM złożonej z warstw $x, h_1$\n",
    "  * ta jest uczona do stanu względnej stabilności\n",
    "  * dodawana jest nowa warstwa tworząca na szczycie nową RBM\n",
    "  * znowu uczenie jej do stanu stabilności\n",
    "    * ustalenie likelihood dla $P(h_2\\mid h_1)$\n",
    "  * tak do osiągniecia ustalonej liczby warstw\n",
    "  * to kończy wstępny etap uczenia __nienadzorowanego__\n",
    "  * na szczycie warstwa z uczeniem nadzorowanym\n",
    "  * uczenie tej warstwy\n",
    "  * następnie __tuning__ całego modelu w trybie nadzorowanym\n",
    "3. Salakhutdinov zaproponował rozszerzenie\n",
    "  * podobnie zachłanne uczenie\n",
    "  * w trakcie dodawania nowych warstw i uczenia $P(h_k\\mid h_{k-1})$\n",
    "    * uczenie $P(h_{k-1}\\mid h_k)$ i wstecz\n",
    "  * bardzo złożony algorytm nawet mimo kilku sprytnych trików\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
