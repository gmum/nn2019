{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Przypomnienie Regresji Logistycznej\n",
    "\n",
    "Regresja logistyczna to metoda **klasyfikacji**. Uczenie jest nadzorowane, więc mamy $N$ przykładów treningowych postaci $\\{(x_i, y_i): i=1 \\dots N\\}$, gdzie $y_i$ jest dyskretną etykietą dla wektora cech $x_i=[x_{i1},\\dots,x_{iD}]$.\n",
    "\n",
    "##### 1. Binarna Regresja Logistyczna\n",
    "\n",
    "By uzyskać odpowiedź modelu, dla danego wektora cech $x \\in \\mathbb{R}^D$ obliczany jest najpierw **logit** przy pomocy przekształcenia liniowego:\n",
    "\n",
    "\\begin{equation*}\n",
    "l(x; a, b) = \\ln\\left(\\frac{p(x; a, b)}{1-p(x; a, b)}\\right) = xa^T + b,\n",
    "\\end{equation*}\n",
    "\n",
    "gdzie wektor wag $a \\in \\mathbb{R}^D$ i skalar $b \\in \\mathbb{R}$ (zwany biasem) to parametry modelu. Dla uproszczenia można założyć $x = [1, x_1, \\dots, x_D]$, wtedy $a = [b, a_1, \\dots, a_D]$ i wzór się upraszcza:\n",
    "\n",
    "\\begin{equation*}\n",
    "l(x; a) = xa^T \\in \\mathbb{R}.\n",
    "\\end{equation*}\n",
    "\n",
    "Prawdopodobieństwo $0 \\leq p \\leq 1$ dla danego wektora $x$ można odtworzyć przy pomocy **funkcji logistycznej** $\\sigma(t)=\\frac{1}{1+e^{-t}}$:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x;a) = \\sigma(l(x;a)) = \\frac{1}{1+e^{-l(x;a)}}.\n",
    "\\end{equation*}\n",
    "\n",
    "Celem będzie maksymalizacja funkcji **log-likelihood** ([więcej](https://stats.stackexchange.com/questions/304988/understanding-the-logistic-regression-and-likelihood)), co jest tożsame z minimalizacją **entropii krzyżowej** (ang. cross entropy):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell(X,Y; a) = \\sum_{i=1}^N y_i \\log(p(x_i;a)) + (1-y_i) \\log(1 - p(x_i;a)).\n",
    "\\end{equation*}\n",
    "\n",
    "Funkcję kosztu będziemy optymalizować gradientowo.\n",
    "\n",
    "##### 2. Wieloklasowa Regresja Logistyczna\n",
    "\n",
    "W przypadku klasyfikacji $K$-klasowej potrzebne jest stworzenie $K$ liniowych predyktorów zamiast jednego. Można łatwo tego dokonać zastępując wektor $a$ macierzą wag $A \\in \\mathbb{R}^{K \\times (D+1)}$, w której $i$-ty wiersz odpowiada $i$-temu predyktorowi:\n",
    "\n",
    "\\begin{equation}\n",
    "l(x; A) = xA^T \\in \\mathbb{R}^K.\n",
    "\\end{equation}\n",
    "\n",
    "*Uwaga: Chociaż wynik tej operacji, z matematycznego punktu widzenia, nie powinien być traktowany już jako logit (logarytm szans), to w uczeniu maszynowym zwyczajowo taki wynikowy wektor dla klasyfikacji nazywa się logitami.*\n",
    "\n",
    "Odpowiednikiem funkcji logistycznej dla przypadku wieloklasowego jest **softmax**$: \\mathbb{R}^K \\rightarrow \\mathbb{R}^K$:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x;A)_i = \\operatorname{softmax}(l(x;A))_i = \\frac{e^{l(x;A)_i}}{\\sum_{k=1}^Ke^{l(x;A)_k}},\n",
    "\\end{equation}\n",
    "\n",
    "a funkcją kosztu ponownie jest **entropia krzyżowa** (minimalizujemy):\n",
    "\n",
    "\\begin{equation}\n",
    "L(X,Y; A) = \\frac{1}{N} \\sum_{i=1}^N -\\ell(x_i, y_i; A), \\\\\n",
    "-\\ell(x,y; A) = - \\sum_{i=1}^K t_i \\log(p(x;A)_i) = - \\log(\\operatorname{softmax}(l(x;A))_y),\n",
    "\\end{equation}\n",
    "\n",
    "gdzie $t$ jest wektorem one-hot ([więcej](https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179)) dla etykiety $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Implementacja w Numpy\n",
    "\n",
    "W ramach przypomnienia biblioteki Numpy do uzupełnienia będzie kod wieloklasowej regresji logistycznej. Do aktualizacji wag metodą spadku gradientu potrzebne będą pochodne implementowanych funkcji, podane poniżej.\n",
    "\n",
    "\n",
    "##### 1. Pochodne\n",
    "\n",
    "Funkcja kosztu względem logitów ([przykładowe wyprowadzenie](https://deepnotes.io/softmax-crossentropy)):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial -\\ell}{\\partial l_i} = \\operatorname{softmax}(l)_i - t_i.\n",
    "\\end{equation}\n",
    "\n",
    "Pochodna predyktora liniowego:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l_i}{\\partial a_{ij}} = x_j, \\quad \\forall k \\neq i: \\frac{\\partial l_i}{\\partial a_{kj}} = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Pochodna kosztu po wagach:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial -\\ell}{\\partial a_{ij}} = \\sum_{k=1}^K \\frac{\\partial -\\ell}{\\partial l_k} \\frac{\\partial l_k}{\\partial a_{ij}} = \\frac{\\partial -\\ell}{\\partial l_i} \\frac{\\partial l_i}{\\partial a_{ij}} = (\\operatorname{softmax}(l)_i - t_i) \\cdot x_j.\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial a_{ij}} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial -\\ell(x_i)}{\\partial a_{ij}}\n",
    "\\end{equation}\n",
    "\n",
    "##### 2. Implementacja\n",
    "\n",
    "Należy uzupełnić kod w oznaczonych miejscach tak, aby powstał model regresji logistycznej optymalizowanej metodą spadku gradientu. Do treningu będziemy używać pomniejszonego 10-krotnie zbioru MNIST. Zbiór zawiera obrazy 28x28 ręcznie pisanych cyfr (widoczne poniżej). Wejściem do modelu będzie wektor kolejnych 784 pikseli obrazów i dodatkowa wartość 1 odpowiedzialna za bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x1152 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (5000, 785), test data shape: (1000, 785)\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_dataset(dataset_name):\n",
    "    data = np.load('data/{}/{}.npz'.format(dataset_name.upper(), dataset_name))\n",
    "    return data['X_train'], data['y_train'], data['X_test'], data['y_test']\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_dataset('mini_mnist')\n",
    "\n",
    "f, ax = plt.subplots(1, 10, sharex='col', sharey='row',figsize=(18, 16))\n",
    "for a in ax:\n",
    "    a.imshow(X_train[np.random.randint(X_train.shape[0])].reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "print(\"train data shape: {}, test data shape: {}\".format(X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_linear(x, a):\n",
    "    '''\n",
    "    Calculate l(x;a) in BxK\n",
    "    \n",
    "    :param x: Bx(D+1) input data\n",
    "    :param a: Kx(D+1) weight matrix\n",
    "    '''\n",
    "    ???\n",
    "\n",
    "def np_softmax(l):\n",
    "    '''\n",
    "    Calculate p(l) in BxK\n",
    "    \n",
    "    :param l: BxK logits\n",
    "    '''\n",
    "    ???\n",
    "\n",
    "def np_cross_entropy(p, y):\n",
    "    '''\n",
    "    Calculate L(p,y)\n",
    "    \n",
    "    :param p: BxK predictions\n",
    "    :param y: B true labels\n",
    "    '''\n",
    "    ???\n",
    "\n",
    "def np_cross_entropy_grad(p, y, x):\n",
    "    '''\n",
    "    Calculate dL/da in Kx(D+1)\n",
    "    \n",
    "    :param p: BxK predictions\n",
    "    :param y: B true labels\n",
    "    :param x: Bx(D+1) input data\n",
    "    '''\n",
    "    ???\n",
    "\n",
    "class NumpyLogisticRegression:\n",
    "        \n",
    "    def __init__(self, n_classes, n_epochs, input_size, learning_rate=0.1, batch_size=256):\n",
    "        self.A = np.zeros((n_classes, input_size))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_epochs = n_epochs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ??? # return model predictions\n",
    "    \n",
    "    def train(self, X, Y, X_test=None, y_test=None):\n",
    "        loss, train_accuracy, test_accuracy = [], [], []\n",
    "        \n",
    "        for e in tqdm(range(self.n_epochs)):\n",
    "            \n",
    "            perm = np.random.permutation(len(X))\n",
    "            X, Y, = X[perm], Y[perm]\n",
    "            \n",
    "            for batch in range(len(X) // self.batch_size):\n",
    "                x = X[batch * self.batch_size:(batch + 1) * self.batch_size]\n",
    "                y = Y[batch * self.batch_size:(batch + 1) * self.batch_size]\n",
    "                \n",
    "                p = self.forward(x)\n",
    "                l = np_cross_entropy(p, y)\n",
    "                \n",
    "                loss.append(l)\n",
    "                train_accuracy.append(self.test(x, y))\n",
    "                if X_test is not None and y_test is not None:\n",
    "                    test_accuracy.append(self.test(X_test, y_test))\n",
    "                \n",
    "                grad_A = ??? # calculate gradient\n",
    "                self.A -= grad_A * self.learning_rate\n",
    "                \n",
    "        return loss, train_accuracy, test_accuracy\n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        p = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(p == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NumpyLogisticRegression(n_classes=10, n_epochs=10, input_size=785)\n",
    "loss, train_accuracy, test_accuracy = clf.train(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(loss, train_accuracy, test_accuracy):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    ax1.set_title(\"Best test accuracy: {:.2f}%\".format(max(test_accuracy) * 100))\n",
    "    ax1.plot(train_accuracy, label='train accuracy')\n",
    "    ax1.plot(test_accuracy, label='test accuracy')\n",
    "    ax1.set_xlabel('steps')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_title(\"Minimal loss: {:.4f}\".format(min(loss)))\n",
    "    ax2.plot(loss)\n",
    "    ax2.set_xlabel('steps')\n",
    "    ax2.set_ylabel('loss')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "show_results(loss, train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1.5, -2., 9.], [6., -1.2, 4.]])\n",
    "A = np.array([\n",
    "    [1., 2., 3.],\n",
    "    [6., 5., 4.]\n",
    "])\n",
    "\n",
    "l = np.array([\n",
    "    [ 24.5,  35. ],\n",
    "    [ 15.6,  46. ]\n",
    "])\n",
    "\n",
    "p = np.array([\n",
    "    [2.75356911e-05, 9.99972464e-01],\n",
    "    [  6.27260226e-14,   1.]\n",
    "])\n",
    "\n",
    "y = np.array([0, 1])\n",
    "ce = 5.250013768\n",
    "\n",
    "assert np.allclose(np_linear(x, A), l)\n",
    "assert np.allclose(np_softmax(l), p)\n",
    "assert np.allclose(np_cross_entropy(p, y), ce)\n",
    "\n",
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Implementacja w PyTorchu\n",
    "\n",
    "##### 1. Wstęp do PyTorcha\n",
    "\n",
    "W PyTorchu odpowiednikiem Numpy'owych tablic (np.array) są tensory (torch.Tensor) - tworzy się je analogicznie i dostępne są dla nich podobne operacje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# wartości można podać bezpośrednio do tensora\n",
    "a = torch.Tensor([[1, 2], [3, 4]])  \n",
    "print(a)\n",
    "\n",
    "# tensor wypełniony jedynkami o wymiarach 2x2\n",
    "b = torch.ones((2, 2))  \n",
    "print(b)\n",
    "\n",
    "# macierz identycznościowa 7x7\n",
    "c = torch.eye(7, 7)\n",
    "print(c)\n",
    "\n",
    "# tensor wypełniony losowymi wartościami N(0,1) o wymiarach 2x1x2\n",
    "d = torch.randn((2, 1, 2))  \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting działa podobnie jak w Numpy\n",
    "# a ma wymiar 2x2, d ma wymiar 2x1x2, więc wynik będzie miał wymiar 2x2x2\n",
    "print((a + d).size()) \n",
    "\n",
    "# Działania mają swoje odpowiedniki \"w miejscu\" oznaczone znakiem \"_\" na końcu nazwy metody\n",
    "# Uwaga: w tym przypadku broadcasting nie może zmieniać wymiaru tensora!\n",
    "# a = a + b\n",
    "a.add_(b)  \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Istnieją odpowiedniki funkcji z Numpy, np.\n",
    "print(torch.log(a))        # odpowiednik np.log\n",
    "print(torch.matmul(a, b))  # odpowiednik np.matmul, również w formie skróconej torch.mm\n",
    "print(torch.sum(a))        # odpowiednik np.sum\n",
    "print(a.view(1, 1, 4))     # odpowiednik x.reshape(1, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensory mogą mieć różne typy, zaznacza się urządzenie (CPU/GPU) tensora\n",
    "print(torch.Tensor([1]))             # tensor domyślnego typu torch.float\n",
    "print(torch.FloatTensor([1]))        # tensor typu torch.float\n",
    "print(torch.cuda.DoubleTensor([1]))  # tensor typu torch.double na GPU\n",
    "print(torch.cuda.LongTensor([1]))    # tensor typu torch.long na GPU\n",
    "\n",
    "# Tensory mogą być też tworzone konstruktorem torch.tensor\n",
    "print(torch.tensor([1], device='cuda', dtype=torch.float))\n",
    "\n",
    "# Zmienne modelu, dla których ma być liczony gradient, oznacza się parametrem requires_grad\n",
    "print(torch.tensor([1.], requires_grad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Implementacja Regresji Logistycznej w PyTorchu\n",
    "\n",
    "Należy uzupełnić kod analogicznie do modelu Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float, device=device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float, device=device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_linear(x, a):\n",
    "    '''\n",
    "    Calculate l(x;a) in BxK\n",
    "    \n",
    "    :param x: Bx(D+1) input data\n",
    "    :param a: Kx(D+1) weight matrix\n",
    "    '''\n",
    "    ???\n",
    "    \n",
    "def t_softmax(l):\n",
    "    '''\n",
    "    Calculate p(l) in BxK\n",
    "    \n",
    "    :param l: BxK logits\n",
    "    '''\n",
    "    ???\n",
    "\n",
    "def t_cross_entropy(p, y):\n",
    "    '''\n",
    "    Calculate L(p,y)\n",
    "    \n",
    "    :param p: BxK predictions\n",
    "    :param y: B true labels\n",
    "    '''\n",
    "    ???\n",
    "\n",
    "class TorchLogisticRegression:\n",
    "    \n",
    "    def __init__(self, n_classes, n_epochs, input_size, learning_rate=0.1, batch_size=256):\n",
    "        self.A = torch.zeros((n_classes, input_size), requires_grad=True, device=device)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_epochs = n_epochs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ??? # return model predictions\n",
    "    \n",
    "    def train(self, X, Y, X_test=None, y_test=None):\n",
    "        loss, train_accuracy, test_accuracy = [], [], []\n",
    "        \n",
    "        for e in tqdm(range(self.n_epochs)):\n",
    "            \n",
    "            perm = torch.randperm(len(X))\n",
    "            X, Y, = X[perm], Y[perm]\n",
    "            \n",
    "            for batch in range(len(X) // self.batch_size):\n",
    "                x = X[batch * self.batch_size:(batch + 1) * self.batch_size]\n",
    "                y = Y[batch * self.batch_size:(batch + 1) * self.batch_size]\n",
    "                \n",
    "                p = self.forward(x)\n",
    "                l = t_cross_entropy(p, y)\n",
    "                \n",
    "                loss.append(l)\n",
    "                train_accuracy.append(self.test(x, y))\n",
    "                if X_test is not None and y_test is not None:\n",
    "                    test_accuracy.append(self.test(X_test, y_test))\n",
    "                \n",
    "                # run backward from the loss\n",
    "                grad_A = ??? # calculate gradient\n",
    "                self.A.data -= grad_A * self.learning_rate\n",
    "                # zero gradients after calculations\n",
    "                \n",
    "        return loss, train_accuracy, test_accuracy\n",
    "    \n",
    "    def test(self, X, Y):\n",
    "        p = torch.argmax(self.forward(X), dim=1)\n",
    "        return torch.mean((p == Y).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TorchLogisticRegression(n_classes=10, n_epochs=10, input_size=785)\n",
    "loss, train_accuracy, test_accuracy = clf.train(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_results(loss, train_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = torch.Tensor([[1.5, -2., 9.], [6., -1.2, 4.]])\n",
    "tA = torch.tensor([\n",
    "    [1., 2., 3.],\n",
    "    [6., 5., 4.]\n",
    "], requires_grad=True, dtype=torch.float)\n",
    "tl = torch.Tensor([\n",
    "    [ 24.5,  35. ],\n",
    "    [ 15.6,  46. ]\n",
    "])\n",
    "tp = torch.Tensor([\n",
    "    [2.75356911e-05, 9.99972464e-01],\n",
    "    [  6.27260226e-14,   1.]\n",
    "])\n",
    "ty = torch.LongTensor([0, 1])\n",
    "\n",
    "assert torch.allclose(t_linear(tx, tA), tl)\n",
    "assert torch.allclose(t_softmax(tl), torch.softmax(tl, dim=1))\n",
    "assert torch.allclose(t_cross_entropy(tp, ty), torch.nn.CrossEntropyLoss()(tl, ty))\n",
    "\n",
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oczekiwania od Rozwiązania\n",
    "\n",
    "- Wykresy dla obu modeli (implementowanego w Numpy i PyTorchu) dają sensowe wykresy.\n",
    " - Przez sensowny wykres rozumiem, że funkcja kosztu maleje i wypłaszcza się, a dokładność wzrasta.\n",
    " - Wykresy dla obu modeli są podobne (modele powinny z założenia liczyć to samo z dokładnością do permutacji danych treningowych).\n",
    " - Dla domyślnych parametrów batch size i learning rate model powinien osiągać dokładność powyżej 86%.\n",
    "- Zaimplementowane funkcje przechodzą wszystkie załączone testy.\n",
    "- (opcjonalnie) Implementowane funkcje w Numpy powinny zawierać jak najmniej pętli ze względów wydajnościowych."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
